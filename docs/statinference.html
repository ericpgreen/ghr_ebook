<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Global Health Research Methods - 4&nbsp; Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./causalinference.html" rel="next">
<link href="./ideas.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Statistical Inference</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Global Health Research Methods</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ghr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Global Health Research</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./collaborations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Build Collaborations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ideas.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Develop Research Ideas and Questions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./statinference.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Statistical Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causalinference.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./construct.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Measurement and Construct Validation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#two-major-approaches-to-statistical-inference" id="toc-two-major-approaches-to-statistical-inference" class="nav-link active" data-scroll-target="#two-major-approaches-to-statistical-inference"><span class="toc-section-number">4.1</span>  Two Major Approaches to Statistical Inference</a></li>
  <li><a href="#frequentist-approach" id="toc-frequentist-approach" class="nav-link" data-scroll-target="#frequentist-approach"><span class="toc-section-number">4.2</span>  Frequentist Approach</a>
  <ul class="collapse">
  <li><a href="#how-it-works" id="toc-how-it-works" class="nav-link" data-scroll-target="#how-it-works"><span class="toc-section-number">4.2.1</span>  HOW IT WORKS</a></li>
  <li><a href="#published-example-hap-trial-results" id="toc-published-example-hap-trial-results" class="nav-link" data-scroll-target="#published-example-hap-trial-results"><span class="toc-section-number">4.2.2</span>  PUBLISHED EXAMPLE: HAP TRIAL RESULTS</a></li>
  <li><a href="#caveats-and-considerations" id="toc-caveats-and-considerations" class="nav-link" data-scroll-target="#caveats-and-considerations"><span class="toc-section-number">4.2.3</span>  CAVEATS AND CONSIDERATIONS</a></li>
  <li><a href="#criticisms" id="toc-criticisms" class="nav-link" data-scroll-target="#criticisms"><span class="toc-section-number">4.2.4</span>  CRITICISMS</a></li>
  <li><a href="#there-has-to-be-a-better-way" id="toc-there-has-to-be-a-better-way" class="nav-link" data-scroll-target="#there-has-to-be-a-better-way"><span class="toc-section-number">4.2.5</span>  “THERE HAS TO BE A BETTER WAY!”</a></li>
  </ul></li>
  <li><a href="#bayesian-approach" id="toc-bayesian-approach" class="nav-link" data-scroll-target="#bayesian-approach"><span class="toc-section-number">4.3</span>  Bayesian Approach</a>
  <ul class="collapse">
  <li><a href="#bayesian-re-analysis-of-the-hap-trial" id="toc-bayesian-re-analysis-of-the-hap-trial" class="nav-link" data-scroll-target="#bayesian-re-analysis-of-the-hap-trial"><span class="toc-section-number">4.3.1</span>  BAYESIAN RE-ANALYSIS OF THE HAP TRIAL</a></li>
  </ul></li>
  <li><a href="#keep-learning" id="toc-keep-learning" class="nav-link" data-scroll-target="#keep-learning"><span class="toc-section-number">4.4</span>  Keep Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="statisticalinference" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Statistical Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Consider the following figure. It comes from a randomized clinical trial of 2,303 healthy postmenopausal women that set out to answer the question, “Does dietary supplementation with vitamin D3 and calcium reduce the risk of cancer among older women?” <span class="citation" data-cites="lappe:2017">(<a href="references.html#ref-lappe:2017" role="doc-biblioref">Lappe, Watson, et al., 2017</a>)</span>. Before we go any further, look at the image and decide what you think.</p>
<div class="no-row-height column-margin column-container"></div><div class="column-page page-columns page-full">
<div id="fig-cancer" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/jamanoeffect.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.1: Invasive and in situ cancer incidence among healthy older women receiving vitamin D and calcium vs placebo. Source: <span class="citation" data-cites="lappe:2017">Lappe, Watson, et al. (<a href="references.html#ref-lappe:2017" role="doc-biblioref">2017</a>)</span>.</figcaption><div class="no-row-height column-margin column-container"><div id="ref-lappe:2017" class="csl-entry" role="doc-biblioentry">
Lappe, J., Watson, P., et al. (2017). Effect of vitamin d and calcium supplementation on cancer incidence in older women: A randomized clinical trial. <em>JAMA</em>, <em>317</em>(12), 1234–1243.
</div></div><p></p>
</figure>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>If you said “No, just look at the p-value”, please be patient and I’ll deal with you in moment. If you said, “Maybe, but there’s no estimate of uncertainty” or “Maybe, but what’s more important is the size of the risk decrease”, then you are my favorite. Please go get a cookie.</p>
</div></div><p>If you said “Yes”, you’re probably in good company. I think most readers will come to the same conclusion. Without knowing anything else about the specific analysis or statistics in general, you can look at this figure and see that both groups started at 0% of participants with cancer (which makes sense given the design), over time members of both groups developed some type of cancer, and by the end of the study period cancer was more common among the non-supplement (placebo) group.</p>
<p>But “Yes” is not what the authors concluded. Here’s what they said:</p>
<blockquote class="blockquote">
<p>In this RCT…supplementation with vitamin D3 and calcium compared with placebo <strong>did not</strong> result in a <strong>significantly</strong> lower risk of all-type cancer at 4 years. [emphasis added]</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<p>Media headlines lacked the same nuance. <em>The New York Times</em> wrote, “The supplements did not protect the women against cancer”. A headline in <em>Time</em> read, “There were no differences in cancer rates between the two groups”.</p>
</div></div><p>Technically they are correct. While the supplement group had a 30% lower risk for cancer compared to the placebo group (a hazard ratio of 0.70), the 95% confidence interval around this estimate spanned from 0.47 (a 53% reduction) to 1.02 (a 2% increase), thus crossing the line of “no effect” for ratios at 1.0. The <em>p</em>-value was 0.06 and their a priori significance cutoff was 0.05, so the result was deemed “not significant” and the conclusion was that supplements do not lower cancer risk.</p>
<p>But is that the best take? Not everyone thought so. Here’s what Ken Rothman and his colleagues wrote to the journal editors:</p>
<div class="column-page">
<div id="fig-rothman" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/rothman_tweet.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.2: <span class="citation" data-cites="ken_rothman">(<a href="references.html#ref-ken_rothman" role="doc-biblioref"><strong>ken_rothman?</strong></a>)</span>, May 9, 2017, https://ghr.link/AC</figcaption><p></p>
</figure>
</div>
</div>
<p>JAMA ultimately published a different letter to the editor that raised similar issues <span class="citation" data-cites="jaroudi:2017">(<a href="references.html#ref-jaroudi:2017" role="doc-biblioref">Jaroudi et al., 2017</a>)</span>, and the authors of the original paper responded <span class="citation" data-cites="lappe:2017b">(<a href="references.html#ref-lappe:2017b" role="doc-biblioref">Lappe, Garland, et al., 2017</a>)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="ref-jaroudi:2017" class="csl-entry" role="doc-biblioentry">
Jaroudi, S. et al. (2017). Vitamin d supplementation and cancer risk. <em>JAMA</em>, <em>318</em>(3), 299–299.
</div><div id="ref-lappe:2017b" class="csl-entry" role="doc-biblioentry">
Lappe, J., Garland, C., et al. (2017). <a href="https://doi.org/10.1001/jama.2017.7857"><span class="nocase">Vitamin D Supplementation and Cancer Risk</span></a>. <em>JAMA</em>, <em>318</em>(3), 299–300.
</div></div><blockquote class="blockquote">
<p>…the possibility that the results were clinically significant should be considered. The 30% reduction in the hazard ratio suggests that this difference may be clinically important.</p>
</blockquote>
<p>The best answer, at least in my view, is that the trial was inconclusive. The point estimate is that supplements reduced cancer risk by 30%, but the data are also consistent with a relative reduction of 53% and an increase of 2%. In absolute terms, the group difference in cancer prevalence at Year 4 was 1.69 percentage points. It seems like there might be a small effect. Whether a small effect is clinically meaningful is for the clinical experts on your research team to decide.</p>
<p>This example highlights some of the challenges with <strong>statistical inference</strong>. Recall from <a href="#ghr">Chapter 1</a> that science is all about inference: using limited data to make conclusions about the world. We’re interested in this sample of 2,300 because we think the results can tell us something about cancer risk in older women more generally. But to make this leap, we have to make several inferences.</p>
<p>First, we have to decide whether we think the observed group differences in cancer risk in our limited study sample reflect a true difference or not. This is a question about <strong>statistical inference</strong>. Second, we have to ask whether this difference in observed cancer risk was <em>caused</em> by the supplements. This is a question of <strong>causal inference</strong> (and internal validity), and we’ll take this on in the <a href="causalinference.html">next chapter</a>. Finally, if we think the effect is real, meaningful, and caused by the intervention, do we think the results apply to other groups of older women? This is a question of <strong>generalizability</strong> and external validity, a topic we’ll cover in <a href="#samplingandpower">Chapter 14</a>.</p>
<p>But for now, let’s explore statistical inference. There’s a lot to unpack from this example. I’ll start by telling you about the most common approach that involves <em>p</em>-values and null hypotheses, highlight some of the challenges and controversies of this approach, and then present some alternatives. I’ll end with some suggestions about how you can continue to build your statistical inference skills.</p>
<section id="two-major-approaches-to-statistical-inference" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="two-major-approaches-to-statistical-inference"><span class="header-section-number">4.1</span> Two Major Approaches to Statistical Inference</h2>
<div class="cell">

</div>
<p>There are two main approaches to statistical inference: the <strong>Frequentist</strong> approach and the <strong>Bayesian</strong> approach. A key distinction between the two is the assumed meaning of probability.</p>
<p>Believe it or not, smart people continue to argue about the definition of probability. If you are a Frequentist, then you believe that probability is an objective, long-run relative frequency. Your goal when it comes to inference is to limit how often you will be wrong <em>in the long run</em>.</p>
<div class="cell">

</div>
<p>If you are a Bayesian, however, you favor a subjective view of probability that says you should start with your degree of belief in a hypothesis and update that belief based on the data you collect.</p>
<p>I’ll explain what this all means, but before we get too far along, please think about what YOU want to know most:</p>
<ol type="a">
<li>the probability of observing the data you collected if your preferred hypothesis was not true; or</li>
<li>the probability of your hypothesis being true based on the data you observed?</li>
</ol>
</section>
<section id="frequentist-approach" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="frequentist-approach"><span class="header-section-number">4.2</span> Frequentist Approach</h2>
<div class="cell">

</div>
<p>Open just about any medical or public health journal and you’ll find loads of tables with <em>p</em>-values and asterisks, and results described as “significant” or “non-significant”. These are artifacts of the <strong>Frequentist</strong> approach, specifically the Neyman-Pearson approach.</p>
<section id="how-it-works" class="level3 page-columns page-full" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="how-it-works"><span class="header-section-number">4.2.1</span> HOW IT WORKS</h3>
<p>In the Neyman-Pearson approach, you set some ground rules for inference, collect and analyze your data, and compare your result to the benchmarks you set. Inference is essentially <strike>mindless</strike> automatic once you set the ground rules. To explore how it works, let’s return to the trial of the <em>Healthy Activity Program</em> for depression discussed in <a href="#measurement">Chapter 7</a>. As a reminder, <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> conducted a randomized controlled trial to answer the following research question:</p>
<div class="cell">

</div>
<blockquote class="blockquote">
<p>Among adults 18–65 years of age with a probable diagnosis of moderately severe to severe depression from 10 primary health centres in Goa, India, does enhanced usual care plus a manualised psychological treatment based on behavioural activation delivered by lay counselors—the Healthy Activity Program, or HAP—reduce depression severity compared to enhanced usual care alone? [not a direct quote]</p>
</blockquote>
<section id="step-1-specify-two-hypotheses" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-1-specify-two-hypotheses">Step 1: Specify two hypotheses</h4>
<p>In hypothesis testing, we set up <strong>two</strong> precise statistical hypotheses: a <strong>null hypothesis</strong> (H<sub>0</sub>) and an <strong>alternative hypothesis</strong> (H<sub>1</sub>). Most often the null hypothesis is stated as the hypothesis of no difference:</p>
<div class="cell">

</div>
<blockquote class="blockquote">
<p>H<sub>0</sub>: μ<sub>1</sub> <code>=</code> μ<sub>2</sub> (or μ<sub>1</sub> <code>-</code> μ<sub>2</sub> <code>=</code> 0), meaning there is no difference in average depression severity between the group that was invited to receive HAP plus enhanced usual care compared to the group that only received enhanced usual care</p>
</blockquote>
<div class="cell">

</div>
<p>A “two-tailed” alternative hypothesis states that there <em>is</em> a difference, but does not specify which arm is superior:</p>
<blockquote class="blockquote">
<p>H<sub>1</sub>: μ<sub>1</sub> <code>≠</code> μ<sub>2</sub> (or μ<sub>1</sub> <code>-</code> μ<sub>2</sub> <code>≠</code> 0), meaning that the average difference between the groups is not zero.</p>
</blockquote>
<p>It might seem confusing because H<sub>1</sub> is the hypothesis we talk about and write about, but it’s actually the null hypothesis (H<sub>0</sub>) that we test and decide to reject or accept (technically, ‘fail to reject’).</p>
<div class="cell">

</div>
<p><strong>The null is where statistical inference happens.</strong> The Frequentist rejects or retains the null hypothesis, but does not directly prove the alternative. They simply decide whether there is sufficient evidence to convict (i.e., reject) the null.</p>
</section>
<section id="step-2-imagine-a-world-in-which-h0-is-true-i.e.-innocent" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="step-2-imagine-a-world-in-which-h0-is-true-i.e.-innocent">Step 2: Imagine a world in which H<sub>0</sub> is true (i.e., innocent)</h4>
<p>This is where things get a bit weird. Frequentists subscribe to the <em>long run</em> view of probability. In this framework, you have to establish a <em>collective</em>, a group of events that you can use to calculate the probability of observing any single event <span class="citation" data-cites="dienes2008">(<a href="references.html#ref-dienes2008" role="doc-biblioref">Dienes, 2008</a>)</span>. Your study is just one event. You can’t determine the probability of obtaining your specific results without first defining the collective of all possible studies.</p>
<div class="no-row-height column-margin column-container"></div><p>I know what you’re thinking. This seems nuts. In my defense, I said it gets a bit weird. Hang with me though. The good news is that you do not have to repeat your study an infinite number of times to get the collective. You can do it with your imagination and the magic of statistics.</p>
<p>So put on your wonder cap and imagine that you conducted thousands of experiments where H<sub>0</sub> was true. Yeah, that’s right, I’m asking you to picture running your study over and over again with a new group of people, but the truth is always that the intervention does not work. <strong>The aim of this thought exercise is to establish what type of data we’re likely to find <em>when the null hypothesis is TRUE</em>.</strong></p>
<div class="cell">

</div>
<p>To kick things off, consider this hypothetical accessible population of 6,705 depressed people who are eligible for the HAP trial (see Figure @ref(fig:happop)). In each of your imagined studies, you’ll recruit a new sample of 332 patients from this population. Let’s assume that the baseline level of depression severity among these patients ranges from a score of 10 to 63 on the depression instrument you’re using, the Beck Depression Inventory-II (BDI-II). I say “assume” because you’ll never get to know more than 332 of these 6,705 patients. We know they exist, but we don’t know the true population size or the true average level of depression among this group.</p>
<div class="cell page-columns page-full" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/hap_pop.png" class="img-fluid figure-img" width="1108"></p>
<p></p><figcaption class="figure-caption">Hypothetical population of depressed patients. We get the center and spread of the distribution from the authors’ trial protocol that reports that in a prior study the control group BDI-II mean was 24.5, and the standard deviation was 10.7 <span class="citation" data-cites="patel:2014">(<a href="references.html#ref-patel:2014" role="doc-biblioref">Patel et al., 2014</a>)</span>. <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> did not use the BDI-II to determine eligibility for the HAP trial—just the PHQ-9—but we can assume that no participants in the trial had a BDI-II score of less than 10 at enrollment.</figcaption><div class="no-row-height column-margin column-container"><div id="ref-patel:2014" class="csl-entry" role="doc-biblioentry">
Patel, V. et al. (2014). <a href="https://doi.org/10.1186/1745-6215-15-101">The effectiveness and cost-effectiveness of lay counsellor-delivered psychological treatments for harmful and dependent drinking and moderate to severe depression in primary care in <span>India</span>: <span>PREMIUM</span> study protocol for randomized controlled trials</a>. <em>Trials</em>, <em>15</em>, 101.
</div></div><p></p>
</figure>
</div>
</div>
</div>
<p>Next, imagine that each orange dot in Figures @ref(fig:happop) and @ref(fig:hapsamp) represents 1 of the 332 patients you recruited into the actual study you conducted. You <em>do</em> know each one of these people, you <em>do</em> measure their depression level at baseline, and you <em>do</em> calculate the sample mean. This is the only sample you’ll see as the researcher in real life, but let’s pretend that this sample was #7,501 out of an imaginary set of 10,000 possible studies.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_samp.png" class="img-fluid figure-img" width="1108"></p>
<p></p><figcaption class="figure-caption">Here’s the sample of 332 patients you recruit into your study. <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> did not actually administer the BDI-II at baseline, but we can pretend. Why a sample of 332? <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> tell us they planned to recruit 500 participants but expected to lose track of 15% of them. That’s a planned sample size of 425. But they also tell us that the study design was a bit more complicated, so the planned effective sample size was <code>425/1.28 = 332</code>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-fig.margin="true" data-fig.fullwidth="false">

</div>
<p>Your trial design is a randomized controlled trial, so you randomize these 332 people to the treatment group (HAP plus enhanced usual care) or the control group (enhanced usual care only). As shown in Figure @ref(fig:haprand), you allocate 1:1, meaning that 50% (166) patients end up in the treatment arm, and 50% in the control arm.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_rand.png" class="img-fluid figure-img" width="1108"></p>
<p></p><figcaption class="figure-caption">Distribution of baseline BDI-II scores by study arm. Even with random assignment the group means are not 100% identical at baseline. This is normal. As the sample size gets bigger, randomization produces better balance.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Now I’d like you to imagine that your intervention is NOT superior to enhanced usual care (and vice versa). A few months after the treatment arm completes the program, you reassess everyone in the study and find that the average depression score in <em>both groups</em> decreases by 5 points (see Figure @ref(fig:hapate)). Since the baseline mean for the HAP group was 24.2, and the enhanced usual care arm mean was 28.0, the endline means shift down by 5 points to 19.2 and 23.0, respectively. The <strong>effect size</strong>—in this example the average post-intervention difference between groups—is <code>19.2 - 23.0 = -3.8</code>. The instrument you’re using to measure this outcome, the BDI-II, ranges from a possible score of 0 to 63. So an absolute difference of 3.8 points is small, but it’s not 0.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_ate.png" class="img-fluid figure-img" width="1107"></p>
<p></p><figcaption class="figure-caption">Distribution of endline BDI-II scores by study arm. Here’s what it might look like if depression severity reduces by 5 points in both groups.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong><em>Student in the first row raises hand:</em></strong> If the null hypothesis of no difference is actually true, why isn’t every study result <em>exactly zero</em>?</p>
<p>It’s a good question. The reason is this: there’s error in data collection and sampling error that comes from the fact that we only include a small fraction of the population in our study samples. Therefore, we might get a result that is near 0—but not exactly 0—<em>even if the null hypothesis is really true</em>.</p>
<p>Hopefully Figure @ref(fig:hapnull) will make this point clear. To help you imagine a world in which H<sub>0</sub> is true, I drew 10,000 samples of 332 people from the simulated accessible population of ~6700, randomly assigned each person to a study arm, and calculated the treatment effect for the study if everyone’s depression score reduced by exactly 5 points. This figure plots all 10,000 study results.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_null.png" class="img-fluid figure-img" width="1108"></p>
<p></p><figcaption class="figure-caption">10,000 simulated results when there’s no effect. Unlike the previous figures, this time the dots are study results, not people.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Here’s the key thing to observe:</strong> I simulated 10,000 studies where everyone always improved by an <em>equal amount</em>—i.e., no treatment effect—but there is NOT just one stack of results piled 10,000 high at exactly zero. Instead, the results form a nice bell shaped curve around 0.</p>
<div class="cell">

</div>
<p>This is the central limit theorem at work (aka, the magic of statistics). When plotted together, the results of our imaginary study replications form a distribution that approximates a normal distribution as the number of imaginary replications increases. This is fortunate because we know useful things about normal curves. For instance, we can find any study result on the curve and know where it falls in the distribution. Is it in the fat part around 50%? Or is it a rather extreme result far in the tails at 1% or 2%?</p>
<p>To conclude Step 2, let’s think back to the Frequentist definition of probability that relies on having some <em>collective</em> of events. The plot in Figure @ref(fig:hapnull) represents this collective. Frequentists can only talk about a particular result being in the 1st or 50th percentile of results if there is a group of results that make up the collective. Without the collective—the denominator—there can be no probability.</p>
<p>Of course in reality, no Frequentist repeats a study over and over 10,000+ times to get the collective. They rely on the central limit theorem to imagine the most plausible set of results that might occur when the null hypothesis is really true. This statistically derived, but imaginary, collective is fundamental to the Frequentist approach.</p>
</section>
<section id="step-3-set-some-goal-posts" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-3-set-some-goal-posts">Step 3: Set some goal posts</h4>
<p><strong>Skeptical student:</strong> OK, I get that a study result does not have to be exactly zero for the null to be true. But how different from zero must a result be for Frequentists to reject the null hypothesis that the treatment had no effect?</p>
<div class="cell">

</div>
<p>In the Frequentist approach, you decide if the data are extreme relative to what is plausible under the null by setting some goal posts <em>before</em> you conduct the study. If the result crosses the threshold, it’s automatically deemed “statistically significant” and the null hypothesis is rejected. If it falls short, the null hypothesis of no difference is retained. This threshold is known as the <strong>alpha level</strong>. For Frequentists, alpha represents how willing they are to be wrong in the long run when it comes to rejecting the null hypothesis. Traditionally, scientists set alpha to be no greater than 5%.</p>
<p>Returning to our simulated results in Figure @ref(fig:hapnull), you can see that I drew the goal posts as dotted red lines. They are positioned so that 5% of the distribution of study results falls outside of the lines, 2.5% in each tail. This is a two-tailed test, meaning that we’d look for a result in either direction—treatment group gets better (left, negative difference) OR worse (right, positive difference).</p>
<div class="cell">

</div>
<p>You can also draw a single goal post that contains the full alpha level (e.g., 5%). I show this in Figure @ref(fig:hapnull3) below. This is appropriate when you have a directional alternative hypothesis, such as the treatment mean minus the control mean will be negative. In the HAP example, this would indicate that the treatment group ended the trial with a lower level of depression severity.</p>
</section>
<section id="step-4-make-a-decision-about-the-null" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-4-make-a-decision-about-the-null">Step 4: Make a decision about the null</h4>
<p>With the goal posts set, the decision is automatic. Your actual study result either falls inside or outside the goal posts, and you must either retain or reject the null hypothesis. I’ll say it again: statistical inference happens on the null.</p>
<p>Going back to our example, imagine that you conducted study #7,501 of 10,000. When you collected endline data, you found a mean difference between the two arms of -3.8 as shown in Figure @ref(fig:hapnull3). What’s your decision with respect to the null? Do you retain or reject?</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_reject.png" class="img-fluid figure-img" width="1109"></p>
<p></p><figcaption class="figure-caption">10,000 simulated results when there’s no effect. Result of study #7501 falls outside of the goal post, so the null hypothesis is rejected.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Reject! A difference of -3.8 falls outside of the alpha level <em>you</em> set at 5%. Therefore, you automatically reject the null hypothesis and label the result “statistically significant”.</p>
<p>But there’s something else we know about this result, the raw effect size of -3.8: it falls at the 1st percentile of our simulated collective. It’s an extreme result relative to what we expected if the null is true. We’d say it has a <em>p</em>-value of 1%. The <strong><em>p</em>-value</strong> is a conditional probability. It’s the probability of observing a result as big or bigger than our study result (-3.8)—<em>and here’s the conditional part</em>—IF THE NULL HYPOTHESIS IS TRUE.</p>
<p><em><strong>Smart student mutters to self while taking notes</strong>: Why does he keep saying “if the null hypothesis is true” like some lawyer who loves fine print?</em></p>
<div class="cell">

</div>
<div class="cell">

</div>
<p>I heard that! And good thing, because it’s important to say this again: <strong>when it comes to inference, we never know the “truth”.</strong> We do not know if the null hypothesis is actually true or false. That’s why the p-value is a conditional probability. A <em>p</em>-value is the probability of observing the data if the null hypothesis is true <em>P</em>(D|H), NOT the probability that the null hypothesis is true given the data <em>P</em>(H|D).</p>
<p>Let that sink in. The <em>p</em>-value might not mean what you want it to mean.</p>
<p><strong>Furthermore, since we can’t know the truth about the null hypothesis, it’s possible that we make the wrong decision when we reject or retain it.</strong></p>
<p>If the null hypothesis is really true—and that’s what I simulated—we made a mistake by rejecting the null. We called the result statistically significant, but this is a false positive. Statisticians refer to this mistake as a <strong>Type I error</strong>, though I think the term false positive is more intuitive since we’re falsely claiming that our treatment had an effect when it did not.</p>
<p>The good news is that in the long run we will only make this mistake 5% of the time if we stick to the Frequentist approach. The bad news is that we have no way of knowing if THIS EXPERIMENT is one of the times we got it wrong.</p>
<p>The other type of mistake we can make is called a <strong>Type II error</strong>—a false negative. We make this mistake when the treatment really does have an effect, but we fail to reject the null. We’ll talk more about false negatives—and power—in <a href="#samplingandpower">Chapter 14</a>.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/errors.png" class="img-fluid figure-img" width="1250"></p>
<p></p><figcaption class="figure-caption">Possible outcomes of inferential statistics.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="you-are-significant-even-if-your-p-value-is-not" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="you-are-significant-even-if-your-p-value-is-not">You are significant, even if your <em>p</em>-value is not</h4>
<div class="cell">

</div>
<p>Check out Figure @ref(fig:hapgif) for an animated look at the simulation discussed above. It flips through 20 of the 10,000 studies I simulated to show no treatment effect. Watch for your study, #7501. It’s the only one of the set of 20 that rejects the null.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="false">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hapgif.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Here’s an animated gif of 20 draws from a simulation of 10,000 studies where the treatment has no effect. Watch for the one study that rejects the null.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="published-example-hap-trial-results" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="published-example-hap-trial-results"><span class="header-section-number">4.2.2</span> PUBLISHED EXAMPLE: HAP TRIAL RESULTS</h3>
<div class="cell" data-fig.margin="true" data-fig.fullwidth="false">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_flow.png" class="img-fluid figure-img" width="801"></p>
<p></p><figcaption class="figure-caption">Participant flow diagram, HAP trial. Source: <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> enrolled and randomized 495 depressed adults in Goa, India to receive the HAP intervention plus enhanced usual care or enhanced usual care alone. They reported 2 post-randomization exclusions and 5% loss to follow-up, for a final intent-to-treat analysis sample of 493 (245 treatment, 248 control).</p>
<p>Figure @ref(fig:hapresults) shows the primary and secondary outcomes. We’re interested in the BDI-II score measured 3-months after the treatment group completed the HAP intervention (red outline). After adjusting for the study site (PHC) and participants’ depression scores at baseline (measured via a different instrument, PHQ-9), the authors find that HAP reduced depression severity by an average of 7.57 points (BDI-II ranges from 0 to 63).</p>
<p>If you look back to Figure @ref(fig:hapnull3), you’ll see that a difference of this size is off the chart. So you should not be surprised that <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> report a <em>p</em>-value of &lt; 0.0001. If the null hypothesis is true—meaning that there really was no treatment effect—you would expect to get a difference at least this big less than 0.01% of the time.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="false">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_results.png" class="img-fluid figure-img" width="552"></p>
<p></p><figcaption class="figure-caption">HAP trial primary and secondary outcomes. Source: <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="caveats-and-considerations" class="level3 page-columns page-full" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="caveats-and-considerations"><span class="header-section-number">4.2.3</span> CAVEATS AND CONSIDERATIONS</h3>
<p>Keep these in mind when you review manuscripts or write up your own Frequentist analysis.</p>
<section id="a.-statistical-significance-does-not-imply-practical-or-clinical-significance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a.-statistical-significance-does-not-imply-practical-or-clinical-significance">A. Statistical significance does not imply practical or clinical significance</h4>
<div class="cell">

</div>
<p>To say that a result is “statistically significant” tells the world only that the result was sufficiently surprising to reject the null hypothesis (based on your definition of surprising—alpha—and what data you could expect to see if the null hypothesis is true). <strong>Statistical significance does not imply any type of practical or clinical significance.</strong> It does not mean that your finding is “significant” in the colloquial sense of “meaningful” or “important”.</p>
<div class="cell">

</div>
<p>If statistical significance is your jam, just (plan to) get more fruit. For reasons that will become clear in <a href="#samplingandpower">Chapter 14</a>, simply increasing the sample size will shrink the <em>p</em>-value. With enough resources, you could conduct a study that finds a new intervention “significantly” reduces average systolic blood pressure (the top number) from 120 to 119, <em>p</em> &lt; 0.05. But who cares? Whether this effect size is clinically or practically significant is completely separate from whether you have enough data to say that the effect is not zero. This is why you should always report effect sizes, not just <em>p</em>-values. More on this in a moment.</p>
</section>
<section id="b.-you-can-peek-at-your-data-but-you-have-to-pay-in-alpha" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="b.-you-can-peek-at-your-data-but-you-have-to-pay-in-alpha">B. You can peek at your data, but you have to pay in alpha</h4>
<div class="cell">

</div>
<p>More data = more precision = lower <em>p</em>-values = statistical significance, but be careful. In the Neyman-Pearson approach that takes the long view on probability, you cannot look at your data, find that <em>p</em> = 0.052, and recruit more participants just to push down the <em>p</em>-value—<em>without</em> paying a statistical price.</p>
<div class="cell">

</div>
<p>To be clear, there are lots of reasons why it could make sense to take an interim look at the data before the end of your study. Chief among them is participant safety. If you are testing a new intervention that could cause harm, you would likely create a committee and a plan for looking at your data at various points of the study. But you have to pay the price for peeking. When you take interim peeks at the data you have to move the goal posts outward. Rather than an alpha of 0.05, you might have to raise the bar to 0.01, for instance. There is no free lunch.</p>
</section>
<section id="c.-no-your-p-value-is-not-trending-toward-significance" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="c.-no-your-p-value-is-not-trending-toward-significance">C. No, your <em>p</em>-value is not “trending toward significance”</h4>
<div class="cell">

</div>
<p>In the Neyman-Pearson approach, results are either above the alpha level you set or below it, statistically significant or non-significant. You may not modify the word “significant” with language like “trending” or “marginally” or “approaching”. If you set alpha to 0.05 so your long term error rate is 5%, a <em>p</em>=0.052 for a particular study is non-significant. <em>p</em>-values are not a measure of the strength of your evidence <span class="citation" data-cites="dienes2008">(<a href="references.html#ref-dienes2008" role="doc-biblioref">Dienes, 2008</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dienes2008" class="csl-entry" role="doc-biblioentry">
Dienes, Z. (2008). <em>Understanding psychology as a science: An introduction to scientific and statistical inference</em>. Macmillan International Higher Education.
</div></div></section>
<section id="d.-a-non-significant-finding-does-not-equal-no-effect" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="d.-a-non-significant-finding-does-not-equal-no-effect">D. A non-significant finding does not equal “no effect”</h4>
<div class="cell">

</div>
<p>A common mistake is to infer from a <em>p</em>-value of 0.052 that there is “no effect” or “no difference”. As Lakens explains in his <a href="https://www.coursera.org/learn/statistical-inferences/home/info">Coursera course on statistical inference</a>, all you can take away from a <em>p</em>-value greater than your alpha threshold is that the result is not sufficiently surprising if the null hypothesis is true. It’s possible that the effect is small—too small for you to detect with a small sample size. Scroll to the beginning of this chapter to see an example of this mistake in print.</p>
</section>
</section>
<section id="criticisms" class="level3 page-columns page-full" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="criticisms"><span class="header-section-number">4.2.4</span> CRITICISMS</h3>
<div class="cell">

</div>
<p>The Frequentist approach dominates the literature, but it’s not without its critics. Lots of critics, in fact. More than 800 scientists recently signed on to a proposal to abandon statistical significance <span class="citation" data-cites="amrhein:2019">(<a href="references.html#ref-amrhein:2019" role="doc-biblioref">Amrhein et al., 2019</a>)</span>, and some journals have banned reporting <em>p</em>-values. Other researchers have proposed keeping significance testing, but redefining statistical significance to a higher bar, from an alpha of 0.05 to 0.005 <span class="citation" data-cites="benjamin:2018">(<a href="references.html#ref-benjamin:2018" role="doc-biblioref">Benjamin et al., 2018</a>)</span>. The misuse and misunderstanding of <em>p</em>-values and statistical significance is so widespread that the American Statistical Association issued a statement reminding scientists of what a <em>p</em>-value does and does not tell us <span class="citation" data-cites="wasserstein:2016">(<a href="references.html#ref-wasserstein:2016" role="doc-biblioref">Wasserstein et al., 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-amrhein:2019" class="csl-entry" role="doc-biblioentry">
Amrhein, V. et al. (2019). <a href="https://doi.org/10.1038/d41586-019-00857-9">Scientists rise up against statistical significance</a>. <em>Nature</em>, <em>567</em>(7748), 305–307.
</div><div id="ref-benjamin:2018" class="csl-entry" role="doc-biblioentry">
Benjamin, D. J. et al. (2018). <a href="https://doi.org/10.1038/s41562-017-0189-z">Redefine statistical significance</a>. <em>Nature Human Behaviour</em>, <em>2</em>(1), 6–10.
</div><div id="ref-wasserstein:2016" class="csl-entry" role="doc-biblioentry">
Wasserstein, R. L. et al. (2016). <a href="https://doi.org/10.1080/00031305.2016.1154108">The <span>ASA Statement</span> on p-<span>Values</span>: <span>Context</span>, <span>Process</span>, and <span>Purpose</span></a>. <em>The American Statistician</em>, <em>70</em>(2), 129–133.
</div><div id="ref-meehl:1967" class="csl-entry" role="doc-biblioentry">
Meehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. <em>Philosophy of Science</em>, <em>34</em>(2), 103–115.
</div><div id="ref-meehl:1990" class="csl-entry" role="doc-biblioentry">
Meehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. <em>Psychological Reports</em>, <em>66</em>(1), 195–244.
</div></div><p>Frustration with null-hypothesis significance testing, or NHST, is not new, however. People like Paul Meehl have been warning us for decades <span class="citation" data-cites="meehl:1967 meehl:1990">(<a href="references.html#ref-meehl:1967" role="doc-biblioref">Meehl, 1967</a>, <a href="references.html#ref-meehl:1990" role="doc-biblioref">1990</a>)</span>. So why the recent hubbub? <em>Come closer. No, closer.</em></p>
<p><strong><em>We’ve realized that science is in a crisis—a replication crisis. And NHST is partly to blame.</em></strong></p>
<p>You might remember from <a href="#ghr"><strong>Chapter 1</strong></a> that replication is the process of repeating a study to see if you get the same results. Most methods books will tell you that replication is core to the practice of science, but until recently replication studies in the health and social sciences were very rare. Strong incentives for novelty keep researchers, funders, and journals looking ahead, not behind. But a 2011 paper concluding that ESP is real—yes, that ESP, extrasensory perception—was the breaking point for some. This paper led to some serious soul searching, followed by real efforts to repent, reform, and replicate.</p>
<p>Over the course of 10 years, social psychologist Daryl Bem ran 9 experiments, ~100 participants per experiment, in which he asked people to respond to experimental stimuli <em>before</em> the stimuli were presented to test his ideas about precognition and premonition <span class="citation" data-cites="bem:2011">(<a href="references.html#ref-bem:2011" role="doc-biblioref">Bem, 2011</a>)</span>. <a href="http://www.cc.com/video-clips/bhf8jv/the-colbert-report-time-traveling-porn---daryl-bem">Some of the experiments had college students guess which of two curtains on a computer monitor was hiding an erotic image</a>. <em>After</em> the student responded, the answer was randomly assigned.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bem:2011" class="csl-entry" role="doc-biblioentry">
Bem, D. J. (2011). <em><a href="https://doi.org/10.1037/a0021524">Feeling the future: <span>Experimental</span> evidence for anomalous retroactive influences on cognition and affect</a></em>. <em>100</em>, 407–425.
</div></div><p>All but 1 of these 9 experiments produced statistically significant results supporting ESP. Many in the scientific community lost their minds. <a href="https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html">A 2017 <em>Slate</em> piece by Daniel Engber</a> provides a fascinating look at the scientific reaction to the ESP paper when it was published in <em>Journal of Personality and Social Psychology</em> .</p>
<blockquote class="blockquote">
<p>…for most observers, at least the mainstream ones, the paper posed a very difficult dilemma. It was both methodologically sound and logically insane. Daryl Bem had seemed to prove that time can flow in two directions—that ESP is real. If you bought into those results, you’d be admitting that much of what you understood about the universe was wrong. If you rejected them, you’d be admitting something almost as momentous: that the standard methods of psychology cannot be trusted, and that much of what gets published in the field—and thus, much of what we think we understand about the mind—could be total bunk.</p>
</blockquote>
<div class="cell">

</div>
<p>The conclusion for many skeptics of Bem’s work was that this paper followed the conventions of the time, <em>but the conventions were flawed</em>. Bem’s paper was just a symptom of the disease—QRP.</p>
<div class="cell" data-fig.margin="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/pvalues.png" class="img-fluid figure-img" width="1050"></p>
<p></p><figcaption class="figure-caption">It’s almost like you need a p-value less than 0.05 to get published.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>One notorious QRP goes hand in hand with NHST: <em>p</em>-hacking. You see Reader, data analysis is <a href="https://slate.com/technology/2013/07/statistics-and-psychology-multiple-comparisons-give-spurious-results.html">a garden of forking paths</a>. Even simple analyses require the analyst to make lots of decisions. There are many pathways one can take to get from question to answer. <strong><em>p</em>-hacking</strong> is going down one path, finding a <em>p</em>-value of 0.052, and turning around to go down another path that leads to 0.049. It’s true that an analyst of any stripe can engage in a multitude of QRPs, but <em>p</em>-hacking is uniquely Frequentist.</p>
<p>Of course, it’s not the <em>p</em>-value’s fault that it’s often misunderstood and abused. Even the “abandon statistical significance” camp recognizes its value for some tasks. Their main criticism is that the <em>conventional use</em> of <em>p</em>-values encourages us to think dichotomously—there either is an effect or there is not—and this is bad for science. Non-significant doesn’t mean “no effect”, but that’s often the conclusion when p = 0.052 (just scroll to the top of the chapter for an example).</p>
<p>Furthermore, when publication decisions are made on the basis of <em>p</em> &lt; 0.05, we distort the literature and encourage QRPs. And when we encourage QRPs—particularly when our sample sizes are small and we look for small effect sizes—we end up with a crisis. We publish a lot of noise that fails to replicate. We’ll pick up this replication crisis thread in a later chapter on open science. For now, let’s consider some alternatives to <em>p</em>-values and NHST.</p>
</section>
<section id="there-has-to-be-a-better-way" class="level3 page-columns page-full" data-number="4.2.5">
<h3 data-number="4.2.5" class="anchored" data-anchor-id="there-has-to-be-a-better-way"><span class="header-section-number">4.2.5</span> “THERE HAS TO BE A BETTER WAY!”</h3>
<div class="cell">

</div>
<p>Some argue that the way to avoid dichotomous thinking is to embrace uncertainty and focus on estimating effects—to embrace the “New Statistics” <span class="citation" data-cites="calin-jageman:2019">(<a href="references.html#ref-calin-jageman:2019" role="doc-biblioref">Calin-Jageman et al., 2019</a>)</span>. Figure @ref(fig:newstats) compares the “new” and the “old” (NHST) using the HAP trial results.</p>
<div class="no-row-height column-margin column-container"><div id="ref-calin-jageman:2019" class="csl-entry" role="doc-biblioentry">
Calin-Jageman, R. J. et al. (2019). <a href="https://doi.org/10.1080/00031305.2018.1518266">The <span>New Statistics</span> for <span>Better Science</span>: <span>Ask How Much</span>, <span>How Uncertain</span>, and <span>What Else Is Known</span></a>. <em>The American Statistician</em>, <em>73</em>(sup1), 271–280.
</div></div><div class="cell" data-fig.margin="false">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_newstats.png" class="img-fluid figure-img" width="700"></p>
<p></p><figcaption class="figure-caption">From statistical significance to estimation and quantification of uncertainty. 3-month endline data from the HAP trial <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Panel A represents NHST, where the goal is to determine whether or not there is a statistically significant difference (there is!). Panel B compares the same group means but places the “significant” results in the context of individual data points from all study participants, reminding us that a lot of people in the treatment group remained at a high level of depression severity even though the treatment group improved on average relative to the control group.</p>
<p>Panel C focuses on estimation. It displays the point estimate of the average treatment effect (treatment mean minus control mean, -7.6) and the 95% confidence interval. The confidence interval gives us the same information as a <em>p</em>-value, <em>plus more</em>.</p>
<ul>
<li>That fact that the interval does not include 0 tells us that the difference is statistically significant; the <em>p</em>-value is less than 0.05.</li>
<li>-7.6 is our best estimate of the treatment effect, but the range of the interval tells us that we cannot reject effects between -10.3 and -4.9.</li>
<li>The flip side of this is that we can rule out very large effects greater than -10.3 and very small effects less than -4.9.</li>
</ul>
<p><strong>But here’s the catch with confidence intervals: they are still bound by Frequentists ideas of probability.</strong> Therefore, a confidence interval is a long-run metric. If you were to repeat your study over and over and estimate confidence intervals each time, 95% of the time the true value would fall inside the interval. But we don’t know if in this study—the one we actually conducted—the true value falls inside this specific interval. 95% of the time the interval will contain the true value, but this might be one of those times it doesn’t.</p>
<p>Bottom line: Frequentist confidence intervals are an improvement over <em>p</em>-values, but they still do not tell you the probability that your hypothesis is correct. For that we need the Bayesian approach.</p>
</section>
</section>
<section id="bayesian-approach" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="bayesian-approach"><span class="header-section-number">4.3</span> Bayesian Approach</h2>
<p>Figure @ref(fig:bayespanels) shows the basic idea behind Bayesian analysis in three panels. The first panel shows a simulation of results we believe are plausible <em>before</em> collecting data. The second panel displays the data we collect and our best estimate of how well our model fits the data. The third panel updates our prior belief based on the data we observed.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/bayespanels.png" class="img-fluid figure-img" width="1024"></p>
<p></p><figcaption class="figure-caption">Bayes’ Theorem in three panels. Source: Tristan Mahr, <a href="https://tinyurl.com/ya2tvoaj">https://tinyurl.com/ya2tvoaj</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In Bayesian data analysis, we proclaim our <strong>prior</strong> belief about the hypothesis <em>P</em>(H), collect some data and determine the <strong>likelihood</strong> of the data given our hypothesis <em>P</em>(D|H), and combine the likelihood and prior to obtain the <strong>posterior</strong> probability of the hypothesis given the data we observed <em>P</em>(H|D). This is Bayes’ theorem at work: the posterior is proportional to the likelihood times the prior.</p>
<div class="cell">

</div>
<p>Conceptually, this is very different from the Frequentist approach, even though the answers we get might be similar. In the Frequentist approach, we get a <em>p</em>-value that quantifies the probability of the DATA if the null hypothesis were true. But in the Bayesian approach, we get the probability of the HYPOTHESIS given the data we observed. With Bayesian data analysis, there’s no need to imagine an infinite number of trials—and no need to worry about multiple comparisons or peeks at the data. You can look at your data every day if you want. Every data point updates your belief in the hypothesis. Let’s reexamine the HAP trial from a Bayesian perspective to see the real benefits of a Bayesian approach when it comes to interpretation.</p>
<section id="bayesian-re-analysis-of-the-hap-trial" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="bayesian-re-analysis-of-the-hap-trial"><span class="header-section-number">4.3.1</span> BAYESIAN RE-ANALYSIS OF THE HAP TRIAL</h3>
<p>Figure @ref(fig:hapbayes) compares the Frequentist 95% confidence interval reported in <span class="citation" data-cites="patel:2016">(<a href="references.html#ref-patel:2016" role="doc-biblioref"><strong>patel:2016?</strong></a>)</span> to the Bayesian 95% <strong>credible interval</strong> based a weakly informative prior and the trial data. The results are essentially the same, but the interpretation is very different.</p>
<div class="cell" data-fig.margin="false" data-fig.fullwidth="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hap_concred.png" class="img-fluid figure-img" width="1397"></p>
<p></p><figcaption class="figure-caption">Bayesian re-analysis of HAP primary outcome of depression severity. Anonymized data provided by the authors.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>With Frequentist confidence intervals, we can say that if we repeated this trial over and over, 95% of intervals would contain the true value. We don’t know if the interval on the left is one of the times we got it wrong, but if we act like this interval contains the true value we will only be wrong only 5% of the time.</p>
<div class="cell">

</div>
<p>The Bayesian credible interval on the right tells us something different: we are 95% confident that the treatment effect falls in *this specific interval. We can say this because Bayesian analysis gives us a posterior probability distribution that is not conditional on the null hypothesis being true. A particularly handy feature of posterior distributions is that we can also estimate the probability of any particular point. For instance, there is a 87% chance that the effect size is less than -6.</p>
<p>And look at that—we get the probability of a hypothesis given the data. Nice, right?</p>
</section>
</section>
<section id="keep-learning" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="keep-learning"><span class="header-section-number">4.4</span> Keep Learning</h2>
<p>Statistical inference is hard. Here are a few resources to keep learning:</p>
<ul>
<li><strong>Statistical Inference</strong>: Daniël Lakens’s Coursera course, <a href="https://www.coursera.org/learn/statistical-inferences/home/info">“Improving your statistical inferences”</a></li>
<li><strong>Statistical Inference</strong>: Zoltan Dienes’s book <a href="https://amzn.to/34S9lRm"><em>Understanding Psychology as a Science</em></a></li>
<li><strong>Frequentist Approach</strong>: <a href="https://amzn.to/3evfGqF"><em>Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</em></a>, by Deborah Mayo</li>
<li><strong>New Statistics</strong>: <a href="https://amzn.to/2VGek3H">Understanding the New Statistics</a>, by Geoff Cumming</li>
<li><strong>Bayesian Approach</strong>: <a href="https://amzn.to/2XMHLDP"><em>Statistical Rethinking</em></a> by Richard McElreath</li>
</ul>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ideas.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Develop Research Ideas and Questions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./causalinference.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Causal Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>