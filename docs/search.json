[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Global Health Research Methods",
    "section": "",
    "text": "Preface\nThis book is in the process of being updated. Chapters will be released on a rolling basis. This work is shared under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License."
  },
  {
    "objectID": "ghr.html#what-is-global-health",
    "href": "ghr.html#what-is-global-health",
    "title": "1  Global Health Research",
    "section": "1.1 What is Global Health?",
    "text": "1.1 What is Global Health?\nNew York County Courthouse, Lower Manhattan, New York City, circa 2009\nJudge presiding over jury selection: And what do you do, Mr. Green?\nMe: Global health research.\nJudge:\nMe: I study access to mental health services.\nJudge: So health policy then?\nMe: No, mostly intervention research.\nJudge: Globally.\nMe: No, not quite.\nJudge: What is global health, Mr. Green?\nMe: Well, you see…rambles…\nJudge: Thank you, Mr. Green. You are dismissed.\nI’ve had this conversation hundreds of times since that court appearance. Now when asked, I say something like, “global health takes a global perspective on public health problems,” drawing inspiration from Skolnik (2019). In the wake of the pandemic, I find that people nod along at this framing. It makes sense to them. Thanks, COVID-19!\n\nSkolnik, R. (2019). Global health 101, fourth edition. Jones & Bartlett Learning.\n\nMerson, M. H. et al. (2018). Global health: Diseases, programs, systems, and policies (4th ed.). Jones & Bartlett Learning.\n\nKoplan, J. P. et al. (2009). Towards a common definition of global health. The Lancet, 373(9679), 1993–1995.\nGo any deeper below the ontological surface, however, and you’ll find that there is not a consensus definition of global health (Merson et al., 2018). We’ll adopt this one from Koplan et al. (2009):\n\nGlobal health is an area for study, research, and practice that places a priority on improving health and achieving equity in health for all people worldwide. Global health emphasizes transnational health issues, determinants, and solutions; involves many disciplines within and beyond the health sciences and promotes interdisciplinary collaboration; and is a synthesis of population-based prevention with individual-level clinical care.\n\nTake note of two key elements of their definition:\n\nit includes scholars, researchers, and practitioners working across disciplinary boundaries; and\nit goes beyond simply improving health to include the goal of achieving health equity.\n\nTo expand on the first point, this definition reflects the reality that global health challenges are complex, so the search for solutions must span disciplines. In the study of malaria, for example, you can read about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), and the adoption and use of bed nets (psychology), just to name a few areas of inquiry.\nThe second point is that global health is action-oriented, seeking to achieve health equity for all people worldwide. The WHO (2021) defines equity as follows:\n\nWHO. (2021). Health Equity.\n\nEquity is the absence of unfair, avoidable or remediable differences among groups of people, whether those groups are defined socially, economically, demographically, or geographically or by other dimensions of inequality (e.g. sex, gender, ethnicity, disability, or sexual orientation). Health is a fundamental human right. Health equity is achieved when everyone can attain their full potential for health and well-being.\n\nPut another way, health inequities are unfair and unjust differences in healthcare access or health outcomes that can be prevented or fixed. Health inequities are structural, often resulting from decisions we make about who gets access to resources.\nThe consequence of inequity is often inequality. For instance, inequitable access to healthcare services can lead to unequal health outcomes—health inequalities—between groups. Differences in health status are also referred to as health disparities.\nThe COVID-19 pandemic has given us many examples of health inequities and disparities. For instance, data compiled by the website Health Inequities Tracker, visualized in Figure 1.1, show that through at least August 2021, Hispanics and Latinos in the United States were over-represented in COVID-19 hospitalizations, while non-Hispanic Whites were substantially under-represented (Satcher Health Leadership Institute, 2021).\n\nSatcher Health Leadership Institute. (2021). Health Equity Tracker.\n\n\n\n\nFigure 1.1: COVID-19 health disparities.\n\n\n\nMacias Gil et al. (2020) point to several factors that might help to explain why Latinos and Hispanics were disproportionately affected by COVID-19:\n\nMacias Gil, R. et al. (2020). COVID-19 pandemic: Disparate health impact on the hispanic/latinx population in the united states. The Journal of Infectious Diseases, 222(10), 1592–1595.\n\nHigher rates of co-morbid health conditions\nMore likely to be underinsured or uninsured\nUndocumented immigration status\nLanguage barriers to accessing services\nOverrepresentation in “essential” jobs where working from home was not possible\nGreater financial pressures to show up to work, even if unwell or potentially exposed to the virus\nMore likely to live in multigenerational homes where transmission was more likely\n\nSeveral of these underlying factors, such as undocumented immigration status, fall into the category of social determinants of health (WHO, 2013).\n\nWHO. (2013). Social determinants of health: Key concepts.\n\nThe circumstances in which people are born, grow up, live, work and age, and the systems put in place to deal with illness.\n\nConsider the directed acyclic graph, or DAG, in Figure 1.2 that illustrates how social determinants of health might have influenced the pandemic. I’ll introduce DAGs in more detail in a later chapter, so for now think of this as a (simplified) hypothesized conceptual model of what precipitates COVID-19 infection and hospitalization.\n\n\n\n\nFigure 1.2: Based on Nafilyan et al. (2021).\nNafilyan, V. et al. (2021). Occupation and COVID-19 mortality in England: A national linked data study of 14.3 million adults. medRxiv.\n\n\n\n\nAs represented in this DAG, hospitalization with COVID-19 is directly caused by infection with the novel coronavirus, SARS-CoV-2, but who gets infected is not completely random. Infectious diseases are social affairs, and some people are more vulnerable because of their context. For instance, vaccinated individuals are less likely to get infected, and vaccination rates are highest in the U.S. among the most educated. Looking further back in the causal chain, educational attainment is highest among groups without historical social inequities such as systematic racism.\nA diagram like this suggests that to prevent future pandemics we need to gain a deeper understanding of the interaction between social determinants of health and disease risk. More importantly, it implies that we have work to do to fix the underlying societal inequities that make certain groups more vulnerable. As we’ve seen with COVID-19, technological solutions alone—like developing a vaccine in record time—may not be sufficient.\n\n\n\n\n\nFigure 1.3: Global access to COVID-19 vaccines.\n\n\n\nSirleaf, E. J. et al. (2021). Achieving vaccination justice: A call for global cooperation. PLOS Global Public Health, 1(10), 1–3.\nAnother example of a COVID-19 inequity is global access to vaccines (see Figure 1.2). By October 2021, nearly half of the world’s population had received at least one dose of a COVID-19 vaccine, but the first 6.5 billion doses mostly went into the arms of citizens of wealthy countries. Less than 3 percent of people in low-income countries had received even one dose. While true that many low-income countries escaped the worst of the pandemic’s first few waves, the glacial roll out of vaccines globally—what some have decried as a vaccine apartheid—leaves many nations vulnerable to deadly new variants. This puts everyone at risk. Ellen Johnson Sirleaf and Helen Clark, the former heads of Liberia and New Zealand, respectively, state this plainly in the inaugural issue of PLOS Global Public Health (Sirleaf et al., 2021):\n\nAchieving vaccination justice is the first great test of this pandemic era—it requires targets and aspirations for vaccine access to be determined by health criteria not a country’s economic status, and timely delivery not a two-speed world where high-income populations are fully immunized within months but the poor are denied access for years. Meeting the vaccine justice test will signal that we have both understood the interdependence that determines our planetary future and have the capacity to act on it. Failing the test will condemn us to a ‘forever crisis’ of insecurity and recrimination as virus variants are given free rein and new vaccines struggle to keep up."
  },
  {
    "objectID": "ghr.html#what-is-research",
    "href": "ghr.html#what-is-research",
    "title": "1  Global Health Research",
    "section": "1.2 What is Research?",
    "text": "1.2 What is Research?\n\n\n\nThis definition comes from Title 45 of the United States Code of Federal Regulations, Part 46, Subpart A, also known as the Common Rule.\n\nResearch is a systematic investigation designed to develop or contribute to generalizable knowledge. Let’s break down this definition.\n\nResearch is systematic in that it follows a documented and repeatable methodology.\nInvestigations include research development, testing, and evaluation.\nBy generalizable knowledge, we mean that the investigation intends to advance our scientific understanding. The goal is to go beyond the collection of facts about a particular sample and make conclusions that have relevance for other scholars, practitioners, or policymakers.\n\nFor instance, you might interview parents of young children about their use of mosquito nets. You intend to analyze the transcripts and develop new ideas about the barriers to bed net use that you hope to publish in an academic journal. Other scholars will read this work use it to develop new theories of health behavior and create new interventions that promote bed net use. This is research.\nBut what if a journalist wants to write a feature article about the burden of malaria and interviews a few of the same parents? Is this research?\n\n\nThe Common Rule definition of research excludes journalism activities, public health surveillance activities in support of an order from a public health agency, criminal justice investigations, and operational activities related to national security.\nNo.\nFor one, the journalist might not follow a systematic method for deciding which parents to approach, how to conduct the interviews, or how to synthesize what they learn. Second, the journalist has a different objective. Whereas you wanted to systematically advance our understanding of barriers to bed net use—insights that might apply to different parents in other settings—the journalist intends to inform the public by telling the stories of a few specific parents.\nAnother way that a study can advance scientific understanding is by developing and testing scientific methods and procedures. For instance, a research team might plan a small pilot test to collect initial data that will inform the design of a larger study. In most cases, we’d consider these pre-study activities to be research—even if the team does not intend to publish the results—because the pilot study is part of the knowledge generation process.\nBut here again, intent matters. Let’s say Facebook (Meta) randomly assigns a small percentage of its users to receive email campaign A or campaign B and tracks which campaign generates the most clicks or sales. The company’s objective in this case is to determine which campaign optimizes their marketing spend for conversions, not to say something more general about human perception and behavior. Therefore, it’s not considered research.\nAs we’ll discuss in a later chapter, it’s always a good idea to consult with an Institutional Review Board to determine if your proposed work is considered research (and if it is, whether it falls under policies requiring ethical review)."
  },
  {
    "objectID": "ghr.html#what-makes-research-scientific",
    "href": "ghr.html#what-makes-research-scientific",
    "title": "1  Global Health Research",
    "section": "1.3 What Makes Research Scientific?",
    "text": "1.3 What Makes Research Scientific?\nWhether you’re designing a study that relies on qualitative methods, quantitative methods, or a blend of both, there are several main characteristics of scientific research that apply to global health (King et al., 2021; Leary, 2012):\n\nKing, G. et al. (2021). Designing Social Inquiry: Scientific Inference in Qualitative Research, New Edition. Princeton University Press.\n\nLeary, M. (2012). Introduction to behavioral research methods (6th ed.). Pearson.\n\nthe approach is empirical;\nthe procedures are public;\nthe goal is inference; and\nthe conclusions are uncertain.\n\n\nTHE APPROACH IS EMPIRICAL\n\n\nPeople often ask me questions that, at least in theory, can be answered with data, but I don’t know the answers. In these situations, I like to remove my glasses, stare into middle distance, and say, “That’s an interesting empirical question”. It means, “I don’t know. We should collect some data.”\nScience is built on systematic data collection. That’s what makes it an empirical endeavor. Expert opinion is a form of evidence, but it’s not empirical evidence. Empirical evidence comes from systematic observation, and the method of observation can be quantitative or qualitative. Contrary to what some people believe, empirical is not a synonym for quantitative.\n\n\nTHE PROCEDURES ARE PUBLIC\n\n\n\n\n\n\nA poorly documented Method section. Source: Internet meme, unknown author.\n\n\n\nScientific research uses public methods that can be examined and replicated. A Method section in a scientific paper is like a recipe. If you’ve ever tried to follow a confusing recipe, you can appreciate the importance of good documentation. Your study’s recipe must be clear (well written), thorough (no ‘dash’ of this or that), and shared publicly (not a secret passed down to lab members).\nWe care about complete and transparent reporting in science for several reasons. First, as consumers of research, we rely on authors’ descriptions of their empirical methods to come to our own conclusions about the findings. If research colleagues cannot inspect your methods, they will have little reason to trust your results. Second, no one study should ever rule the day. If the results of your study are robust, another research group should be able to follow the recipe and replicate the findings. When such findings are replicated, we all have more confidence in the results. Third, sharing your methods makes scientific progress possible.\n\n\nTHE GOAL IS INFERENCE\nEmpiricism is essential to science, but science is more than observation. To create generalizable knowledge, you need data and inference. There are two broad categories of inference: (a) descriptive inference and (b) causal inference.\nDescriptive inference is using the data we observe to make conclusions about that which we don’t or can’t observe directly. For instance, you might survey 200 people about their health beliefs, but your real aim is to make conclusions about the broader group. You use the data you have from 200 people to make this inference.\nCausal inference involves a different mental leap where we ask “what if” to make conclusions about causes and effects. Consider the case where we want to know which pill works better to resolve an illness, the red one or the blue one. The fundamental challenge to getting an answer is that we can’t give someone both pills simultaneously. An individual can only take one pill at a time. In this situation we might be able to randomly assign people to each type of pill as a tool for making a causal inference. But frequently, random assignment isn’t possible and we have to use other strategies for asking “what if”.\n\n\nTHE CONCLUSIONS ARE UNCERTAIN\nEvery method has limitations, every measurement has error, and every model is wrong to some extent. Take the estimation of maternal mortality rates as an example. Hogan published estimates for 181 countries (Hogan et al., 2010). Some countries, such as the United States, have vast amounts of data in vital registries that attempt to track all births and deaths. It’s not perfect, so we still estimate the maternal mortality rate using a statistical model.\n\nHogan, M. C. et al. (2010). Maternal mortality for 181 countries, 1980–2008: A systematic analysis of progress towards millennium development goal 5. The Lancet, 375(9726), 1609–1623.\n\n\n\nThis is still several hundred mostly preventable deaths per year. Currently in the U.S., the rate of maternal mortality among non-hispanic Black women is at least 2.5 times higher than the rate for non-hispanic White women.\n\nAs you can see in the left panel of Figure 1.4, the United States has a (relatively) low level of maternal mortality, between about 10-20 maternal deaths for every 100,000 live child births. Compared to some countries, the US has a lot of data points for estimating the level and trend in maternal deaths, so the uncertainty band is narrow.\n\n\n\n\nFigure 1.4: Estimates of the maternal mortality rate in the United States and Afghanistan.\n\n\n\nNow take a look at Afghanistan on the right. Note that the y-axis scale is much larger in the 1000s, reflecting the fact that many more Afghan women die of causes related to pregnancy or childbirth. Next, pay attention to the width of the uncertainty band. It spans a range of more than 3000 deaths. Compare this to a range of fewer than 5 deaths in the US! This is because there are very few data points available to estimate the ‘true’ value in Afghanistan, and these individual data points can differ by more than 1000 deaths.\nThe takeaway message is that there is uncertainty in everything. No single estimate can be considered the answer. Embrace uncertainty and you will become a better scientist."
  },
  {
    "objectID": "ghr.html#what-constitutes-global-health-research",
    "href": "ghr.html#what-constitutes-global-health-research",
    "title": "1  Global Health Research",
    "section": "1.4 What Constitutes Global Health Research?",
    "text": "1.4 What Constitutes Global Health Research?\nGlobal health research brings together scholars and practitioners from many different disciplines to tackle big challenges. Therefore, the methods of these disciplines are the methods of global health research. We can organize the research landscape as shown in Figure 1.8.\n\n\n\n\nFigure 1.5: A research taxonomy.\n\n\n\nResearch is divided into two main categories: basic and applied. Overlapping with applied research is an area of work called “monitoring and evaluation”, or M&E. Let’s examine what constitutes research before turning to M&E.\n\nBASIC RESEARCH\n\n\nBasic research is sometimes referred to as the ‘bench’ in the ‘bench to bedside’ cascade of research needed to take an idea from the lab to a new medical treatment. This is accurate but incomplete. Fields like psychology also conduct basic research into constructs like emotion and violence that seek to expand our understanding. This is not ‘bench science’ as typically imagined, but it’s basic research nonetheless.\nBasic research—also known as “pure” or “blue skies” research—is the pursuit of fundamental knowledge of phenomena. For example, scientists conduct laboratory experiments to understand the parasitic life cycle and how parasites interact with humans at different stages. Another example is the scientific investigation of the properties of cancer cells to better understand how they grow and spread.\nThe information generated by basic science becomes the basis for applied science. Harvard neurobiologist Dr. Rachel Wilson explains this beautifully:\n\n\n\n\n\nWatch Dr. Wilson’s full remarks.\n\n\nThe new therapies of today were the prototypes of yesterday. And the prototypes of yesterday were previously just findings in laboratories, and before that they were just an idea. Unless we have new ideas, we’re not going to have useful therapies. Great new therapies don’t just fall like apples from a tree.\n\n\n\nAPPLIED RESEARCH\nApplied research focuses on specific problems or real-world applications. Much of global health research falls into the applied domain because our mission is to improve health and achieve equity in health for all people worldwide.\n\nClinical Research\nApplied science takes many different forms, including clinical research. Clinical research is a broad field that aims to understand human disease, develop better ways to detect, diagnose, prevent, and treat disease, and to promote health (Roundtable et al., 2002). Table 1.1 lists several domains of clinical research.\n\nRoundtable, I. of M. (US). C. R. et al. (2002). Definitions of Clinical Research and Components of the Enterprise. National Academies Press (US).\n\n\n\n\n\nTable 1.1:  Different types of clinical research \n \n  \n    Domain \n    Description \n  \n \n\n  \n    Treatment/prevention research \n    Test new approaches for preventing or treating illness; includes clinical trials of drugs, biologics, devices, instruments, and behavioral interventions \n  \n  \n    Screening research \n    Develop and evaluate methods for detecting illness risk factors or markers \n  \n  \n    Diagnostic research \n    Develop and evaluate methods for identifying health conditions or illness \n  \n  \n    Genetic studies \n    Examine links between genes and disorders \n  \n  \n    Epidemiological studies \n    Study the patterns, causes, prevalence, and incidence of disease in a population \n  \n  \n    Health services research \n    Study how people access healthcare services, healthcare costs, and outcomes; operations research \n  \n\n\n\n\n\n\n\n\n\n\n\n\nThese phases are broadly similar across different regulatory bodies around the world, including the U.S. Federal Drug Administration (FDA), the European Medicines Agency (EMA), the Central Drugs Standard Control Organization in India, and the National Medical Products Administration in China, among many others. Here’s a brief overview of the phases from the NIH’s National Cancer Institute.\n\nOne type of clinical research is a clinical trial. Table 1.2 lists the phases of trials that drugs, biologics, devices, and instruments complete prior to going to market. In the early phases of drug trials, the objective is to understand how the compound affects the body. What is a safe dose that could be effective? Phase 3 trials put the optimal dose to the test, seeking to determine if the drug “works” and to quantify the size of the effect. Drugs that pass this test are typically approved for use by the governing regulatory body. Once a drug reaches market, Phase 4 trials monitor side effects and efficacy over time.\n\n\n\n\n\nTable 1.2:  Clinical trial phases \n \n  \n    Phase \n    Enrollment \n    Goal \n  \n \n\n  \n    Preclinical research \n     \n    Are there signs that the drug candidate will have an effect in the lab? \n  \n  \n    0 \n    <10 \n    What happens in the body (pharmacokinetics) when a very low dose is administered to human subjects? (optional phase) \n  \n  \n    1 \n    10s \n    Is the drug safe? What is the best dose that balances possible effects with toxicity? \n  \n  \n    2 \n    100s \n    When using this optimal dose, is there any effect of the drug on clinical markers or health outcomes? \n  \n  \n    3 \n    1000s \n    What is the effect of the drug on clinical markers or health outcomes when compared to an existing treatment or placebo in a randomized evaluation? Success at this stage is required for regulatory approval in some countries. \n  \n  \n    4 \n     \n    Are there long-term adverse effects of the drug once it is available on the market? \n  \n\n\n\n\n\n\n\n\n\nClick here to see a list of international trial registries.\n\nIn the United States, the FDA requires that most interventional studies of any regulated products with research sites in the U.S. be registered in the ClinicalTrials.gov trial registry (Food and Drug Administration Amendments Act of 2007, n.d.). Many countries have their own registries, and the World Health Organization maintains a web portal that searches across registries.\n\nFood and Drug Administration Amendments Act of 2007. (n.d.). Pub. L. No. 110-85, 121 Stat. 904 (2007).\n\n\n\nClick here to read about Aunt Debbie’s experience in a Phase 1 poliovirus trial at Duke University.\n\nTrial registries are useful for researchers and patients alike. When my wife’s Aunt Debbie learned she had an aggressive brain tumor, we searched ClinicalTrials.gov and identified several trials recruiting patients for experimental glioblastoma treatments. Debbie decided to take part in a Phase 1 trial that injected modified poliovirus into her tumor. She lived for another five years and helped to advance the science of glioblastoma treatment.\nHealthy volunteers and patients like Debbie are the backbone of clinical research. Their sacrifices, combined with the ingenuity of scientists and research teams, have created thousands of medical breakthroughs. But for every new drug approved, there is a trail of failure (see Figure 1.6). Recent studies estimate that fewer than 15% of candidates entering Phase 1 trials are ultimately approved by the FDA (Wong et al., 2019).\n\nWong, C. H. et al. (2019). Estimation of clinical trial success rates and related parameters. Biostatistics, 20(2), 273–286.\n\n\n\n\nFigure 1.6: Pipeline from basic research to FDA approval.\n\n\n\nThis pipeline does not include studies of behavioral interventions, social programs, or policies because the FDA only regulates drugs, biologics, devices, and instruments. Nevertheless, we still design studies to estimate the efficacy of interventions, programs, and policies, and many of these tests meet the World Health Organization’s definition of a ‘clinical trial’ (WHO, 2020):\n\nWHO. (2020). Clinical trials.\n\nThe International Committee of Medical Journal Editors adopted this definition in 2007 and recommended that all clinical trials should be registered in an appropriate trial registry. Many peer-reviewed health journals follow this guidance and will not accept manuscripts from unregistered studies.\n\n\nany research study that prospectively assigns human participants or groups of humans to one or more health-related interventions to evaluate the effects on health outcomes. (World Health Organization definition)\n\n\n\n\n\n\n\nDr. Pedro Alonso from the World Health Organization explains why this vaccine is a breakthrough in the prevention of malaria.\n\n\n\n\n\n\n\n\n\n\n\nCASE STUDY: Developing the first malaria vaccine\n\n\n\nIn 2021, after more than 35 years of research, the World Health Organization recommended widespread use of a vaccine candidate called RTS,S/AS01, or Mosquirix™, to prevent P. falciparum malaria in children.\nDevelopment of RTS,S/AS01 began in 1984, and soon after, a promising vaccine candidate entered preclinical research. Researchers performed tests on nonhuman subjects to collect data on how well the vaccine worked (efficacy), how much damage it could do to an organism (toxicity), and how the body affected the vaccine (pharmacokinetics).\nClinical research on humans began in 1992. Researchers conducted a Phase 1 safety and immunogenicity trial with 20 adults in The Gambia in 1997. Results suggested that the vaccine did not have any significant toxicity but did produce the expected antibodies.\nSeveral Phase 2 studies conducted over the next decade demonstrated the efficacy of the vaccine against several endpoints. A Phase 2b trial began in Mozambique in 2003 with more than 2,000 children. Each child was randomly assigned to receive 3 doses of RTS,S or a control vaccine. After 6 months, the prevalence of malaria was 37% lower in the treatment group than in the control group. This Phase 2 trial was an important proof-of-concept study.\nThe results of a large Phase 3 trial with more than 15,000 infants and young children in 7 African countries were published in 2015. Children who participated in the study were randomly assigned to 1 of 3 arms: (i) 3 doses of RTS,S and a booster dose at month 20, (ii) 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20, or (iii) 4 doses of a comparator vaccine. RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3 to 4-year period. This Phase 3 trial demonstrated that the treatment was efficacious.\nOn the basis of these results, the European Medicines Agency issued a favorable “European scientific opinion”. This led the health ministries in Ghana, Kenya and Malawi to authorize a pilot study in 2019 to assess the feasibility of administering the required four doses of the vaccine as part of routine childhood immunization programs. After more than 800,000 children were immunized under this program, the WHO recommended that countries with moderate to high transmission adopt the vaccine.\n\n\n\n\n\nTranslational, Implementation, and Policy Research\n\n\n\nBauer and Kirchner make the point that our failure to put good ideas to use is not a new phenonmenon. The British Navy observed in 1601 that citrus cured scurvy on long sea voyages—and collected confirming ‘trial’ evidence in 1747—but it was not until 1795 that using citrus became routine practice.\n\nIt’s a long road from idea to impact (Morris et al., 2011), and most ideas don’t complete the trip (Bauer et al., 2020). Practitioners of translational research point to four key bottlenecks where ideas often stall (see also Figure 1.8):\n\nMorris, Z. S. et al. (2011). The answer is 17 years, what is the question: Understanding time lags in translational research. Journal of the Royal Society of Medicine, 104(12), 510–520.\n\nBauer, M. S. et al. (2020). Implementation science: What is it and why should I care? Psychiatry Research, 283, 112376.\n\nT1: Translation from basic science to clinical research\nT2: Translation from early clinical trials to Phase 3 trials and beyond with larger patient populations\nT3: Translation from efficacy trials (i.e., Phase 3 trials) to real-world effectiveness through implementation research\nT4: Translation from evidence about delivery at scale to the adoption of new policies\n\nTranslational research originally focused on moving from “bench to bedside”, or from basic research in the lab to clinical research with humans (T1), and later expanded to address the challenge of moving from early to late-stage human trials (T2). Today we recognize that other bottlenecks (T3 and T4) prevent good ideas from impacting population health and policy (Woolf, 2008).\n\nWoolf, S. H. (2008). The Meaning of Translational Research and Why It Matters. JAMA, 299(2), 211–213.\n\nDadonaite, B. (2019). Oral rehydration therapy: A low-tech solution that has saved millions of lives. In Our World in Data.\nFor instance, we know that giving children a simple mixture of water, sugar, and salts when they are dehydrated from diarrhea can prevent death in 90% of cases, but only 4 in 10 children receive this life saving oral rehydration therapy [ORT; Dadonaite (2019)]. And each year, several hundred thousand children under five years die from diarrheal diseases.\n\n\n\n\nFigure 1.7: Time from introduction to achievement of public health coverage targets. Source: AVAC Report 2017: Mixed Messages and How to Untangle Them. avac.org/report2017.\n\n\n\nThis is an example of a ‘delivery gap’ or ‘know-do’ gap in global health—the space between discovering what works and delivering the solution at scale (see Figure 1.7 for other examples).\n\n\nWork in this area has evolved independently across several disciplines. In addition to translational research and implementation research, you can find references to dissemination science, diffusion of innovations, quality improvement, knowledge transfer, and learning healthcare systems.\n\nKruk, M. E. et al. (2016). Transforming Global Health by Improving the Science of Scale-Up. PLOS Biology, 14(3), e1002360.\nPolicy and implementation research, or PIR, aims to close these gaps. PIR is the science of scale up (Kruk et al., 2016). It combines implementation research with health policy and systems research.\nImplementation research is the study of strategies for expanding the reach and coverage of tested ideas to improve population health. A common scenario is where intervention research demonstrates that a treatment is efficacious, such as ORT, but take-up is low and slow, thus limiting its impact. The implementation research question is how to best promote its use.\n\n\n\nHow is implementation science different from implementation research? Implementation science develops the frameworks and methods that are used in implementation research. Click here to watch an introductory workshop on implementation science from Northwestern University.\n\nHealth policy and systems research shares the same goal of population impact but takes an even broader view of the systemic and policy factors that can hinder or facilitate scale-up. Take for example a retrospective analysis by Lam et al. that evaluated the impact of policymaking in Uganda on ORT and zinc coverage (Lam et al., 2019). The authors triangulated data from various sources on government actions, distribution of supplies, and treatment with ORT, and estimated that the proportion of young children with diarrhea who received ORT increased 30-fold between 2011 and 2016, from 1% to 30%. Their policy analysis concluded that government actions likely made the difference.\n\nLam, F. et al. (2019). A retrospective mixed-methods evaluation of a national ORS and zinc scale-up program in Uganda between 2011 and 2016. Journal of Global Health, 9(1), 010504.\n\n\n\nMONITORING AND EVALUATION\nAnother area of applied work in global health is monitoring and evaluation (M&E), also known as program evaluation.\n\n\n\n\n\nEconomist and 2019 Nobel laureate Dr. Ester Duflo made a similar argument three decades after Campbell in a great TED Talk on social experiments to fight poverty.\n\n\nEvaluation\nProgram evaluation became commonplace in the United States by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi et al., 2003). But is program evaluation really research?\n\nCampbell, D. T. (1969). Reforms as experiments. American Psychologist, 24(4), 409.\nMethods giant Donald Campbell thought so (Campbell, 1969):\n\nThe United States…should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available.\n\nBut not everyone agrees. Some have argued that program evaluation is really designed for program implementers and funders, and that the messy nature of program implementation requires a loosening of research standards (Cronbach, 1982).\n\nCronbach, L. J. (1982). Designing evaluations of educational and social programs. Jossey-Bass.\n\nRossi, P. H. et al. (2003). Evaluation: A systematic approach. Sage Publications.\nIn their introductory text on evaluation, Rossi et al. (2003) strike a balance in views on this question of whether program evaluation is research. Their answer is perhaps a bit unsatisfying but is arguably true nevertheless: It depends. In essence, program evaluations should be as rigorous as logistics, ethics, politics, and resources permit—and no less. Some evaluations are more rigorous than others and will meet the definition of research (a systematic investigation designed to develop or contribute to generalizable knowledge). For this reason, in Figure 1.8 I represent Monitoring & Evaluation as a block that intersects applied research but extends outside the research boundary.\n\nImpact Evaluations\n\n\n\n\n\nA clinical trialist who conducts RCTs for a living is unlikely to refer to a clinical trial as an ‘impact evaluation’, even though an RCT is a type of impact evaluation. This language is more commonly used by economists and others who study the impact of social sector programs and interventions. This video from the World Bank introduces impact evaluations.\n\nEvaluations can take different forms and serve various purposes. A subset of both applied research and program evaluation is the impact evaluation. An impact evaluation is a study that aims to quantify the causal effect—or impact—of a program or policy on some outcome of interest. There are several research designs that can generate evidence of impact. One example is the randomized controlled trial, or RCT, a mainstay of Phase 2 and Phase 3 clinical trials. Not all impact evaluations use random assignment to make causal inferences, but they all share the goal of making a cause-and-effect claim. As we’ll see in later chapters, the strength of this claim rests on the assumptions of the particular research design.\n\n\n\nMonitoring\nProgram monitoring is concerned with documenting the implementation of programs and interventions. How are resources being used? How many people participate? Does the program reach the intended targets? Not all programs are evaluated (“do they work?”), but most are monitored (“what happened?”) to some degree for accountability to funders. Researchers can use monitoring data to document participants’ exposure to the program and to conduct economic analyses related to program costs and impacts.\nA related activity is the process evaluation. A process evaluation goes beyond monitoring counts and tallies, largely an administrative task, to ask if a program is being delivered as intended. This gets at the question of fidelity of the implementation to the original design. Process evaluations are essential for impact evaluations: If a program fails to show an impact, the next question is why? Did the program fail because the idea or theory behind the program was wrong (theory failure)? Or was the implementation of the program so troubled that there was never a chance for success (implementation failure)?"
  },
  {
    "objectID": "ghr.html#who-funds-global-health-research",
    "href": "ghr.html#who-funds-global-health-research",
    "title": "1  Global Health Research",
    "section": "1.5 Who Funds Global Health Research?",
    "text": "1.5 Who Funds Global Health Research?\n\n\nAccording to Micah et al., “Development assistance for health refers to the financial and non-financial resources that are disbursed through international development agencies to maintain or improve health in low-income and middle-income countries.”\n\nMicah, A. E. et al. (2021). Tracking development assistance for health and for COVID-19: A review of development assistance, government, out-of-pocket, and other private spending on health for 204 countries and territories, 1990–2050. The Lancet, 398(10308), 1317–1343.\nBillions of dollars are spent on global health priorities every year. For instance, the international community disbursed $35-40 billion in development assistance for health yearly from 2010 to 2019. In 2020 this figure jumped to $55 billion because of the COVID-19 pandemic, with the United States accounting for one-quarter of the 2020 total ($14 billion) (Micah et al., 2021). Since the mid-2000s, most of this money has flowed to infectious disease programs (Figure @fig-funding).\n\n\n\n\nFigure 1.8: Development assistance for health by health focus area, 1990 to 2000.\n\n\n\nDevelopment assistance for health includes funded research, but it’s not inclusive of all research in low- and middle-income countries (e.g., does not include domestic spending on research). Nor does it capture research on global health issues in wealthy nations. Inequities in health exist everywhere, so by definition global health research is a broad domain. It’s difficult to put a dollar amount on the enterprise as a whole. A 2013 analysis estimated that the global investment in health research and development (R&D) topped $300 billion (in 2021 dollars) (Røttingen et al., 2013).\n\nRøttingen, J.-A. et al. (2013). Mapping of available health research and development data: What’s there, what’s missing, and what role is there for a global observatory? Lancet (London, England), 382(9900), 1286–1307.\n\nPolicy Cures Research. (n.d.). G-FINDER data portal.\nAccording to this analysis, most investments in health R&D come from private industry (60%), with the rest coming from the public sector (30%) and other sources (10%). Very little money is directed to pharmaceutical products and technologies for global health priorities that disproportionately affect poor countries—less than $4 billion in 2019 (Policy Cures Research, n.d.). In this 2019 accounting, public and philanthropic funders provided almost 90% of the money. The two leading funders were the United States National Institutes of Health (44%) and the Bill and Melinda Gates Foundation (16%), contributing nearly two-thirds of all research dollars between them.\nThe model for investments in R&D for health is funding from the rich, to the rich, (mostly) for the rich."
  },
  {
    "objectID": "ghr.html#who-sets-the-agenda-and-conducts-global-health-research",
    "href": "ghr.html#who-sets-the-agenda-and-conducts-global-health-research",
    "title": "1  Global Health Research",
    "section": "1.6 Who Sets the Agenda and Conducts Global Health Research?",
    "text": "1.6 Who Sets the Agenda and Conducts Global Health Research?\nThis is important context because funders have an outsized role in setting the research agenda (Sridhar, 2012). Outside of industry, researchers depend primarily on grants to fund their work. As nearly all global health research funding comes from governments and philanthropies in high-income countries, the global health research agenda is set in large part by the wealthy (Ii et al., 2018).\n\nSridhar, D. (2012). Who Sets the Global Health Research Agenda? The Challenge of Multi-Bi Financing. PLOS Medicine, 9(9), e1001312.\n\nIi, Y. B. et al. (2018). Advancing equitable global health research partnerships in Africa. BMJ Global Health, 3(4), e000868.\n\nWHO. (n.d.). Investments on grants for biomedical research by funder, type of grant, health category and recipient.\n\nKyobutungi, C. et al. (2021). PLOS Global Public Health, charting a new path towards equity, diversity and inclusion in global health. PLOS Global Public Health, 1(10), e0000038.\nThe global health research agenda is also, in large part, carried out by the wealthy. Only a fraction of 1% of funding for biomedical research goes directly to recipients in low-income countries (WHO, n.d.). In-country researchers too often only play the role of partner or collaborator, rather than principal investigator, if even consulted (Kyobutungi et al., 2021). We’ll revisit these power imbalances, and what can be done to chart a more equitable and inclusive course for global health research, in the next chapter."
  },
  {
    "objectID": "ghr.html#where-is-global-health-research-published",
    "href": "ghr.html#where-is-global-health-research-published",
    "title": "1  Global Health Research",
    "section": "1.7 Where is Global Health Research Published?",
    "text": "1.7 Where is Global Health Research Published?\n\n\n\n\n\nEditors at The Lancet discuss publishing global health research.\n\nGlobal health research is published in medical journals (e.g., The Lancet, JAMA), general science journals (e.g., Science, Nature, PLOS ONE), discipline-specific journals (e.g., The Journal of Immunology, Epidemiology), and disease-specific journals (e.g., AIDS, Malaria Journal). Journals specializing in global health research include The Lancet Global Health, BMJ Global Health, Global Health: Science and Practice, and PLOS Global Public Health."
  },
  {
    "objectID": "collaborations.html#decolonizing-global-health",
    "href": "collaborations.html#decolonizing-global-health",
    "title": "2  Build Collaborations",
    "section": "2.1 Decolonizing Global Health",
    "text": "2.1 Decolonizing Global Health\nToday’s global health emphasizes equity in healthcare access and health outcomes, but we can trace some of its lineage back through international health and tropical medicine to European colonization and ‘colonial medicine’—back to a time when the motivation was protecting colonial rulers and promoting national interests, not equity (Holst, 2020). Global health has come a long way since then—as has the health and wellbeing of people everywhere—but some argue that our approach to collaboration in global health has not fully parted ways with its colonial influences (Abimbola et al., 2020).\n\nHolst, J. (2020). Global health–emergence, hegemonic trends and biomedical reductionism. Globalization and Health, 16(1), 1–11.\n\nAbimbola, S. et al. (2020). Will global health survive its decolonisation? Lancet (London, England), 396(10263), 1627–1628.\n\nWHO. (n.d.). Investments on grants for biomedical research by funder, type of grant, health category and recipient.\n\nKyobutungi, C. et al. (2021). PLOS Global Public Health, charting a new path towards equity, diversity and inclusion in global health. PLOS Global Public Health, 1(10), e0000038.\nThis is because many collaborations in global health have been, and continue to be, an unequal venture, not a true partnership. In the previous chapter, I noted that the global health research agenda is, in large part, set and carried out by the wealthy. Less than 1% of funding for biomedical research goes directly to scientists in low-income countries (WHO, n.d.), and consequently, authors from the Global North are overrepresented in scientific publications (Kyobutungi et al., 2021).\nCalls to ‘decolonize’ global health are not new (Costello et al., 2000), but we’ve witnessed a new urgency in recent years, with students and trainees often leading the way. There is a compelling argument that says the only path to decolonization is radical transformation of institutions (Hirsch, 2021); efforts to improve diversity and inclusion are welcome, but they are not a substitute for dismantling a system that was designed to benefit those with power (Pai, n.d.). Abimbola and Pai frame the aims of the decolonize global health movement as follows:\n\nCostello, A. et al. (2000). Moving to research partnerships in developing countries. Bmj, 321(7264), 827–829.\n\nHirsch, L. A. (2021). Is it possible to decolonise global health institutions? Lancet, 397(10270), 189–190.\n\nPai, M. (n.d.). Decolonizing Global Health: A Moment To Reflect On A Movement. In Forbes.\n\nTo decolonise global health is to remove all forms of supremacy within all spaces of global health practice, within countries, between countries, and at the global level. Supremacy is not restricted to White supremacy or male domination. It concerns what happens not only between people from HICs and LMICs but also what happens between groups and individuals within HICs and within LMICs. Supremacy is there, glaringly, in how global health organisations operate, who runs them, where they are located, who holds the purse strings, who sets the agenda, and whose views, histories, and knowledge are taken seriously.\n\nThe ground is shifting beneath global health. Time will tell whether this moment will lead to institutional reforms, but as individuals we don’t need to wait to reform how we approach collaborations. I suspect student readers wouldn’t have it any other way."
  },
  {
    "objectID": "collaborations.html#team-science",
    "href": "collaborations.html#team-science",
    "title": "2  Build Collaborations",
    "section": "2.2 Team Science",
    "text": "2.2 Team Science\nMost modern science, and global health research in particular, is a never-ending series of group projects. This might be a chilling prospect if you haven’t had good experiences with group work, but I’m here to tell you that team science can be very rewarding and productive given the right environment. As it’s very likely that you’ll join many teams in your global health career, we should discuss what makes a team effective and how to prepare yourself to be a good teammate.\nThe National Cancer Institute of the NIH (U.S.) defines team science as:\n\na collaborative effort to address a scientific challenge that leverages the strengths and expertise of professionals, oftentimes trained in different fields\n\nTeams span the continuum from small investigator-led ‘labs’ to large, highly integrated groups of professionals sharing leadership responsibilities (Bennett et al., 2018). Increasingly, given the complexity of today’s research problems and a trend toward specialization in research expertise and methods, teams bring together people from different disciplines and locations. While the payoff of creating more diverse and skilled teams can be substantial, so are the challenges.\n\nBennett, L. M. et al. (2018). Collaboration team science: Field guide. US Department of Health & Human Services, National Institutes of Health.\nRecognizing the growing importance of team science, researchers established a new field of inquiry in 2006 called the Science-of-Team-Science to study what makes scientific teams click. Many of the findings have made their way into Collaboration and Team Science: A Field Guide, a handbook published by the NIH in 2010 and updated in 2018. In the following sections, I’ll walk you through each of the Field Guide’s top ten takeaways for getting the most out of team science.\n\nVISION\n\nA strong and captivating vision attracts people to the team and provides a foundation for achieving team goals. Shared vision provides a focal point around which a highly functioning team can coalesce.\n\nSome science collaborations are like the 2001 American heist comedy film, Ocean’s Eleven. In the movie, Danny Ocean, played by George Clooney, is released from prison and immediately begins recruiting a hand-selected, nine-member crew to rob three of the biggest casinos in Las Vegas. One-by-one, Ocean pitches them on the idea and secures their cooperation. Ocean’s vision was simple: steal $150 million in cash without getting caught. Once the team was in place, they worked together to hatch a plan.\nOthers collaborations are more like the 1995 film, Apollo 13, about America’s third crewed mission to the moon. Fifty-five minutes into the mission, when the spacecraft was about 330,000 km from Earth, there was an explosion in the service module that caused oxygen to leak into space. The lunar landing was aborted, and the three-man crew moved into the lunar module for a dramatic rescue attempt. The team on the ground at mission control and the astronauts in space had to work together to find a way return the spacecraft to Earth before the astronauts ran out of water and oxygen.\nIn both movies, the teams had a clearly defined and shared vision. As well, team members understood their roles and how they were to contribute to the vision. Research on teams suggests that these factors are key to creating group cohesion. When team members can’t articulate the vision or don’t understand how their contributions fit into the bigger picture, the team’s work often suffers.\nThe stories portrayed in these films differed in an important way with respect to teams, however: in how the vision was created. In Ocean’s Eleven, the vision belonged to Danny Ocean and he made a compelling case to prospective team members. But in Apollo 13, the team co-created the vision in the context of an ongoing collaboration.\n\n\nWe’ll talk more about how to write a compelling Specific Aims document later in the book. It’s a great format for articulating a vision.\nIn this respect, the Ocean’s Eleven model is more prevalent in global health research. A lead scientist—the principal investigator—finds a funding opportunity, writes a short concept note, often called a Specific Aims document, and invites collaborators to join the proposal. There is nothing inherently wrong with this model, but the opening of this chapter should lead us to reflect on this privilege. Namely, whose vision becomes reality in global health is driven by whose vision is funded. Funding decisions tend to favor scholars from the Global North, so visions of the Global South are underrepresented in global health research. One way we can promote change is to build lasting collaborations. When collaborators continue to work together over time, visions can come from anyone on the mission.\n\n\nTEAM EVOLUTION AND DYNAMICS\n\nResearch teams form and develop through critical stages to achieve their highest potential (Forming, Storming, Norming, Performing). A positive team dynamic sustains and further strengthens a research team, enabling it to achieve successful outcomes.\n\nThe most famous framework for understanding how teams evolve is Tuckman’s 1965 Model of Group Development, as described in Field Guide:\n\nForming—The team is established using either a top-down (Ocean’s Eleven) or bottom-up (Apollo 13) approach.\nStorming—Team members establish roles and responsibilities. This process may trigger disagreements or “turf battles” and reveal a reluctance to appreciate the perspectives and contributions of people from different disciplines or training. However, if collegial disagreement is supported and premature pressure to consensus is resisted, people will begin to open up to one another.\nNorming—Team members begin to work together effectively and efficiently, start to develop trust and comfort with one another, and learn they can rely on each other.\nPerforming—The team works together seamlessly, focuses on a shared goal, and efficiently resolves issues or problems that emerge.\nAdjourning or Transforming—Once the team accomplishes its goal, it can celebrate the accomplishment and disband or take on a new problem.\n\nA proven way to strengthen team dynamics is to maintain a collegial environment where members are recognized for their contributions and given opportunities to grow.\n\n\nTRUST\n\nIt is almost impossible to imagine a successful collaboration without trust. Trust provides the foundation for a team. Without trust it is nearly impossible to sustain a collaboration.\n\nNew teams can establish trust by establishing rules and norms. It’s common for teams to co-create a written charter or collaboration agreement that details how their members will work together, resolve disputes, share the workload, and share the credit. This practice is particularly important when teams bring together people from different backgrounds, disciplinary and otherwise, where the existing norms can differ.\n\n\nThis is calculus-based trust. You trust that other people will follow the rules because not doing so has consequences. Another form of trust is competency-based trust: when your reputation or expertise precedes you and folks know that you can be trusted to perform.\nWith time and experience, teams can build deeper forms of identity-based trust based on personal connections and a recognition of shared values. This type of trust is often earned through actions and should not be assumed. Creating and maintaining trust takes substantial effort.\n\n\nCOMMUNICATION\n\nEffective communication within and outside a research team contributes to effective group functioning. It depends on a safe environment where team members can openly share and discuss new scientific ideas and take research into new, previously unconsidered directions as well as ensure that difficult conversations can take place.\n\nTrust and communication are reciprocal. Teams that trust each other communicate openly, and open communication builds trust. However, effective communication can be challenging for new inter-disciplinary teams where members may not share a vocabulary for the science. A recurring theme in this chapter is that successful teams make space for and devote time to establishing common frameworks, including a shared vocabulary.\nThis extends to creating expectations around communication and participation. How often will you meet as a team? What will the format be, and how will you give opportunities for members to be heard? What type of communication is suitable for email vs business messaging apps? When should an email or message thread become a quick phone call or meet-up?\n\n\nCONFLICT AND DISAGREEMENT\n\nConflict can be both a resource and a challenge—a resource because disagreement can expand thinking, add new knowledge to a complex scientific problem, and stimulate new directions for research. A challenge because if it is not handled skillfully, conflict impedes effective team functioning and stifles scientific advancement.\n\nWhenever teams are communicating, conflict and disagreement are possible. In fact, effective teams often encourage the type of critical reflection and constructive criticism that can make conflict and disagreement more likely. This is because they know that conflict is a normal part of collaboration and that, when properly managed, conflict can lead to progress and cohesion. Of course, it’s also true that scientific conflict and disagreement can lead to interpersonal conflict and tension that impairs the team’s ability to achieve its vision.\nTeam leaders play a large role in keeping debate and disagreement productive and in mediating conflicts, but each of us is responsible for our own contributions to the collective dynamic. The Field Guide recommends that we consider the following steps for managing and resolving conflict:\n\nUnderstand the culture and the context of conflict—seek out the meaning of the conflict for yourself and/or the other parties.\nActively listen—assure others you have heard what they said and ask questions to confirm your understanding.\nAcknowledge emotions—they will likely be part of the conflict, but expressing them and hearing them can help lift barriers to resolution.\nLook beneath the surface for hidden meaning—hidden fears, needs, histories, or goals may be the underlying source of the problem.\nSeparate what matters from what is in the way—get away from discussing who is right or wrong and focus more on how to satisfy mutual needs.\nLearn from difficult behaviors—let those experiences help you develop your skills in managing difficult situations and having empathy for and patience with others.\nSolve problems creatively and negotiate collaboratively—this also means committing to action.\nUnderstand why others might be resistant to change—the problem could be an unmet need.\n\n\n\nSELF-AWARENESS AND EMOTIONAL INTELLIGENCE\n\nEmotional Intelligence among team members contributes to the effective functioning of research teams. Self awareness gives people greater control over their own emotional reactions to others, improves the quality of their interactions, and helps build other-awareness.\n\nResearch and science textbooks do not typically emphasize the importance of self-reflection, but the ability to reflect and become self-aware is a core skill required of today’s team science. Someone who lacks self-awareness has limited options for responding to challenging colleagues. People who can take someone else’s perspective and embrace what makes them different have a superpower in team science.\nReflection is also key in collaborations that bring together people from different backgrounds. Whether your work takes you to a new neighborhood or halfway around the world, it’s critical to have the humility to listen and learn.\n\n\nLEADERSHIP\n\nStrong collaborative leadership elicits and capitalizes on the team members’ strengths and is a critical component of team success. Leadership can be demonstrated by every team member, not just the formal leader(s).\n\nI’ve worked with many effective leaders throughout my career. Although they took different approaches to leadership, each person possessed an ability to encourage and motivate individual team members, articulate and maintain a shared vision, and have difficult conversations. The ineffective leaders I’ve encountered were variously disengaged, timid, defensive, or hostile.\nEvery scientific collaboration you join as a trainee is an opportunity to observe and practice leadership. Take notes on what you appreciate in your leaders, and reflect on how poor leadership stifles progress. Spend time developing self-awareness and other-awareness, and approach difficult conversations with openness.\n\n\nMENTORING\n\nMentoring is an indispensable aspect of successful collaboration. A mentor recognizes the strengths of each team member, identifies areas in which newer scientists have the greatest potential to grow, and can help coach people to attain their aspirations. With good mentoring, the development of scientists is synchronous with strengthening team dynamics.\n\nOne of the best investments you can make as a trainee is in finding a good mentor. The return should go far beyond leaving with a good letter of recommendation. A good mentor-mentee relationship can set a strong foundation for a scientific career. If you look at the curriculum vitae—or scholarly record—of successful scientists, you’ll likely see the fingerprints of one or more helpful mentors. Mentors can expose you to new collaborations and resources, help to nurture your ideas, steer you around traps, and teach you the ‘hidden curriculum’ of your scientific discipline that you won’t learn in the classroom.\nFinding a mentor can be a daunting task, especially if you are introverted. Even when you get up the courage to reach out to a potential mentor, your email might go unreturned or come back without an offer to meet. Keep trying, but consider this advice:\n\nAttend scientific talks and department events when possible and introduce yourself\nReach out to the mentor’s other students to learn more about potential opportunities\nKeep your correspondence short with a clear request\nDo you homework—general requests to learn about a person’s work are less effective than a specific statement of how your interests align\n\n\n\nRECOGNITION AND SHARING SUCCESS\n\nIndividual contributions should be recognized, reviewed, and rewarded in the context of a collaboration. Recognition and reward of all team members should be done thoughtfully and fairly in the context of the team and the institution.\n\nMost years on December 10, going back to 1901, the King of Sweden awards several prizes in fields such as medicine, chemistry, and physics, in honor of Swedish inventor Alfred Nobel. Each prize can be given to a single laureate or shared by no more than three laureates.\nIn 2015, the Nobel Prize in Physiology or Medicine was divided between three scientists in recognition of two discoveries that shaped treatments in global health. William Campbell and Satoshi Ōmura shared half of the prize “for their discoveries concerning a novel therapy against infections caused by roundworm parasites”, and Tu Youyou received the other half “for her discoveries concerning a novel therapy against Malaria”.\nThe importance of these discoveries cannot be overstated. Ivermectin (Ōmura and Campbell) and artemisinin (Tu) have helped hundreds of millions of people. These accomplishments deserved to be recognized, and these scientists played significant roles. Tu Youyou even volunteered to be the first human to test her team’s new drug!\nThat said, these individual awards don’t recognize the teams behind this work. Campbell noted as much in his Nobel Lecture, taking a moment to graciously acknowledge his collaborators (Campbell, n.d.):\n\nCampbell, W. C. (n.d.). Nobel lecture.\n\nThere is a question that warrants a slight digression here. In the past few weeks I have often been asked how I felt when I heard that I had won the Nobel Prize. I can say without hesitation that my mind was instantly flooded with two emotions. One was a sense of joy and gratitude. The other was a feeling of sadness—sadness that so many of the people who made this discovery a success could not be named individually. But I represent the research team at Merck & Co., Inc., and in that role I feel honored and grateful beyond imagining.\n\nDespite the growing prevalence of team science, professional recognition and career advancement still depend in large part on individual achievement. This is one reason that it is critical for teams to plan ahead to share recognition and credit.\n\n\nDepending on your line of research, credit might also come in the form of patents.\n\nICMJE. (n.d.). Recommendations for the Conduct, Reporting, Editing, and Publication of Scholarly work in Medical Journals.\nOne of the clearest records of achievement in science is academic publication. Decisions about who receives recognition as an author of a scientific paper can help to make or break careers. On some research teams, the leader decides who deserves to be an author with little to no input from more junior members. This has the potential to lead to resentment and create competition rather than collaboration. Team science advocates encourage a different approach: creating a transparent plan as soon as possible to identify how members will contribute, be recognized, and share the credit. Sometimes this credit will come in the form of authorship. The International Committee of Medical Journal Editors, or ICMJE, recommends four criteria that should determine who should be offered the opportunity (ICMJE, n.d.):\n\n\nA person who meets some but not all of the criteria should be acknowledged in the paper for their contributions.\n\nMakes substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafts the work or revises it critically for important intellectual content; AND\nGives final approval of the version to be published; AND\nAgrees to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\n\nIn recent years it has become more common for authors to include a footnote on the title page of an article indicating that two or more authors should be considered to be co-first authors in recognition of equal contributions.\nIt’s often not enough to decide who gets to be an author. Many teams also have to think through order of authorship. In some fields, including public health and medicine, author order is interpreted to be a reflection of the magnitude of each person’s contributions. For instance, the first author is the lead author and receives the most credit in the eyes of some hiring and promotion committees—“look, they are establishing their ‘independence’”. The principal investigator might take this position or choose instead to go last in the order, taking what is known as the senior author slot. Everyone in between is a middle author. On some teams, the closer to the front you fall, the more work might be expected of you.\n\n\nLarge teams spanning multiple organizations sometimes form study groups and publish under the name of the group. For instance, I’ve published several papers with the Depression Screening Data PHQ Collaboration, or DEPRESSD.\nThink this is too complicated? Then become an economist! Economists often default to alphabetical order. (But if your name is Zarby you might as well stick to medicine.)\n\n\nNAVIGATING AND LEVERAGING NETWORKS AND SYSTEMS\n\nHighly collaborative teams can transcend different organizational structures, extending their reach across and beyond the organization. They often function within the context of multiple and sometimes interconnected systems, and they can help establish strong networks of researchers who together can accomplish more than they could as individuals.\n\nEvery new job requires you to learn the lay of the land, and jobs in global health and academia are no exception. But as team science and interdisciplinary research become more common, this task gets increasingly difficult. It’s part of your team leader’s responsibility to help you understand and navigate the system in which your team operates, but I want to leave you with a few common networks that you’ll find in global health:\nAcademic Institutions: University > School > Department > Center\nInstitutes: Often sit outside of schools and drawing faculty from across the University. Can have one or more centers.\nWorking/Study Groups: A formal or informal group of investigators from one or more institutions that share a common interest or purpose.\nConsortiums: A collection of organizations that are partnering on a specific project or initiative. Consortiums are often led by one organization (the prime) that makes subawards or subcontracts to several partner organizations (subs).\nCommissions/Expert Panels: A temporary group established by a body such as a journal that brings together experts to study an issue and create a report or recommendations.\n\n\n\n\n\n\n\nBONUS PRINCIPLE: Fairness\n\n\n\nA challenge for many collaborators is a lack of time and attention. This is particularly true for senior collaborators who might be juggling several projects, as well as for collaborators of all ranks who are not adequately funded by the project. This routinely happens in global health projects where researchers and practitioners from under-resourced settings are asked to join a team but are not compensated and do not have protected research time from their institutions. It’s common in these situations to spend a portion of every meeting reminding team members of the project goals—the vision.\nWhen your team has busy, but well-funded senior colleagues, creating a vision statement and always orienting the team to this North Star can be a helpful strategy. But even the best crafted documents are no substitute for offering funded time when you are asking resource-constrained colleagues to lend their time and attention to your vision."
  },
  {
    "objectID": "collaborations.html#community-based-participatory-research",
    "href": "collaborations.html#community-based-participatory-research",
    "title": "2  Build Collaborations",
    "section": "2.3 Community-Based Participatory Research",
    "text": "2.3 Community-Based Participatory Research\nThe ultimate form of team science might be community-based participatory research, or CBPR (Israel et al., 1998). In CBPR, research teams join with community members to define the vision, generate knowledge, and create positive change. CBPR requires researchers to share power and credit with people who are most often ‘subjects’, not partners.\n\nIsrael, B. A. et al. (1998). Review of community-based research: Assessing partnership approaches to improve public health. Annual Review of Public Health, 19(1), 173–202.\n\nPuffer, E. S. et al. (2013). Developing a family-based HIV prevention intervention in rural kenya: Challenges in conducting community-based participatory research. Journal of Empirical Research on Human Research Ethics, 8(2), 119–128.\nLet’s look at an example from the HIV prevention literature (Puffer et al., 2013). A US-based team with links to a local non-profit organization in rural Kenya recruited 20 community members—church leaders, healthcare staff, teachers, village chiefs—to join a community advisory committee. This academic-community partnership collaborated on all aspects of the research, starting with workshops designed to share expertise and establish a common vision. The team then planned an assessment of community needs and resources and jointly analyzed and interpreted the data. This was followed by a series of intervention development workshops to create a model of HIV prevention that was informed by science and local priorities. The partnership continued through the design and implementation of a randomized controlled pilot trial to test the intervention."
  },
  {
    "objectID": "ideas.html#find-a-research-problem",
    "href": "ideas.html#find-a-research-problem",
    "title": "3  Develop Research Ideas and Questions",
    "section": "3.1 Find a Research Problem",
    "text": "3.1 Find a Research Problem\nEvery study begins with a motivating research problem. A research problem is your study’s ikigai, its “reason for being”, to borrow a Japanese concept. A research problem should convey a clear reason for being, a clear sense of purpose. Typically, a research problem gets framed as the gap in our knowledge—a gap in the literature.\nA defining characteristic of all research problems is that they are potentially solvable (Leary, 2012). To qualify as a research problem, we must be able to use systematic, public methods to gather and analyze data on the problem. For instance, research has shown that insecticide treated bed nets can prevent malaria infections in kids, but in many places kids are not sleeping underneath bed nets every night. Therefore, a research problem to solve might be that we don’t know how to encourage families to adopt and use this effective method of prevention.\nFinding research problems worth studying becomes easier with experience. Once you’ve studied a field for a few years, you’ll have cultivated a list of journals, conferences, funders, and colleagues whom you follow to keep up with the latest developments and priorities. As you read and learn, it’ll become easier to spot important gaps that you can help to fill. Likewise, if you’re a practitioner or a policy-maker, your accumulated daily observations will generate ideas.\nBut let’s assume that this does not describe you today. So where do YOU start?\nStudents often come to me with a general awareness about a health challenge, not a specific problem or research idea. “I’m interested in mental health”, they’ll say. For students at this early stage of training, I usually encourage them to adopt someone else’s research problem and jump into an ongoing project to gain experience, rather than develop an independent project.\nAside from soaking in experience, there are also steps you can take to expose yourself to new ideas:\n\nRead the health and science sections of major news publications and set alerts for topics you find interesting\nLook up the national and international professional associations for your research area and subscribe to their mailing lists\nAttend research talks in-person or via online webinars hosted by universities, global health research organizations, or professional associations\nFollow experts active on social media\nNetwork with local practitioners and policy-makers who have a front row seat to applied challenges and gaps in our knowledge\n\n\n\n\nNot sure what journals to search? Check out Google Scholar’s Top Publications feature.\n\nAbove all else, start reading the scientific literature. A useful strategy for finding reading material is to visit the websites of the most popular journals in your discipline and browse recent issues. Once you find a few articles of interest, try the following strategy for expanding your search and identifying gaps in the literature worth studying:\n\nFind the keywords: An article’s keywords often make great search terms. Look near the abstract.\nRead the Introduction: A good Introduction will frame gaps in our knowledge of a topic, so pull out a highlighter and get to work.\nReview the Discussion: The Discussion section may also hold new leads. Authors typically use the Discussion to link their study results to the existing literature to demonstrate how the results add to what is already known. A good Discussion section will also include limitations of the current study and might offer ideas for future research.\nTake note of the cited authors and journals: The Reference section holds clues to your next great find. Tools like Google Scholar let you search for these references and discover where else these citations appear.\n\n\n\n\n\n\n\n\nStart with systematic reviews\n\n\n\nA great starting point in the literature is a special type of review called a systematic review. Systematic reviews critically appraise the best available evidence on a topic and synthesize what we know and don’t know. When you start with a systematic review, you let the experts do some of the hard work for you. I’ll teach you about these reviews in a later chapter.\n\n\n\nOnce you start looking, you’ll find research problems everywhere. Your next challenge will be deciding where to direct your attention and energy. I encourage you to favor problems that meet the following criteria:\n\nImportant to you\nContribute to your field\nMeaningful to society\nPotentially solvable using systematic, public methods (discussed above)\n\nFirst, the quality of your work will be better if you believe you are working on something interesting and important. Research projects in global health can have a long life-cycle with ups and downs. A genuine interest in solving the problem can help you get through the low points.\nSecond, when selecting a research problem for study, you should seek to understand from the outset how the answers will contribute to your field. Will your study provide an initial answer to a known problem? Clarify an issue marked by contradictory findings? Support an evolving consensus or provide an important counterpoint to a previously held view? Bring attention to an overlooked issue or apply cross-disciplinary learning to help scholars in your field think differently about a problem? Knowing your study’s purpose will help you to frame your contribution.\nNot everyone will agree on the the third criterion, importance to society. There are many areas in research, especially in the basic sciences, where you can engage in scientifically important work without knowing if or how that work might benefit society one day. I believe global health is different. There are almost endless applied problems to solve, and you can’t work on all of them. Therefore, in asking to you find problems that are meaningful to society, I’m really suggesting that you be intentional. You’ll make many decisions over your career—decisions about training programs, mentors, grant opportunities, collaborators—and each decision has an opportunity cost. Some research problems won’t be a good use of your time and talent (ask me how I know), and we need both to solve big problems. When deciding to pursue new work, stop and ask yourself, “How are we better off if we solve this problem?”\n\n\n\n\n\n\n\nCONSIDER THIS: Whose research problem are you solving?\n\n\n\nThe decolonizing global health movement encourages us to reflect on who gets to define the research agenda. If you’re drawn to research problems that require fieldwork with a community that is not your own, start by soliciting local perspectives and building a team to conceptualize the problem and design the research. See the note about community-based participatory research in the previous chapter for ideas."
  },
  {
    "objectID": "ideas.html#consider-your-contribution-to-theory",
    "href": "ideas.html#consider-your-contribution-to-theory",
    "title": "3  Develop Research Ideas and Questions",
    "section": "3.2 Consider Your Contribution to Theory",
    "text": "3.2 Consider Your Contribution to Theory\ntodo Contribute to your field"
  },
  {
    "objectID": "ideas.html#identify-your-learning-goal",
    "href": "ideas.html#identify-your-learning-goal",
    "title": "3  Develop Research Ideas and Questions",
    "section": "3.3 Identify Your Learning Goal",
    "text": "3.3 Identify Your Learning Goal\nStudies typically seek to describe, explain, or predict. It’s important to understand which goal you are pursuing with your work before you can ask a focused research question and design a study.\n\nDESCRIBE\nEvery study uses an element of description. Let’s say you recruit a sample of 100 people who suffer from the same disorder and conduct a trial to estimate the effect of a new drug on some clinical outcome. When you summarize what you know about these 100 people at the time they were recruited, for instance the average age of the group, you’re describing your sample. Descriptive summaries appear in nearly every research article, usually as ‘Table 1’. But we must distinguish between the use of descriptive statistics—e.g., what is the mean age of these 100 people, the sample—and descriptive research questions.\nDescription is important in every discipline, but possibly no more so than in epidemiology. Epidemiologists ask descriptive research questions about people, place, and time. Fox and colleagues define descriptive epidemiology as follows (Fox et al., 2022):\n\nFox, M. P. et al. (2022). On the need to revitalize descriptive epidemiology. American Journal of Epidemiology.\n\nWho is affected (People)? Where are cases rising and falling (Place)? How are rates changing over time (Time)?\n\n\nDescriptive epidemiology seeks to characterize the distributions of health, disease, and harmful or beneficial exposures in a well-defined population as they exist, including any meaningful differences in distribution, and whether that distribution is changing over time\n\nLet’s break down this definition.\n\nDescriptive epidemiology seeks to characterize the distributions of health, disease, and harmful or beneficial exposures…\n\nTo “characterize the distribution” of something means to describe how often it occurs. The focus of descriptive studies is frequently a health status (often referred to as an outcome), like cancer, or potential exposures, such as environmental carcinogens. But description extends to knowledge, attitudes, practices—what people know, think, and do. For instance, a study might ask, “What percentage of women of reproductive age in Nepal use a modern method of contraception?”\n\nDescriptive epidemiology seeks to characterize the distributions of health, disease, and harmful or beneficial exposures in a well-defined population as they exist\n\n\n\n‘as they exist’ alludes to the notion that if your goal is description, statistical adjustment for confounding might be unnecessary or worse. You likely just want to quantify what you know about a particular group.\nTo ask a good descriptive research question, you must be clear about who makes up your group of interest, or your target population. I expand on this idea in a later chapter on sampling, so for now I’ll simply say that your target population is often a specific group, such as women of reproductive age in Nepal. Typically you’ll recruit a sample of individuals from this group and use the data you collect to make an inference from this limited sample to the larger group—your target population.\n\nDescriptive epidemiology seeks to characterize the distributions of health, disease, and harmful or beneficial exposures in a well-defined population as they exist, including any meaningful differences in distribution, and whether that distribution is changing over time\n\n\n\nDemography is another field that leans hard into description, often focusing on describing trends in births, deaths, marriage, employment, education, and migration.\nThere are often theoretical or programmatic reasons to quantify variation, or differences, in an outcome or exposure across subgroups (people), geographies (place), and time. For instance, you might want to stratify an analysis by ethnicity and race to quantify disparities in outcomes or in access to services. Doing this over time tells you whether conditions are improving, worsening, or staying the same.\nHere’s an example to tie it all together. The descriptive research question is, “What percentage of women of reproductive age in Nepal use a modern method of contraception?”\n\n\n‘Modern’ methods, such as condoms, implants, pills, are distinguished from (and are more effective than) ‘traditional’ methods, such as withdrawal and the rhythm method.\n\nMinistry of Health and Population et al. (2011). Nepal Demographic and Health Survey 2011. Ministry of Health; Populationh.\nThe Demographic and Health Surveys (DHS) Program has answered this question—and many others!—in more than 90 countries by designing, conducting, and analyzing large, nationally representative surveys on population, health, HIV, and nutrition. For instance, in 2011 DHS researchers surveyed a random sample of 10,826 households across Nepal and interviewed 12,674 women between the ages of 15 and 49 about their health behaviors and preferences (Ministry of Health and Population et al., 2011). They estimated that 43% of married women reported using some modern method of contraception. Stratified by age, modern method use was twice as common among married women ages 35 to 49 compared to married women ages 15 to 24 (see Figure 3.1).\n\n\n\n\nFigure 3.1: Descriptive summary of modern method use in Nepal\n\n\n\n\n\nEXPLAIN\nDescription is essential to science and decision-making related to needs and resources. The result from Nepal suggests that more than half of married women of reproductive age were not using a modern method of contraception in 2010. This is a very useful thing to know if you work for the Ministry of Health and are concerned about promoting reproductive health.\nBut you probably also want to go the next step and ask, “Why is uptake relatively low among younger women?”, and “What happens if we introduce a new policy or program intended to promote contraceptive use?”. Can we explain patterns of modern method use?\n\nQualitative Inquiry\nOne way to explore a “why” question is simply to ask people. That’s what Navin Bhatt and colleagues did in Nepal to better understand the reasons for low use of modern methods of contraception among young people (Bhatt et al., 2021). The research team organized six group discussions and conducted 25 interviews with a diverse mix of informants from one village, including teachers, youths, health workers, religious leaders, and government officials. One woman explained her non-use this way:\n\nBhatt, N. et al. (2021). Perceptions of family planning services and its key barriers among adolescents and young people in eastern nepal: A qualitative study. PloS One, 16(5), e0252184.\n\nMy husband works abroad. Last year, when he came home during Dashain (festival), we had (intercourse). Later, he returned to his workplace. Meanwhile, I came to know that I was pregnant, after 3 months. I was shocked to hear that. We already had 3 children; 2 of them were unplanned. I did not have enough information about contraceptive measures in this situation. Had I known about them; I would have used them. —Female, 24 years old\n\nThe authors analyzed transcripts with passages like this and reported that young people face numerous barriers to initiating family planning, including a lack of awareness as described by this woman. They concluded the article with several ideas for designing new interventions (programs) to promote modern method use among this group. We’ll return to qualitative methods of data collection and analysis in a future chapter.\n\n\nCausal Inference\nAnother common method for exploring “why” and “what if” questions is explanatory modeling, or the use of statistical models for testing causal explanations (Shmueli, 2010). This work also falls under the label of causal inference, or what Gelman et al. (2020) define as follows:\n\nGelman, A. et al. (2020). Regression and other stories. Cambridge University Press.\n\nwhat would happen to an outcome y as a result of a treatment, intervention, or exposure z, given pre-treatment information x.\n\nSometimes researchers ask these questions in the context of an experiment that features random assignment to conditions and some active introduction of the treatment, intervention, or exposure. But as Shmueli explains, most often research in the social sciences tests or quantifies associations between variables in non-experimental data.\nLet’s consider two examples of the type of causal inference that we’ll examine in later chapters. Both examples come from Nepal and feature the issue of modern method use.\nIn our first example, Wu et al. (2020) looked at the prevalence of modern contraceptive use in a rural municipality in one of Nepal’s poorest districts before and after a pilot program called Nyaya Health Nepal was implemented. In this program, community health workers conducted home visits with pregnant women and new mothers and offered them contraceptive counseling. The authors reported that modern method use increased from 29% pre-intervention to 46% post-intervention (see Figure 3.2). But as we’ll discuss, this non-experimental design requires some strong assumptions for attributing the observed change to this program.\n\nWu, W.-J. et al. (2020). Community-based postpartum contraceptive counselling in rural nepal: A mixed-methods evaluation. Sexual and Reproductive Health Matters, 28(2), 1765646.\n\n\n\n\nFigure 3.2: Modern contraceptive use by time since delivery. Image source: Wu et al., 2020, CC BY-NC 4.0.\n\n\n\nIn our second example, Pradhan et al. (2019) used a stepped-wedge design to estimate the causal effect of offering postpartum family planning counseling on the proportion of new moms who opt to have an IUD inserted following childbirth. In this design, which you’ll meet more formally in a later chapter, the counseling intervention was turned on in six hospitals in a stepwise fashion. The three hospitals randomly assigned to Group 1 received the intervention first in a staggered start. Approximately six months later, the remaining hospitals began offering the counseling intervention, one after the other. Figure 3.3 demonstrates that rates of uptake appeared to jump in each group after the intervention was introduced. The authors estimated that the intervention increased IUD uptake by 4.4 percentage points [95%CI: 2.8–6.4 pp].\n\nPradhan, E. et al. (2019). Integrating postpartum contraceptive counseling and IUD insertion services into maternity care in nepal: Results from stepped-wedge randomized controlled trial. Reproductive Health, 16(1), 69.\n\n\n\n\nFigure 3.3: Trends in PPIUD Uptake. Note: Approximate intervention start-dates in group 1 and 2 hospitals shown by black and red vertical lines respectively. Image source: Pradhan et al., 2019, CC BY 4.0.\n\n\n\n\n\n\n\n\n\nGLOBAL HEALTH IN PRACTICE: Postpartum family planning\n\n\n\nResearch in low-income countries has estimated that nearly all women want to prevent or space their next pregnancy after they have just given birth, but 6 in 10 women do not start a method of family planning after delivering a baby. Precisely when women become fertile after childbirth varies and is influenced by factors like breastfeeding, but in most cases ovulation returns before family planning is started. This puts women at risk of getting pregnant again before they want to. One option is to have an intrauterine device, or IUD, inserted immediately after delivery of the placenta or within the first month postpartum.\n\n\n\n\n\nPREDICT\nThe third common learning goal is to use data and algorithms to predict new or future observations (Shmueli, 2010). You might know this as the domain of artificial intelligence, machine learning, or deep learning, but I’ll stick with the more general term of prediction modeling.\n\nShmueli, G. (2010). To explain or to predict? Statistical Science, 25(3), 289–310.\nFunding for prediction modeling in medicine has exploded in the last decade. Data scientists who might have once worked hard for incremental gains in the quality of Netflix’s movie recommendations are now searching for ways to teach computers to outperform radiologists in detecting pathology. And increasingly, the same tools are being applied to solving public health challenges. The COVID-19 pandemic offers numerous examples.\nFor instance, organizations like the Institute for Health Metrics and Evaluation, or IHME, raced to forecast cases, hospitalizations, and deaths in the early days of the pandemic (IHME, n.d.). Hundreds of other teams developed models for predicting who was most at risk for developing COVID-19 or for predicting the prognosis of COVID-19 patients. This interest in solving applied problems is encouraging, but according to the authors of a massive ‘living’ systematic review of these models (updated through 2022), there is a lot of room for improvement before the next pandemic (Wynants et al., 2020):\n\nIHME. (n.d.). COVID-19 Projections. In Institute for Health Metrics and Evaluation.\n\nWynants, L. et al. (2020). Prediction models for diagnosis and prognosis of covid-19: Systematic review and critical appraisal. BMJ, 369, m1328.\n\nPrediction models for COVID-19 entered the academic literature to support medical decision making at unprecedented speed and in large numbers. Most published prediction model studies were poorly reported and at high risk of bias such that their reported predictive performances are probably optimistic.\n\nReturning to the topic of contraception, prediction models might help us to address a key public health challenge: more than 1 in 3 women in low- and middle-income countries stop using contraception within 12 months of starting, putting them at risk for unintended and mistimed pregnancies. What if there was a way to accurately predict who would eventually discontinue and offer them additional support when starting a new method of contraception?\nRothschild et al. (2020) took on this question, recruiting a cohort of more than 700 Kenyan women who did not want to get pregnant and following up with them over the course of 24 weeks to see who remained on a method and who stopped. The authors used data from 75% of the women to develop a discontinuation risk score (based on things like method choice, education, marital status) and reserved the remaining 25% to validate their new tool. The authors found that discontinuation was almost 2-6 times higher among women labeled as high risk vs low risk, but the positive predictive value of the score was only 31%. This means that most women labeled by the tool as “high risk” did not actually discontinue. In practice, mislabeling women as high risk for discontinuation would be unlikely to cause harm, but it could direct scarce resources to people who don’t need additional support.\n\nRothschild, C. W. et al. (2020). A risk scoring tool for predicting Kenyan women at high risk of contraceptive discontinuation. Contraception, 2, 100045.\n\nBy splitting the sample 75/25, the authors separated model development and model validation. This is a key idea in prediction modeling: testing your model on unseen data.\n\nThese examples are evidence that there is a great opportunity for public health-minded data scientists to make substantial contributions to the development, validation, and implementation of prediction models to improve lives and support health systems.\n\n\n\n\n\n\nWATCH OUT: I predict confusion\n\n\n\nIn papers that report regression results, you will often find references to predictor variables. These are the independent variables that ‘predict’ the outcome, or dependent variable. You can use regression to predict future (or unseen) values, but regression as often applied in the social sciences is mostly just fancy averaging with cross-sectional data obtained at one time point."
  },
  {
    "objectID": "ideas.html#specify-a-research-question",
    "href": "ideas.html#specify-a-research-question",
    "title": "3  Develop Research Ideas and Questions",
    "section": "3.4 Specify a Research Question",
    "text": "3.4 Specify a Research Question\nA good research question addresses a research problem—a gap in our knowledge—and is answerable. ‘Answerable’ does not mean easy to answer, just possible to answer (Leary, 2012). The question, “Do mosquitoes have an afterlife?”, represents a gap in our knowledge, but we don’t have any empirical means for finding an answer. It’s an interesting question to pose to my kids to probe their imagination, but it’s a poor research question because it’s not answerable.\n\nLeary, M. (2012). Introduction to behavioral research methods (6th ed.). Pearson.\nWriting good research questions takes practice. Here are two acronyms to help you get started: FINER and PICO.\n\nFINER\nFINER stands for Feasible, Interesting, Novel, Ethical, and Relevant (Hulley et al., 2007).\n\nHulley, S. et al. (2007). Getting started: The anatomy and physiology of clinical research (S. Hulley et al., Eds.; Third, pp. 3–15). Lippincott Williams & Wilkins.\nFeasible: Some research questions will take a long time to answer, cost too much, require too many participants, demand skills or equipment that you do not have, or will be too complex to implement. A good research question is not just answerable, it’s feasible for YOU to answer it with the resources currently at your disposal.\nInteresting: Research requires funding and effort. If you don’t ask a sufficiently interesting question, you won’t get funding. If you manage to get funding but lose interest in the question, you might not finish the work. Unlike some other domains, global health research tends to have long timelines, and it’s important to work on things you’ll find interesting over the long term.\nNovel: Replication is an important part of science, but the majority of global health funding goes to research that asks new and interesting questions. Therefore, you should prioritize answering questions that fill a gap in our knowledge.\nEthical: Google’s motto used to be, “don’t be evil”. This is a minimum bar for research with human subjects. You have a responsibility to ensure that the questions you ask and the methods you employ in search of answers shield participants from harm.\nRelevant: In addition to being interesting, a research question should also be relevant to science and society. The answer should move your field forward in some way. Making this determination requires a thorough review of the literature and conversations with senior colleagues.\n\n\nPICO\nPICO stands for Population, Intervention, Comparison, and Outcome (see Table 3.1).\n\n\nIt’s a useful framework for asking clinical questions in particular, but it might not fit your use case. You’ll also see references to PICOT with the addition of time.\n\n\n\n\n\nTable 3.1:  PICO \n\n  \n    P \n    Patient, Population, or Problem \n  \n  \n    I \n    Intervention, Prognostic Factor, or Exposure \n  \n  \n    C \n    Comparison \n  \n  \n    O \n    Outcome \n  \n\n\n\n\n\n\nLet’s use PICO to develop a research question about the efficacy of mosquito bed nets in preventing malaria.\nP: Typically in global health research questions we define the target population or a problem that needs solving. I like to include both. For instance, we might ask a question about malaria infections (problem) among children under 5 years of age living around the Lake Victoria basin in Kenya (population).\nI: This can refer to an intervention, exposure, or prognostic factor. An example of an intervention for preventing malaria infection is the provision of insecticide-treated bed nets. If you’re not conducting an intervention trial, you might instead be interested in an exposure that increases the risk of an outcome, such as traveling to a malaria endemic region, or a prognostic factor that predicts mortality, such as neurological dysfunction in severe cases of malaria.\nC: The choice of a comparator is a critical aspect of your research question. Are you interested in comparing bed nets to another intervention, such as indoor spraying? Comparing insecticide treated bed nets to untreated bed nets? Bed nets compared to nothing? Or maybe your research question is descriptive and has no comparator. For this example, let’s say that we’re interested in the effect of treated bed nets compared to untreated nets.\nO: Outcomes are the specific targets of our investigation. For instance, we might be interested in estimating the impact of insecticide treated bed nets on parasitaemia, or the presence of malaria parasites in the blood.\nWe can combine all of these elements into a single research question:\n\nAmong children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide-treated mosquito nets more effective than untreated nets at preventing parasitaemia?"
  },
  {
    "objectID": "ideas.html#develop-a-hypothesis-maybe",
    "href": "ideas.html#develop-a-hypothesis-maybe",
    "title": "3  Develop Research Ideas and Questions",
    "section": "3.5 Develop a Hypothesis (Maybe)",
    "text": "3.5 Develop a Hypothesis (Maybe)\ntodo\nregister generating"
  },
  {
    "objectID": "statinference.html#two-major-approaches-to-statistical-inference",
    "href": "statinference.html#two-major-approaches-to-statistical-inference",
    "title": "4  Statistical Inference",
    "section": "4.1 Two Major Approaches to Statistical Inference",
    "text": "4.1 Two Major Approaches to Statistical Inference\n\n\n\nThere are two main approaches to statistical inference: the Frequentist approach and the Bayesian approach. A key distinction between the two is the assumed meaning of probability.\nBelieve it or not, smart people continue to argue about the definition of probability. If you are a Frequentist, then you believe that probability is an objective, long-run relative frequency. Your goal when it comes to inference is to limit how often you will be wrong in the long run.\n\n\n\nIf you are a Bayesian, however, you favor a subjective view of probability that says you should start with your degree of belief in a hypothesis and update that belief based on the data you collect.\nI’ll explain what this all means, but before we get too far along, please think about what YOU want to know most:\n\nthe probability of observing the data you collected if your preferred hypothesis was not true; or\nthe probability of your hypothesis being true based on the data you observed?"
  },
  {
    "objectID": "statinference.html#frequentist-approach",
    "href": "statinference.html#frequentist-approach",
    "title": "4  Statistical Inference",
    "section": "4.2 Frequentist Approach",
    "text": "4.2 Frequentist Approach\n\n\n\nOpen just about any medical or public health journal and you’ll find loads of tables with p-values and asterisks, and results described as “significant” or “non-significant”. These are artifacts of the Frequentist approach, specifically the Neyman-Pearson approach.\n\n4.2.1 HOW IT WORKS\nIn the Neyman-Pearson approach, you set some ground rules for inference, collect and analyze your data, and compare your result to the benchmarks you set. Inference is essentially mindless automatic once you set the ground rules. To explore how it works, let’s return to the trial of the Healthy Activity Program for depression discussed in Chapter 7. As a reminder, (patel:2016?) conducted a randomized controlled trial to answer the following research question:\n\n\n\n\nAmong adults 18–65 years of age with a probable diagnosis of moderately severe to severe depression from 10 primary health centres in Goa, India, does enhanced usual care plus a manualised psychological treatment based on behavioural activation delivered by lay counselors—the Healthy Activity Program, or HAP—reduce depression severity compared to enhanced usual care alone? [not a direct quote]\n\n\nStep 1: Specify two hypotheses\nIn hypothesis testing, we set up two precise statistical hypotheses: a null hypothesis (H0) and an alternative hypothesis (H1). Most often the null hypothesis is stated as the hypothesis of no difference:\n\n\n\n\nH0: μ1 = μ2 (or μ1 - μ2 = 0), meaning there is no difference in average depression severity between the group that was invited to receive HAP plus enhanced usual care compared to the group that only received enhanced usual care\n\n\n\n\nA “two-tailed” alternative hypothesis states that there is a difference, but does not specify which arm is superior:\n\nH1: μ1 ≠ μ2 (or μ1 - μ2 ≠ 0), meaning that the average difference between the groups is not zero.\n\nIt might seem confusing because H1 is the hypothesis we talk about and write about, but it’s actually the null hypothesis (H0) that we test and decide to reject or accept (technically, ‘fail to reject’).\n\n\n\nThe null is where statistical inference happens. The Frequentist rejects or retains the null hypothesis, but does not directly prove the alternative. They simply decide whether there is sufficient evidence to convict (i.e., reject) the null.\n\n\nStep 2: Imagine a world in which H0 is true (i.e., innocent)\nThis is where things get a bit weird. Frequentists subscribe to the long run view of probability. In this framework, you have to establish a collective, a group of events that you can use to calculate the probability of observing any single event (Dienes, 2008). Your study is just one event. You can’t determine the probability of obtaining your specific results without first defining the collective of all possible studies.\nI know what you’re thinking. This seems nuts. In my defense, I said it gets a bit weird. Hang with me though. The good news is that you do not have to repeat your study an infinite number of times to get the collective. You can do it with your imagination and the magic of statistics.\nSo put on your wonder cap and imagine that you conducted thousands of experiments where H0 was true. Yeah, that’s right, I’m asking you to picture running your study over and over again with a new group of people, but the truth is always that the intervention does not work. The aim of this thought exercise is to establish what type of data we’re likely to find when the null hypothesis is TRUE.\n\n\n\nTo kick things off, consider this hypothetical accessible population of 6,705 depressed people who are eligible for the HAP trial (see Figure @ref(fig:happop)). In each of your imagined studies, you’ll recruit a new sample of 332 patients from this population. Let’s assume that the baseline level of depression severity among these patients ranges from a score of 10 to 63 on the depression instrument you’re using, the Beck Depression Inventory-II (BDI-II). I say “assume” because you’ll never get to know more than 332 of these 6,705 patients. We know they exist, but we don’t know the true population size or the true average level of depression among this group.\n\n\n\n\n\nHypothetical population of depressed patients. We get the center and spread of the distribution from the authors’ trial protocol that reports that in a prior study the control group BDI-II mean was 24.5, and the standard deviation was 10.7 (Patel et al., 2014). (patel:2016?) did not use the BDI-II to determine eligibility for the HAP trial—just the PHQ-9—but we can assume that no participants in the trial had a BDI-II score of less than 10 at enrollment.\nPatel, V. et al. (2014). The effectiveness and cost-effectiveness of lay counsellor-delivered psychological treatments for harmful and dependent drinking and moderate to severe depression in primary care in India: PREMIUM study protocol for randomized controlled trials. Trials, 15, 101.\n\n\n\n\n\nNext, imagine that each orange dot in Figures @ref(fig:happop) and @ref(fig:hapsamp) represents 1 of the 332 patients you recruited into the actual study you conducted. You do know each one of these people, you do measure their depression level at baseline, and you do calculate the sample mean. This is the only sample you’ll see as the researcher in real life, but let’s pretend that this sample was #7,501 out of an imaginary set of 10,000 possible studies.\n\n\n\n\n\nHere’s the sample of 332 patients you recruit into your study. (patel:2016?) did not actually administer the BDI-II at baseline, but we can pretend. Why a sample of 332? (patel:2016?) tell us they planned to recruit 500 participants but expected to lose track of 15% of them. That’s a planned sample size of 425. But they also tell us that the study design was a bit more complicated, so the planned effective sample size was 425/1.28 = 332.\n\n\n\n\n\n\n\nYour trial design is a randomized controlled trial, so you randomize these 332 people to the treatment group (HAP plus enhanced usual care) or the control group (enhanced usual care only). As shown in Figure @ref(fig:haprand), you allocate 1:1, meaning that 50% (166) patients end up in the treatment arm, and 50% in the control arm.\n\n\n\n\n\nDistribution of baseline BDI-II scores by study arm. Even with random assignment the group means are not 100% identical at baseline. This is normal. As the sample size gets bigger, randomization produces better balance.\n\n\n\n\nNow I’d like you to imagine that your intervention is NOT superior to enhanced usual care (and vice versa). A few months after the treatment arm completes the program, you reassess everyone in the study and find that the average depression score in both groups decreases by 5 points (see Figure @ref(fig:hapate)). Since the baseline mean for the HAP group was 24.2, and the enhanced usual care arm mean was 28.0, the endline means shift down by 5 points to 19.2 and 23.0, respectively. The effect size—in this example the average post-intervention difference between groups—is 19.2 - 23.0 = -3.8. The instrument you’re using to measure this outcome, the BDI-II, ranges from a possible score of 0 to 63. So an absolute difference of 3.8 points is small, but it’s not 0.\n\n\n\n\n\nDistribution of endline BDI-II scores by study arm. Here’s what it might look like if depression severity reduces by 5 points in both groups.\n\n\n\n\nStudent in the first row raises hand: If the null hypothesis of no difference is actually true, why isn’t every study result exactly zero?\nIt’s a good question. The reason is this: there’s error in data collection and sampling error that comes from the fact that we only include a small fraction of the population in our study samples. Therefore, we might get a result that is near 0—but not exactly 0—even if the null hypothesis is really true.\nHopefully Figure @ref(fig:hapnull) will make this point clear. To help you imagine a world in which H0 is true, I drew 10,000 samples of 332 people from the simulated accessible population of ~6700, randomly assigned each person to a study arm, and calculated the treatment effect for the study if everyone’s depression score reduced by exactly 5 points. This figure plots all 10,000 study results.\n\n\n\n\n\n10,000 simulated results when there’s no effect. Unlike the previous figures, this time the dots are study results, not people.\n\n\n\n\nHere’s the key thing to observe: I simulated 10,000 studies where everyone always improved by an equal amount—i.e., no treatment effect—but there is NOT just one stack of results piled 10,000 high at exactly zero. Instead, the results form a nice bell shaped curve around 0.\n\n\n\nThis is the central limit theorem at work (aka, the magic of statistics). When plotted together, the results of our imaginary study replications form a distribution that approximates a normal distribution as the number of imaginary replications increases. This is fortunate because we know useful things about normal curves. For instance, we can find any study result on the curve and know where it falls in the distribution. Is it in the fat part around 50%? Or is it a rather extreme result far in the tails at 1% or 2%?\nTo conclude Step 2, let’s think back to the Frequentist definition of probability that relies on having some collective of events. The plot in Figure @ref(fig:hapnull) represents this collective. Frequentists can only talk about a particular result being in the 1st or 50th percentile of results if there is a group of results that make up the collective. Without the collective—the denominator—there can be no probability.\nOf course in reality, no Frequentist repeats a study over and over 10,000+ times to get the collective. They rely on the central limit theorem to imagine the most plausible set of results that might occur when the null hypothesis is really true. This statistically derived, but imaginary, collective is fundamental to the Frequentist approach.\n\n\nStep 3: Set some goal posts\nSkeptical student: OK, I get that a study result does not have to be exactly zero for the null to be true. But how different from zero must a result be for Frequentists to reject the null hypothesis that the treatment had no effect?\n\n\n\nIn the Frequentist approach, you decide if the data are extreme relative to what is plausible under the null by setting some goal posts before you conduct the study. If the result crosses the threshold, it’s automatically deemed “statistically significant” and the null hypothesis is rejected. If it falls short, the null hypothesis of no difference is retained. This threshold is known as the alpha level. For Frequentists, alpha represents how willing they are to be wrong in the long run when it comes to rejecting the null hypothesis. Traditionally, scientists set alpha to be no greater than 5%.\nReturning to our simulated results in Figure @ref(fig:hapnull), you can see that I drew the goal posts as dotted red lines. They are positioned so that 5% of the distribution of study results falls outside of the lines, 2.5% in each tail. This is a two-tailed test, meaning that we’d look for a result in either direction—treatment group gets better (left, negative difference) OR worse (right, positive difference).\n\n\n\nYou can also draw a single goal post that contains the full alpha level (e.g., 5%). I show this in Figure @ref(fig:hapnull3) below. This is appropriate when you have a directional alternative hypothesis, such as the treatment mean minus the control mean will be negative. In the HAP example, this would indicate that the treatment group ended the trial with a lower level of depression severity.\n\n\nStep 4: Make a decision about the null\nWith the goal posts set, the decision is automatic. Your actual study result either falls inside or outside the goal posts, and you must either retain or reject the null hypothesis. I’ll say it again: statistical inference happens on the null.\nGoing back to our example, imagine that you conducted study #7,501 of 10,000. When you collected endline data, you found a mean difference between the two arms of -3.8 as shown in Figure @ref(fig:hapnull3). What’s your decision with respect to the null? Do you retain or reject?\n\n\n\n\n\n10,000 simulated results when there’s no effect. Result of study #7501 falls outside of the goal post, so the null hypothesis is rejected.\n\n\n\n\nReject! A difference of -3.8 falls outside of the alpha level you set at 5%. Therefore, you automatically reject the null hypothesis and label the result “statistically significant”.\nBut there’s something else we know about this result, the raw effect size of -3.8: it falls at the 1st percentile of our simulated collective. It’s an extreme result relative to what we expected if the null is true. We’d say it has a p-value of 1%. The p-value is a conditional probability. It’s the probability of observing a result as big or bigger than our study result (-3.8)—and here’s the conditional part—IF THE NULL HYPOTHESIS IS TRUE.\nSmart student mutters to self while taking notes: Why does he keep saying “if the null hypothesis is true” like some lawyer who loves fine print?\n\n\n\n\n\n\nI heard that! And good thing, because it’s important to say this again: when it comes to inference, we never know the “truth”. We do not know if the null hypothesis is actually true or false. That’s why the p-value is a conditional probability. A p-value is the probability of observing the data if the null hypothesis is true P(D|H), NOT the probability that the null hypothesis is true given the data P(H|D).\nLet that sink in. The p-value might not mean what you want it to mean.\nFurthermore, since we can’t know the truth about the null hypothesis, it’s possible that we make the wrong decision when we reject or retain it.\nIf the null hypothesis is really true—and that’s what I simulated—we made a mistake by rejecting the null. We called the result statistically significant, but this is a false positive. Statisticians refer to this mistake as a Type I error, though I think the term false positive is more intuitive since we’re falsely claiming that our treatment had an effect when it did not.\nThe good news is that in the long run we will only make this mistake 5% of the time if we stick to the Frequentist approach. The bad news is that we have no way of knowing if THIS EXPERIMENT is one of the times we got it wrong.\nThe other type of mistake we can make is called a Type II error—a false negative. We make this mistake when the treatment really does have an effect, but we fail to reject the null. We’ll talk more about false negatives—and power—in Chapter 14.\n\n\n\n\n\nPossible outcomes of inferential statistics.\n\n\n\n\n\n\nYou are significant, even if your p-value is not\n\n\n\nCheck out Figure @ref(fig:hapgif) for an animated look at the simulation discussed above. It flips through 20 of the 10,000 studies I simulated to show no treatment effect. Watch for your study, #7501. It’s the only one of the set of 20 that rejects the null.\n\n\n\n\n\nHere’s an animated gif of 20 draws from a simulation of 10,000 studies where the treatment has no effect. Watch for the one study that rejects the null.\n\n\n\n\n\n\n\n4.2.2 PUBLISHED EXAMPLE: HAP TRIAL RESULTS\n\n\n\n\n\nParticipant flow diagram, HAP trial. Source: (patel:2016?).\n\n\n\n\n(patel:2016?) enrolled and randomized 495 depressed adults in Goa, India to receive the HAP intervention plus enhanced usual care or enhanced usual care alone. They reported 2 post-randomization exclusions and 5% loss to follow-up, for a final intent-to-treat analysis sample of 493 (245 treatment, 248 control).\nFigure @ref(fig:hapresults) shows the primary and secondary outcomes. We’re interested in the BDI-II score measured 3-months after the treatment group completed the HAP intervention (red outline). After adjusting for the study site (PHC) and participants’ depression scores at baseline (measured via a different instrument, PHQ-9), the authors find that HAP reduced depression severity by an average of 7.57 points (BDI-II ranges from 0 to 63).\nIf you look back to Figure @ref(fig:hapnull3), you’ll see that a difference of this size is off the chart. So you should not be surprised that (patel:2016?) report a p-value of < 0.0001. If the null hypothesis is true—meaning that there really was no treatment effect—you would expect to get a difference at least this big less than 0.01% of the time.\n\n\n\n\n\nHAP trial primary and secondary outcomes. Source: (patel:2016?).\n\n\n\n\n\n\n4.2.3 CAVEATS AND CONSIDERATIONS\nKeep these in mind when you review manuscripts or write up your own Frequentist analysis.\n\nA. Statistical significance does not imply practical or clinical significance\n\n\n\nTo say that a result is “statistically significant” tells the world only that the result was sufficiently surprising to reject the null hypothesis (based on your definition of surprising—alpha—and what data you could expect to see if the null hypothesis is true). Statistical significance does not imply any type of practical or clinical significance. It does not mean that your finding is “significant” in the colloquial sense of “meaningful” or “important”.\n\n\n\nIf statistical significance is your jam, just (plan to) get more fruit. For reasons that will become clear in Chapter 14, simply increasing the sample size will shrink the p-value. With enough resources, you could conduct a study that finds a new intervention “significantly” reduces average systolic blood pressure (the top number) from 120 to 119, p < 0.05. But who cares? Whether this effect size is clinically or practically significant is completely separate from whether you have enough data to say that the effect is not zero. This is why you should always report effect sizes, not just p-values. More on this in a moment.\n\n\nB. You can peek at your data, but you have to pay in alpha\n\n\n\nMore data = more precision = lower p-values = statistical significance, but be careful. In the Neyman-Pearson approach that takes the long view on probability, you cannot look at your data, find that p = 0.052, and recruit more participants just to push down the p-value—without paying a statistical price.\n\n\n\nTo be clear, there are lots of reasons why it could make sense to take an interim look at the data before the end of your study. Chief among them is participant safety. If you are testing a new intervention that could cause harm, you would likely create a committee and a plan for looking at your data at various points of the study. But you have to pay the price for peeking. When you take interim peeks at the data you have to move the goal posts outward. Rather than an alpha of 0.05, you might have to raise the bar to 0.01, for instance. There is no free lunch.\n\n\nC. No, your p-value is not “trending toward significance”\n\n\n\nIn the Neyman-Pearson approach, results are either above the alpha level you set or below it, statistically significant or non-significant. You may not modify the word “significant” with language like “trending” or “marginally” or “approaching”. If you set alpha to 0.05 so your long term error rate is 5%, a p=0.052 for a particular study is non-significant. p-values are not a measure of the strength of your evidence (Dienes, 2008).\n\nDienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\n\nD. A non-significant finding does not equal “no effect”\n\n\n\nA common mistake is to infer from a p-value of 0.052 that there is “no effect” or “no difference”. As Lakens explains in his Coursera course on statistical inference, all you can take away from a p-value greater than your alpha threshold is that the result is not sufficiently surprising if the null hypothesis is true. It’s possible that the effect is small—too small for you to detect with a small sample size. Scroll to the beginning of this chapter to see an example of this mistake in print.\n\n\n\n4.2.4 CRITICISMS\n\n\n\nThe Frequentist approach dominates the literature, but it’s not without its critics. Lots of critics, in fact. More than 800 scientists recently signed on to a proposal to abandon statistical significance (Amrhein et al., 2019), and some journals have banned reporting p-values. Other researchers have proposed keeping significance testing, but redefining statistical significance to a higher bar, from an alpha of 0.05 to 0.005 (Benjamin et al., 2018). The misuse and misunderstanding of p-values and statistical significance is so widespread that the American Statistical Association issued a statement reminding scientists of what a p-value does and does not tell us (Wasserstein et al., 2016).\n\nAmrhein, V. et al. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305–307.\n\nBenjamin, D. J. et al. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6–10.\n\nWasserstein, R. L. et al. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70(2), 129–133.\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of Science, 34(2), 103–115.\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. Psychological Reports, 66(1), 195–244.\nFrustration with null-hypothesis significance testing, or NHST, is not new, however. People like Paul Meehl have been warning us for decades (Meehl, 1967, 1990). So why the recent hubbub? Come closer. No, closer.\nWe’ve realized that science is in a crisis—a replication crisis. And NHST is partly to blame.\nYou might remember from Chapter 1 that replication is the process of repeating a study to see if you get the same results. Most methods books will tell you that replication is core to the practice of science, but until recently replication studies in the health and social sciences were very rare. Strong incentives for novelty keep researchers, funders, and journals looking ahead, not behind. But a 2011 paper concluding that ESP is real—yes, that ESP, extrasensory perception—was the breaking point for some. This paper led to some serious soul searching, followed by real efforts to repent, reform, and replicate.\nOver the course of 10 years, social psychologist Daryl Bem ran 9 experiments, ~100 participants per experiment, in which he asked people to respond to experimental stimuli before the stimuli were presented to test his ideas about precognition and premonition (Bem, 2011). Some of the experiments had college students guess which of two curtains on a computer monitor was hiding an erotic image. After the student responded, the answer was randomly assigned.\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. 100, 407–425.\nAll but 1 of these 9 experiments produced statistically significant results supporting ESP. Many in the scientific community lost their minds. A 2017 Slate piece by Daniel Engber provides a fascinating look at the scientific reaction to the ESP paper when it was published in Journal of Personality and Social Psychology .\n\n…for most observers, at least the mainstream ones, the paper posed a very difficult dilemma. It was both methodologically sound and logically insane. Daryl Bem had seemed to prove that time can flow in two directions—that ESP is real. If you bought into those results, you’d be admitting that much of what you understood about the universe was wrong. If you rejected them, you’d be admitting something almost as momentous: that the standard methods of psychology cannot be trusted, and that much of what gets published in the field—and thus, much of what we think we understand about the mind—could be total bunk.\n\n\n\n\nThe conclusion for many skeptics of Bem’s work was that this paper followed the conventions of the time, but the conventions were flawed. Bem’s paper was just a symptom of the disease—QRP.\n\n\n\n\n\nIt’s almost like you need a p-value less than 0.05 to get published.\n\n\n\n\nOne notorious QRP goes hand in hand with NHST: p-hacking. You see Reader, data analysis is a garden of forking paths. Even simple analyses require the analyst to make lots of decisions. There are many pathways one can take to get from question to answer. p-hacking is going down one path, finding a p-value of 0.052, and turning around to go down another path that leads to 0.049. It’s true that an analyst of any stripe can engage in a multitude of QRPs, but p-hacking is uniquely Frequentist.\nOf course, it’s not the p-value’s fault that it’s often misunderstood and abused. Even the “abandon statistical significance” camp recognizes its value for some tasks. Their main criticism is that the conventional use of p-values encourages us to think dichotomously—there either is an effect or there is not—and this is bad for science. Non-significant doesn’t mean “no effect”, but that’s often the conclusion when p = 0.052 (just scroll to the top of the chapter for an example).\nFurthermore, when publication decisions are made on the basis of p < 0.05, we distort the literature and encourage QRPs. And when we encourage QRPs—particularly when our sample sizes are small and we look for small effect sizes—we end up with a crisis. We publish a lot of noise that fails to replicate. We’ll pick up this replication crisis thread in a later chapter on open science. For now, let’s consider some alternatives to p-values and NHST.\n\n\n4.2.5 “THERE HAS TO BE A BETTER WAY!”\n\n\n\nSome argue that the way to avoid dichotomous thinking is to embrace uncertainty and focus on estimating effects—to embrace the “New Statistics” (Calin-Jageman et al., 2019). Figure @ref(fig:newstats) compares the “new” and the “old” (NHST) using the HAP trial results.\n\nCalin-Jageman, R. J. et al. (2019). The New Statistics for Better Science: Ask How Much, How Uncertain, and What Else Is Known. The American Statistician, 73(sup1), 271–280.\n\n\n\n\n\nFrom statistical significance to estimation and quantification of uncertainty. 3-month endline data from the HAP trial (patel:2016?).\n\n\n\n\nPanel A represents NHST, where the goal is to determine whether or not there is a statistically significant difference (there is!). Panel B compares the same group means but places the “significant” results in the context of individual data points from all study participants, reminding us that a lot of people in the treatment group remained at a high level of depression severity even though the treatment group improved on average relative to the control group.\nPanel C focuses on estimation. It displays the point estimate of the average treatment effect (treatment mean minus control mean, -7.6) and the 95% confidence interval. The confidence interval gives us the same information as a p-value, plus more.\n\nThat fact that the interval does not include 0 tells us that the difference is statistically significant; the p-value is less than 0.05.\n-7.6 is our best estimate of the treatment effect, but the range of the interval tells us that we cannot reject effects between -10.3 and -4.9.\nThe flip side of this is that we can rule out very large effects greater than -10.3 and very small effects less than -4.9.\n\nBut here’s the catch with confidence intervals: they are still bound by Frequentists ideas of probability. Therefore, a confidence interval is a long-run metric. If you were to repeat your study over and over and estimate confidence intervals each time, 95% of the time the true value would fall inside the interval. But we don’t know if in this study—the one we actually conducted—the true value falls inside this specific interval. 95% of the time the interval will contain the true value, but this might be one of those times it doesn’t.\nBottom line: Frequentist confidence intervals are an improvement over p-values, but they still do not tell you the probability that your hypothesis is correct. For that we need the Bayesian approach."
  },
  {
    "objectID": "statinference.html#bayesian-approach",
    "href": "statinference.html#bayesian-approach",
    "title": "4  Statistical Inference",
    "section": "4.3 Bayesian Approach",
    "text": "4.3 Bayesian Approach\nFigure @ref(fig:bayespanels) shows the basic idea behind Bayesian analysis in three panels. The first panel shows a simulation of results we believe are plausible before collecting data. The second panel displays the data we collect and our best estimate of how well our model fits the data. The third panel updates our prior belief based on the data we observed.\n\n\n\n\n\nBayes’ Theorem in three panels. Source: Tristan Mahr, https://tinyurl.com/ya2tvoaj\n\n\n\n\nIn Bayesian data analysis, we proclaim our prior belief about the hypothesis P(H), collect some data and determine the likelihood of the data given our hypothesis P(D|H), and combine the likelihood and prior to obtain the posterior probability of the hypothesis given the data we observed P(H|D). This is Bayes’ theorem at work: the posterior is proportional to the likelihood times the prior.\n\n\n\nConceptually, this is very different from the Frequentist approach, even though the answers we get might be similar. In the Frequentist approach, we get a p-value that quantifies the probability of the DATA if the null hypothesis were true. But in the Bayesian approach, we get the probability of the HYPOTHESIS given the data we observed. With Bayesian data analysis, there’s no need to imagine an infinite number of trials—and no need to worry about multiple comparisons or peeks at the data. You can look at your data every day if you want. Every data point updates your belief in the hypothesis. Let’s reexamine the HAP trial from a Bayesian perspective to see the real benefits of a Bayesian approach when it comes to interpretation.\n\n4.3.1 BAYESIAN RE-ANALYSIS OF THE HAP TRIAL\nFigure @ref(fig:hapbayes) compares the Frequentist 95% confidence interval reported in (patel:2016?) to the Bayesian 95% credible interval based a weakly informative prior and the trial data. The results are essentially the same, but the interpretation is very different.\n\n\n\n\n\nBayesian re-analysis of HAP primary outcome of depression severity. Anonymized data provided by the authors.\n\n\n\n\nWith Frequentist confidence intervals, we can say that if we repeated this trial over and over, 95% of intervals would contain the true value. We don’t know if the interval on the left is one of the times we got it wrong, but if we act like this interval contains the true value we will only be wrong only 5% of the time.\n\n\n\nThe Bayesian credible interval on the right tells us something different: we are 95% confident that the treatment effect falls in *this specific interval. We can say this because Bayesian analysis gives us a posterior probability distribution that is not conditional on the null hypothesis being true. A particularly handy feature of posterior distributions is that we can also estimate the probability of any particular point. For instance, there is a 87% chance that the effect size is less than -6.\nAnd look at that—we get the probability of a hypothesis given the data. Nice, right?"
  },
  {
    "objectID": "statinference.html#keep-learning",
    "href": "statinference.html#keep-learning",
    "title": "4  Statistical Inference",
    "section": "4.4 Keep Learning",
    "text": "4.4 Keep Learning\nStatistical inference is hard. Here are a few resources to keep learning:\n\nStatistical Inference: Daniël Lakens’s Coursera course, “Improving your statistical inferences”\nStatistical Inference: Zoltan Dienes’s book Understanding Psychology as a Science\nFrequentist Approach: Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars, by Deborah Mayo\nNew Statistics: Understanding the New Statistics, by Geoff Cumming\nBayesian Approach: Statistical Rethinking by Richard McElreath"
  },
  {
    "objectID": "causalinference.html#what-is-causal-inference",
    "href": "causalinference.html#what-is-causal-inference",
    "title": "5  Causal Inference",
    "section": "5.1 What is Causal Inference?",
    "text": "5.1 What is Causal Inference?\n\n\n\nCasual Inference Podcast, Season 3, Episode 6, A Casual Look at Causal Inference History.\n\nCausal inference is what we do when we identify and estimate the causal effect of some proposed cause on an outcome of interest. We use causal inference methods in global health to answer key questions about health policy and practice. Do bed nets prevent malaria, and by how much? Is it better to subsidize bed nets or sell them at full retail cost? And so on.\n\n\n\n\nFigure 5.1: Causes, effects, and outcomes\n\n\n\nAs someone who has likely perfected the art of causal inference in your daily life, you might be surprised to learn that causal inference in science is still a rapidly evolving field. Even core terms like cause and effect are up for debate.\n\n\n\n\n\n\n\nEffects of causes or causes of effects?\n\n\n\nThe most common type of causal inference question we ask is about the effects of causes. What is the effect of 𝑋 on 𝑌? For instance, what is the effect of a new therapy on depression severity? Given a well-defined cause, 𝑋, we can estimate what happens to 𝑌 if 𝑋 changes. Questions about the causes of effects—what causes 𝑌?—are harder to answer. For example, what causes depression?\n\n\n\n\nCAUSES\n\n\n\n\n\nWatch Dr. Judea Pearl discuss what he calls the new science of cause and effect.\n\nThe Turing Award-winning computer scientist Dr. Judea Pearl and his co-author, mathematician-turned-science writer Dr. Dana Mackenzie, offered the following definition of causes in their 2018 instant classic, The Book of Why (Pearl et al., 2018):\n\nA variable 𝑋 is a cause of 𝑌 if 𝑌 “listens” to 𝑋 and determines its value in response to what it hears\n\nThis definition has several implications for causal relationships (Shadish et al., 2002):\n\nCauses must come before effects. 𝑋 speaks and then 𝑌 listens.\n\nCauses and effects are associated, meaning they go together or covary. When 𝑋 happens, 𝑌 is more likely to happen. I say “more likely” because the effect does not always need to happen for there to be a causal relationship between 𝑋 and 𝑌. For instance, smoking increases the probability of developing lung cancer, but not all smokers will develop lung cancer. Most of the relationships we study in global health are like this—probabilistic in nature, not deterministic.\nThere are no other plausible alternative explanations for the effect other than the proposed cause. In other words, 𝑌 listens to 𝑋 and not to something else that also happens to be related to 𝑋. During the smoking debate of the 1950s and 1960s, some proponents of smoking asked whether the apparent causal link between smoking and lung cancer could be explained by a smoking gene that predisposed people to smoking and to lung cancer.2\n\nThe need to rule out plausible alternative explanations keeps many researchers up at night. Claims from studies that fail to do this convincingly are characterized as having low internal validity. In other words, there is not a strong justification for inferring that the observed relationship between 𝑋 and 𝑌 is causal.\n\nCauses in global health research\nWe study a variety of potential causes in global health research and call them by different names. For instance, global mental health researchers often develop and test interventions delivered to individuals or groups to prevent or treat conditions such as depression. Development agencies administer programs to improve people’s well-being, including economic assistance programs intended to reduce poverty. Clinical researchers and biostatisticians test the efficacy of drugs and medical devices, generally referred to as treatments or therapies, on health outcomes, such as the COVID-19 vaccinations developed in 2020. Policy researchers and health economists study the health and financial impacts of policies, such as removing fees to deliver a baby at public health facilities. Epidemiologists estimate the effect of exposures, such as smoking, on the health status or prognosis of a target population.\n\n\n\n\n\n\n\nNo causation without manipulation?\n\n\n\nSome scholars believe that it must be possible to manipulate, or change, a variable for that variable to be considered a cause. This would exclude immutable variables such as age and genetic sex as causes of effects because there is not currently a way to plausibly intervene to change them. This is sometimes framed as the need for a “well-defined intervention”. I don’t personally agree with the “no causation without manipulation” mantra, but I value the way it encourages us to focus on research questions that can improve public health.\n\n\n\n\n\n\nEFFECTS\nCausal inference is an exercise in counterfactual thinking, full of “what if” questions about the road not taken. We ask ourselves these questions all the time. What would have happened if I had taken that job? Said “yes” instead of “no”? How would life be different today?\n\nCounterfactuals and potential outcomes\nA counterfactual is the hypothetical state of a “what if” question. With counterfactual thinking, there is what actually happened, and then there is the hypothetical counterfactual of what would have happened, counter to fact, under the alternative scenario. The difference between what did happen and what would have happened is a known as the causal effect.\nRobert Frost fans see the problem.\n\n\n\n\n\n“The Road Not Taken”, by Robert Frost, The Atlantic Monthly, 1915.\n\n\n\nTwo roads diverged in a yellow wood, and sorry I could not travel both and be one traveler…\n\nRobert has to choose one road; he can’t take both simultaneously. Robert can either take the road more traveled, a decision we’ll call 𝑋 = 0, or he can take the road less traveled, a decision we’ll call 𝑋 = 1.\n\n\n\n\n\n\n\nWhat’s with the Xs and Ys, 1s and 0s?\n\n\n\nIn some ways it might be easier to refer to Robert’s decision as the variable road, or 𝑅, and set the values of road to “more traveled” or “less traveled”, representing his two choices. But instead I’m referring to his decision as 𝑋 and to the values of 𝑋 as 0 (more traveled) or 1 (less traveled). Why?\nOften we refer to potential causes as 𝑋 and to response variables as 𝑌. And typically, when the treatment (or exposure) 𝑋 can take two levels, such as treated/not treated, we label treated 1, and not treated 0.\nIn this example, I imagine Robert looking left and then looking right. Observing that the second path was “grassy and wanted wear”, he decided to go right, taking the “one less traveled by”. Frost claims that his decision to take the uncommon path “made all the difference”, so I’m labeling this path (less traveled) as the treatment, 𝑋 = 1.\n\n\n\nRobert’s two choices correspond to two potential outcomes, or states of the world, that he could experience (Rubin, 1974). There is the potential outcome that results from taking the road more traveled, and the potential outcome that results from taking the road less traveled. We’ll call these scenarios Y𝑖X=0 (what happens if he takes the road more traveled) and Y𝑖X=1 (what happens if he takes the road less traveled).\n\nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66(5), 688.\nSo what does he do? He famously takes the road less traveled. Robert’s factual outcome is what happens after making this choice. His other potential outcome will never be observed. Taking the road more traveled is now the counterfactual. He can only wonder what would have happened, counter to fact, if he had taken the beaten path.\n\n\nCould Robert experience the counterfactual by returning at a later date to take the other road? No, because he wouldn’t be the same person who stood there at the start. He could not un-experience the road less traveled. Also, we’d be measuring his happiness at two different points in time. Besides, he did not expect to return: “Yet knowing how way leads on to way / I doubted if I should ever come back.”\nAnd yet, Robert boldly claims that taking the road less traveled “made all the difference”. How can he know for sure? We defined causal effects as the difference between what did happen and what would have happened, but we only observed what happened, not what would have. Therefore, we have a missing data problem.\nThis is what’s often called the fundamental problem of causal inference (Holland, 1986): we only get to observe one potential outcome for any given person (or unit, more generally). The causal effect of taking the road less traveled is the difference in potential outcomes: 𝛿i = 𝑌𝑖𝑋=1 - 𝑌𝑖𝑋=0, but 𝑌𝑖𝑋=0 is missing. Therefore, we can’t measure this effect for Robert (𝑖).\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945–960.\n\n\nAverage treatment effect\nWe can, however, compare groups of people like Robert who take one road or the other and estimate the average treatment effect, or ATE. I’m emphasizing “estimate” because truly calculating the ATE would require knowing both potential outcomes for each person (or unit, like classrooms or schools). We can only observe one potential outcome for any given person, but let’s ignore this for a moment to understand the true ATE.\n\n\nThe average treatment effect is also called the sample average treatment effect, or SATE.\nWhile we’re pretending, let’s imagine that the response variable Robert writes about in his poem is happiness later in life, and happiness, or 𝑌, is measured on a scale of 0 to 10 where 0 is not at all happy and 10 is very happy.\nFigure 5.2 shows fictional happiness data for a sample of 12 people, including Robert, under both potential outcomes. Notice how each person in the left panel has two values: the happiness that would result if they took the road less traveled (𝑋=1) and the happiness that would result if they took the road more traveled (𝑋=0). Some people, like Traveler 1, would be happiest taking the road more traveled, whereas other people, such as Robert (11), would find greater happiness on the road less traveled. (Remember, we’re pretending here. We never observe both potential outcomes.)\n\n\n\n\nFigure 5.2: Potential outcomes, part 1.\n\n\n\nAs shown in the right panel of Figure 5.2, the ATE is calculated as the difference between group averages (𝐸[𝑌𝑖𝑋=1]-𝐸[𝑌𝑖𝑋=0]), which is equivalent to the average of traveler’s individual causal effects. In this example where we are all knowing, the ATE is 1.5; taking the road more traveled increases happiness on average by 1.5 points on a scale of 0 to 10.\nWe’re not all knowing, of course, and we only get to observe one potential outcome for each person. This means we have to estimate the ATE by comparing people like Robert who take the road less traveled to people who take the road more traveled. But how do people come to take one road versus the other?\nFigure 5.3 shows two scenarios (of an infinite number). On the left (A), travelers are randomly assigned to a road. Imagine that they come to the fork in the road and pull directions out of a hat. Half are assigned to the road less traveled, and half to the road more traveled. On the right (B), however, travelers choose a road themselves. Imagine that they go with their gut and all happen to pick the road that maximizes their individual happiness. In both panels, the grey dots represent the unobserved counterfactuals.\n\n\n\n\nFigure 5.3: Potential outcomes, part 2.\n\n\n\nNotice that under perfect randomization (left, A), the estimated ATE 1.5 is equal to the true (but unknowable) ATE. But on the right (B), when travelers selected their own road, the estimate is 4.7. This does not line up with the true ATE because the estimate includes bias. Remember that bias is anything that takes us away from the truth.\n\n\nThe simple difference in outcomes (SDO) is an estimator for the ATE. It contains the ATE plus bias.\nCunningham (2020) very effectively shows how to decompose the simple difference estimator (SDO) into three parts: the ATE, selection bias, and heterogeneous treatment effect bias. Selection bias occurs when we are comparing unequal groups. Heterogeneous treatment effect bias occurs when a treatment (or exposure) has differential effects on units based on their characteristics.\nAs we will see in later chapters, randomization can be a very effective way to neutralize selection bias, but randomization is not always possible or maintained. Most research is non-experimental, or what many would call observational. Thus even if we assume that treatment effects are constant (effectively ignoring heterogeneous treatment effect bias), selection bias will remain a threat in many cases. Unfortunately for us, we can’t simply calculate it and subtract it away because the exact quantity is typically unknowable. That leaves us with with research design and statistical adjustment as our only defense. As Cunningham (2020) states:\n\nOne could argue that the entire enterprise of causal inference is about developing a reasonable strategy for negating the role that selection bias is playing in estimated causal effects.\n\n\n\n\nCAUSAL INFERENCE METHODS\n\n\n\n\n\nWatch Dr. Esther Duflo’s speech at the Nobel Banquet, 10 December 2019.\n\nThere are different causal inference methods for addressing selection bias, and which one a researcher chooses tends to be heavily influenced by their context and discipline. For instance, clinical researchers, biostatisticians, and behavioral interventionists often prefer to use experimental designs that randomly allocate people (or units) to different treatment arms.3 There is also a rich tradition of experimentation in the social sector among economists and public policy scholars. Economists Abhijit Banerjee, Esther Duflo, and Michael Kremer won the 2019 Nobel Prize in Economics4 for their experimental approach to alleviating global poverty.\nBut as I stated previously, many research questions in global health are not amenable to experimentation, and the approach to causal inference must be rooted in non-experimental (or observational) data. Matthay, Hagan, et al. (2020) divide these non-experimental approaches into two main buckets: confounder-control and instrument-based. Confounder-control is characterized by the use of statistical adjustment to make groups more comparable. You’ll find many examples of confounder-control in epidemiology and public health journals. Instrument-based studies, sometimes called quasi-experimental designs, estimate treatment effects by finding and leveraging arbitrary reasons why some people are more likely to be treated or exposed. Instrument-based studies are quite common in economics and psychology. I’ll introduce each approach in turn.\n\n\n\n\n\n\n\nAdditional causal inference frameworks\n\n\n\nBradford Hill’s Considerations. In 1965, Epidemiologist and statistician Sir Austin Bradford Hill proposed a set of nine domains to consider when evaluating the evidence for a causal relationship: (1) strength, (2) consistency, (3) specificity, (4) temporality, (5) biological gradient, (6) plausibility, (7) coherence, (8) experiment, and (9) analogy. It does not feature prominently in current debates about causal inference, and it’s commonly misapplied. Modern Epidemiology has a good summary and critique: ghr.link/mod.\nSufficient Cause Framework and Causal Pies. The sufficient-component cause model is an approach for conceptualizing cause and effect used largely for pedagogical purposes. Dr. Kenneth Rothman defined sufficient causes as the minimal set of conditions that produce a given outcome. This set of causes is often represented by pie charts."
  },
  {
    "objectID": "causalinference.html#causal-diagrams-and-confounder-control",
    "href": "causalinference.html#causal-diagrams-and-confounder-control",
    "title": "5  Causal Inference",
    "section": "5.2 Causal Diagrams and Confounder-Control",
    "text": "5.2 Causal Diagrams and Confounder-Control\nConfounding is a type of bias where variables 𝑋 and 𝑌 share a common cause 𝑍 that explains some or all of of the 𝑋𝑌 relationship. You’re likely familiar with examples of confounding like ice cream sales and violent crime. If you look just in the data, it looks like increases in ice cream sales could be causing increases in violent crime (or maybe vice versa), but this is what we call a spurious correlation. Ice cream sales and violent crime are both more common when the weather is warm. Once you statistically control for weather, let’s say by looking just at sales on hot days, there is no relationship between ice cream sales and crime.\nCausal relationships observed in non-experimental contexts are at high risk of confounding, and the goal of confounder-control studies is to find and statistically adjust for a sufficient set of variables to eliminate confounding. But this is not just an exercise in statistics because data are profoundly dumb (Pearl et al., 2018). A dataset cannot tell you which variables to adjust for, or what is a cause and what is an effect. For that you need information that lives outside of statistical models. You need causal models that are informed by domain expertise (McElreath, 2020).\n\nPearl, J. et al. (2018). The book of why: The new science of cause and effect (1st ed.). Basic Books, Inc.\n\n\n\n\n\nWatch Dr. Nick Huntington-Klein introduce causal diagrams.\n\nFor this reason, a graphical approach based on causal diagrams has emerged as a popular tool for causal inference in confounder-control studies (Pearl, 1995).5 The most common type of graphical model you’ll encounter is the causal directed acyclic graph, or DAG. Figure 5.4 shows an example DAG of the effect of taking the road less traveled on happiness.\n\nPearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4), 669–688.\n\n\n\n\nFigure 5.4: Causal directed acyclic graph (DAG) of the effect of taking the road less traveled on happiness.\n\n\n\n\nCAUSAL STORIES\n\n\n\nDr. Scott Cunningham’s book, Causal Inference: The Mixtape, is worth every penny. If you’re short on pennies, read it online for free.\n\nCunningham (2020) frames DAGs as storytelling devices. The story I am telling with this DAG is that the road traveled causes happiness directly and indirectly by creating new social relationships. This DAG also shows my assumption that happiness AND the decision to take the road less traveled are both caused in part by one’s cognitive style (e.g., sense of optimism); they share a common cause. Happiness is also caused by income which, like cognition, is a function of background characteristics like genetics and family.\n\nCunningham, S. (2020). Causal inference: The mixtape. Yale University Press.\n\n\n\n\n\n\n\nStart by drawing your assumptions\n\n\n\nBefore I even do anything with this DAG, or any DAG I create, I’ve accomplished a lot just by drawing my assumptions. The DAG represents my belief in the data generating process. It includes all nodes and connections that I believe are relevant to the effect of road traveled on happiness. I’ve made my assumptions clear and can proceed to identify how I will estimate the causal effect of interest.\nNow you might call bullshit, and that’s OK. You can draw a different DAG that might have different implications for the best analysis strategy. You and I should be able to defend our assumptions and be open to modifications based on subject matter criticism. But whether you draw a DAG or not, there is no escaping the need to make assumptions. DAGs just help to make your assumptions clear and transparent.\n\n\n\n\n\nCOMPONENTS OF A DAG\nAs a graph, DAGs consist of nodes and edges (or arrows). Nodes are variables like our exposure of interest, the road traveled, and our outcome of interest, happiness later in life. Nodes can take any form, from discrete values of road traveled (more traveled, less traveled) to continuous values of income. A DAG can include observed (measured) variables and unobserved variables, including background factors such as genetics.\n\n\n\n\n\nFigure 5.5: The same DAG again, reprinted for your convenience.\n\n\nNodes are connected by edges, directed arrows that make causal statements about how two variables are related. For instance, by drawing an arrow from road traveled to happiness, I’m asserting that the road one travels causes happiness. Arrows do not indicate whether this relationship is positive or negative, just that road traveled influences happiness. Equivalently, the absence of an arrow between nodes implies that there is no causal relationship.\nThe only hard rule in a DAG is that cycles are not permitted. Arrows can go into and out of a node, but there must not be any recursive pathways. For instance, social relationships ⟶ happiness ⟶ social relationships is not allowed. Causal effects must only flow forward in time. Spirals are allowed, however, and offer a way to represent that causal relationships between variables measured at different time points (e.g., social relationships_t1 ⟶ happiness_t2 ⟶ social relationships_t3 ⟶ happiness_t4).\nThere are three possible relationship structures in a DAG (McElreath, 2020):\n\nForks: In forks like road traveled ⟵ cognition ⟶ happiness, cognition is a common cause of the focal variables of interest. As such, cognition confounds the causal effect of road traveled on happiness; some (or all) of the observed association is due to cognition. When you see a fork, you should think confounding.\nPipes: Pipes (or chains) involve mediator variables like social relationships that represent an indirect causal chain of effects. For instance, road traveled causes new social relationships which causes happiness. Whether or not you are interested in the indirect causal effect depends on your research question.6\nColliders: Colliders (or inverted forks) are closed pathways like road traveled ⟶ active lifestyle ⟵ happiness where a node on the pathway only has incoming arrows. These pathways are closed by default and only open when conditioning on the collider, thereby distorting the relationship between road traveled and happiness.\n\nAs you will see shortly, being able to recognize these relationships will help you to identify your causal effect of interest.\n\n\n\n\n\n\n\nDescendants and ancestors\n\n\n\nGraphs like DAGs can also be described by the ancestry of the nodes. A descendant variable has at least one incoming arrow. Economists would call this an endogenous variable (vs an exogenous variable like background that has no incoming arrows). In the example DAG, happiness and social relationships are descendants of road traveled. Specifically, social relationships is a child of road traveled, and road traveled is its parent. Therefore, road traveled and social relationships are ancestors of happiness.\n\n\n\n\n\nDECIDE WHICH NODES TO INCLUDE IN A DAG\nIn order to use a DAG to identify a causal effect, you must include all of the relevant nodes and paths (Rohrer, 2018). As you can probably imagine, this can get out of hand quickly. Just look at the slide in Figure 5.6 that diagrams the American military’s perceived challenge in its war in Afghanistan.\n\nThis includes variables that you can measure and the ones you can’t (or didn’t) observe.\n\n\n\n\n\nFigure 5.6: War is hard. Source: U.S. Joint Chiefs of Staff. (Not a DAG, per se, but you get the point.)\n\n\n\n\n\n\nDr. Huntington-Klein’s book, The Effect: An Introduction to Research Design and Causality, should be on your bookshelf. If you’re not in a position to purchase it today, read it online for free.\n\nHuntington-Klein (2021) frames DAG creation as a balancing act:\n\nOn one hand, we want to omit from the diagram every variable and arrow we can possibly get away with. The simpler the diagram is, the easier it is to understand, and the more likely it is that we’ll be able to figure out how to identify the answer to our research question…On the other hand, omitting things makes the model simpler. But the real world is complex. So in our quest for simplicity, we might end up leaving out something that’s really important.\n\nA piece of practical advice is to draw a basic DAG and then add additional variables (nodes) only if you believe they causally affect two or more existing nodes in the DAG (Rohrer, 2018). For instance, maybe you could argue that being left handed also contributes to one’s decision to take the road less traveled. If handedness does not have an arrow into any other nodes, you can safely leave it out of the DAG.\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42.\nIf this feels daunting, you’re doing it right. Science is hard, and I predict that you’ll find this process easier if you have the humility to know that, at best, your study will approximate the truth. There is a very good chance that your DAG will be wrong or incomplete. Your colleagues might tell you as much. This is part of the scientific process. Criticism should lead you to revise your DAG or strengthen how you defend your assumptions.\n\n\nTRACING ALL PATHS\nOnce you’ve drawn your DAG, the next step is to list all of the paths from the proposed cause to the outcome of interest. In this example, it means tracing all of the paths that go from road traveled to happiness.\nStart with road traveled and move your finger along each path until you get to another variable or to the outcome, happiness. When you encounter variables with arrows coming in or out, trace each path to the outcome. The direction of the arrows does not matter for tracing, just don’t trace back to variables you’ve already visited (in other words, no loops).\n\nroad traveled ⟶ happiness\nroad traveled ⟶ social relationships ⟶ happiness\nroad traveled ⟵ cognition ⟶ happiness\nroad traveled ⟵ cognition ⟵ background ⟶ income ⟶ happiness\nroad traveled ⟶ active ⟵ happiness\n\n\n\n\nView the example DAG or create your own at DAGitty.net.\n\nYou can check your work—or skip the manual tracing process entirely—by creating and analyzing your DAG in R. While you can manually enter your graph into R, a shortcut is to create your DAG visually in your browser at DAGitty.net and copy/paste the model code into R. The function paths() returns the same five paths we traced by hand.\n\nlibrary(dagitty)\n\ndag <- dagitty(\n'dag {\nbb=\"0,0,1,1\"\n\"road traveled\" [exposure,pos=\"0.224,0.452\"]\n\"social relationships\" [pos=\"0.355,0.547\"]\nactive [pos=\"0.359,0.677\"]\nbackground [pos=\"0.460,0.166\"]\ncognition [pos=\"0.355,0.340\"]\nhappiness [outcome,pos=\"0.501,0.449\"]\nincome [pos=\"0.505,0.285\"]\n\"road traveled\" -> \"social relationships\"\n\"road traveled\" -> active\n\"road traveled\" -> happiness\n\"social relationships\" -> happiness\nbackground -> cognition\nbackground -> income\ncognition -> \"road traveled\"\ncognition -> happiness\nhappiness -> active\nincome -> happiness\n}'\n)\n\npaths(dag)\n\n$paths\n[1] \"\\\"road traveled\\\" -> \\\"social relationships\\\" -> happiness\"         \n[2] \"\\\"road traveled\\\" -> active <- happiness\"                           \n[3] \"\\\"road traveled\\\" -> happiness\"                                     \n[4] \"\\\"road traveled\\\" <- cognition -> happiness\"                        \n[5] \"\\\"road traveled\\\" <- cognition <- background -> income -> happiness\"\n\n$open\n[1]  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\nEFFECT IDENTIFICATION\nNext we need to determine which paths are good and which are bad (Huntington-Klein, 2021). “Good paths” identify our research question. “Bad paths” are the alternate explanations for the causal effect that we need to close. In our example DAG, as in many DAGS, good paths often start with an arrow exiting the proposed cause and bad paths have arrows entering the proposed cause.\n\nAn exception would be if we are only interested in the direct effect of road traveled on happiness, we would label the mediation pathway through social relationships as a bad path and work to close it.\n\nFigure 5.7 visualizes the five paths in our example DAG and labels them good or bad. Notice that paths 1 and 2—the good paths—start with road traveled and flow forward to happiness. The rest are backdoor paths that we need to close.\n\n\n\n\nFigure 5.7: Good and bad paths in our example DAG.\n\n\n\n\n\n\nDr. Andrew Heiss has a great tutorial on ways to close backdoors through regression, inverse probability weighting, and matching.\n\nOpen backdoor paths bias the causal effect we want to estimate, so we need to close them. We can do this by conditioning on variables that confound the causal effect of interest through techniques like regression. The neat thing is that we do not necessarily need to control for every possible confounder, just a minimum set sufficient to close all backdoor paths.7 For instance, in Figure 5.7, you can see that adjusting for cognition in paths 3 and 4 is sufficient to close the open backdoor paths.\nPath 5 is also a backdoor path, but it’s closed by default because active is a collider (see the two incoming arrows). If we condition on active, let’s say by including it as a covariate in our regression, we will inadvertently open this path and introduce bias. There is such a thing as a bad covariate, and sometimes less is more when it comes to statistical models (McElreath, 2020; Westreich et al., 2013).8\n\nWestreich, D. et al. (2013). The table 2 fallacy: Presenting and interpreting confounder and modifier coefficients. American Journal of Epidemiology, 177(4), 292–298.\n\n\n\nThe logic of d-separation is rooted in Pearl’s do-calculus. Learn more in The Book of Why or this resource that Heiss created.\n\nIf this DAG is correct and complete, adjusting for (controlling for) cognition on the backdoor path makes the relationship between road traveled and happiness d-separated (direction separated) from all other nodes. In other words, the causal effect is said to be identified.\n\n\nEFFECT ESTIMATION\nThere are various statistical techniques to cut the bad paths so they don’t bias our causal effect of interest. I’ll show you regression. See Heiss (2020b) for other examples.\n\nHeiss, A. (2020b). Ways to close backdoors in DAGs.\n\nSimulation\nFor this example, I simulated a dataset on 1000 people who decided to take the road less traveled or the road more traveled.9 This is an observational dataset. There was no random assignment to road_traveled. The core variables include:\n\n\nSimulation is the only way to demonstrate that controlling for cognition recovers the correct effect. In real world datasets you don’t know the right answer. If you did, causal inference would be easy.\n\ncognition (confounding variable): A binary (0/1) variable set to 1 if the person scored high on a measure of optimism, otherwise 0. Imagine that everyone completed a questionnaire before they decided on a road. This questionnaire included items that assessed aspects of their cognitive style, and we used their answers to construct an indicator of high/low optimism.\nroad_traveled (exposure/treatment): A binary (0/1) variable set to 1 if the person chose the road less traveled, otherwise 0. This variable was simulated to be a function of cognition. In this dataset, the odds of taking the road less traveled are 18 times higher for high optimism folks.\nhappiness (outcome): A variable that can range from 0 to 10, where 10 represents greatest happiness. Imagine that data on happiness was collected 10 years after people selected and traveled down one of the roads. This variable was simulated to be a function of cognition and happiness. On average, high optimism folks scored 1 point higher on the measure of happiness.\nactive (collider): A binary (0/1) variable set to 1 if the person has an active lifestyle. Imagine that this variable was collected at sometime after the person traveled the road. This variable was simulated to be a function of road_traveled and happiness. active does not cause anything in the model.\n\n\n\n\nI simulated the data so that the road less traveled increases happiness by 1.5 points. That will be the correct answer going forward. Here are my receipts:\n\ndf_road_A %>% \n  summarize(mean = mean(happiness_road1-happiness_road0))\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1  1.50\n\n\n\n\nThe Problem: Confounding\n\n\n\n\n\nFigure 5.8: cognition confounds the 𝑋𝑌 relationship.\n\n\nFigure 5.8 should remind you that our core problem in this DAG is that the exposure (road_traveled) and the outcome (happiness) share a common cause: cognition. This is to say that cognition confounds the relationship between 𝑋 and 𝑌.\nYou can see this visually in Figure 5.9. In the left panel, people who scored high on optimism are represented in red (vs low in black). Notice that red appears more frequently among the road less traveled group AND red looks to be associated with higher happiness scores. If we just compare happiness scores by road traveled, we get the wrong answer (difference of 2.07). This is because cognition biases the 𝑋𝑌 relationship.\n\n\nRemember, we know the correct answer is 1.5 because that’s how I simulated the data generating process.\nTo get to the right answer, we need to hold cognition constant. In the right panel I show this by looking just at people with a low optimism score. Now if we compare happiness scores by road traveled, we get close to the correct answer (difference of 1.47).10\n\n\n\n\n\nFigure 5.9: Confounding\n\n\n\n\n\n\nA Solution: Multiple Regression\nIn practice we might estimate the effect via a technique like multiple regression, which I’ll show you here.11 You’ll see that I modified the variable names to make Figure 5.10 easier to read. Most notably, the exposure road traveled is represented as x and the outcome happiness is represented as y. Cognition and active are simply c and a, respectively.\nFigure 5.10 presents the results of three models:\n\ny ~ x: naive regression of happiness (y) on road_traveled (x)\ny ~ x + c: same as (1) but also controlling for cognition (c)\ny ~ x + c + a: same as (2) but also controlling for active (a)\n\n\n\n\n\n\nFigure 5.10: Multiple regression models\n\n\n\n\nThe red model y ~ x repeats the same mistake as above. It just estimates the impact of road traveled x on happiness y without accounting for the confounding role of cognition c. It returns the wrong answer.\nThe green model y ~ x + c controls for cognition c, thereby removing the parts of x and y that are explained by c (Heiss, 2020a). This closes the biasing pathway and returns the correct answer.\n\nHeiss, A. (2020a). Causal inference. In R for political data science (pp. 235–273). Chapman; Hall/CRC.\n\nThis should not be surprising. I simulated the data generating process with c confounding the relationship between x and y. Our DAG was therefore correct, and controlling for c recovers the simulated effect.\n\nThe blue model y ~ x + c + a gets us into trouble. In the data I simulated, active a is a collider. It doesn’t cause anything in the DAG, and the bad path it sits on is closed by default. When I control for it by adding it to the regression, I open the pathway and distort the relationship between x and y.\nIt might come as a surprise that something can go wrong by adding covariates to your model. Many of us are taught that it’s probably good or at least neutral to add all seemingly relevant variables to a regression. This is just bad advice. Some covariates will make your estimates worse, not better.\n\n\n\n\n\nWatch Dr. Richard McElreath’s excellent talk, Science Before Statistics: Causal Inference.\n\nMcElreath (2020) uses the term causal salad to describe this very common practice of tossing lots of “control” variables into a statistical model and telling a causal story. This approach can work when the goal is prediction, but it can go very very wrong when the goal is causal inference. One of his core points in Statistical Rethinking is that causal inference requires causal models that are separate from statistical models. Statistics alone can get us to the wrong answers. But if we follow the DAG—our causal model—we know to leave active alone.\n\n\n\nTHE BIG FINE PRINT\nDAGs are useful tools for causal inference, but they are not magic. If your DAG does not completely and correctly identify your causal effect of interest, your estimates will be biased. To make matters worse, there is no test that will tell you if you got it right or wrong.12\n\n\nA step I skipped for space is verifying conditional independencies. DAGitty.net and the {dagitty} R package will get you started.\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\nBut let’s be clear: there is no approach to causal inference that sidesteps the need for unverifiable assumptions (McElreath, 2020). DAGs require you to make and defend your assumptions, but so does every other approach (you just might not know it)."
  },
  {
    "objectID": "causalinference.html#instrument-based-approaches-and-quasi-experimental-designs",
    "href": "causalinference.html#instrument-based-approaches-and-quasi-experimental-designs",
    "title": "5  Causal Inference",
    "section": "5.3 Instrument-Based Approaches and Quasi-Experimental Designs",
    "text": "5.3 Instrument-Based Approaches and Quasi-Experimental Designs\nIf confounder-control is about closing backdoors through statistical adjustment, instrument-based approaches are about isolating front doors (Huntington-Klein, 2021). These approaches are sometimes called quasi-experimental designs because they attempt to mimic the beauty and logic of a perfectly conducted randomized controlled trial (RCT).\n\nHuntington-Klein, N. (2021). The effect: An introduction to research design and causality. Chapman; Hall/CRC.\n\nEXPERIMENTS DESTROY CONFOUNDING\nAs shown in Figure 5.11, RCTs (experiments), are effective because they close all backdoors that run from the proposed cause to the outcome. For instance, if we were somehow able to randomly assign people to the road less traveled or the road more traveled—and if people complied with these assignments—then the only arrow into road_traveled would be randomization. Randomization would be the only cause of road_traveled.\n\n\n\n\nFigure 5.11: Observational DAG vs experimental DAG.\n\n\n\nRandomization destroys confounding. This includes confounding from variables you think to measure, like cognition in this example, as well as variables that you can’t or don’t measure for whatever reason. This idea is so powerful that many people refer to experiments as the gold-standard when it comes to causal inference.13\n\n\nEXOGENOUS VARIATION WITHOUT RANDOMIZATION\nWhen randomization is not possible, confounding is likely. You can try to account for this confounding statistically (confounder control), or you can search for partial causes of the exposure/treatment that are unrelated to the outcome. Sometimes you can get lucky and find an exogenous source of variation in the exposure/treatment and use it to identify the causal effect.\nFor instance, imagine that we couldn’t randomize who travels which road, but we could restrict access to the road less traveled to people born on or after January 1, 1980. By this arbitrary rule, someone born on January 1, 1980 would be allowed to pass, but someone born on December 31, 1979 would have to take the road more traveled. This is the basic setup for a regression discontinuity design that fits in the instrument-based or quasi-experimental bucket (Figure 5.12).\n\n\n\n\nFigure 5.12: Regression discontinuity DAG.\n\n\n\nIn this design, birthdate ≥ 1980-01-01 is an instrument that causes exogenous variation in who is exposed to the road less traveled (see the left panel of Figure 5.13). This variation is almost as good as randomization because the cutoff is arbitrary. Furthermore, when we limit our investigation to people born right around this arbitrary cutoff, any potential link between birthdate and the outcome is broken. We’d argue that people born just before and just after the cutoff are similar in many observable and unobservable ways. The only difference is that people born before the cutoff weren’t allowed to take the road less traveled. This isolates the front door from road_traveled to happiness.\n\n\n\n\n\nFigure 5.13: A regression discontinuity example.\n\n\n\n\nThe key to this design, and others in this category, is that the instrument changes the probability of exposure (treatment) WITHOUT having any other mechanism of impacting the outcome (Matthay, Hagan, et al., 2020). In a later chapter we’ll see examples of regression discontinuity in practice, along with other quasi-experimental designs like instrumental variables, difference-in-differences, and interrupted time series."
  },
  {
    "objectID": "causalinference.html#key-assumptions-and-threats-to-internal-validity",
    "href": "causalinference.html#key-assumptions-and-threats-to-internal-validity",
    "title": "5  Causal Inference",
    "section": "5.4 Key Assumptions and Threats to Internal Validity",
    "text": "5.4 Key Assumptions and Threats to Internal Validity\nI listed three requirements for causal relationships when I defined causes:\n\ncauses must come before effects;\n\ncauses and effects are associated, meaning they go together or covary; and\nthere are no other plausible alternative explanations for the effect other than the proposed cause.\n\nThe third requirement—no other plausible alternative explanations—is the hardest of them all to meet. Every approach to causal inference relies on mostly untestable assumptions related to this requirement.\nA key assumption, no confounding, is known by different names across disciplines: ignorability of the treatment assignment (statistics), conditional independence (economics), and (conditional) exchangability (epidemiology) (Gelman et al., 2020). This assumption says that there is no relationship between the treatment (or exposure) someone receives and their potential outcomes. In observational studies, this assumption needs to hold after adjusting for any covariates. Matthay, Hagan, et al. (2020) gives the example of a violation of this assumption where people who are more likely to succeed across the board, regardless of treatment, are more likely to be treated. It would be like stacking the deck in favor of the treatment. Randomization destroys this confounding, but it’s a concern for observational studies. With confounder-control approaches in particular, you always have to worry about omitted variable bias—failing to adjust for the sufficient set of confounding variables.\n\nGelman, A. et al. (2020). Regression and other stories. Cambridge University Press.\n\nMatthay, E. C., Hagan, E., et al. (2020). Alternative causal inference methods in population health research: Evaluating tradeoffs and triangulating evidence. SSM Population Health, 10, 100526.\n\nNo measurement error is another assumption, but I’ll save that for the chapter on content validity.\n\nOnce you adjust for a set of covariates, you must think about the assumption of positivity. It says that all possible covariate subgroups in your study must have a non-zero chance of being exposed to the treatment. For instance, if your adjustment set includes biological sex, it would be a positivity violation if males could not receive the treatment. If you are not adjusting for sex, this is not a concern. Westreich (2019) gives the example of adjusting for biological sex in a study about the effects of hysterectomy when it’s only possible for people with a uterus to undergo a hysterectomy. The solution is simple: biological sex should not be a covariate in your adjustment set.\nAnother assumption is consistency (or treatment variance irrelevance). This is the idea that any variations in a treatment or exposure are irrelevant to the causal effect (Westreich, 2019). For instance, if the intervention under investigation is text message reminders to promote medication adherence, consistency is the assumption that variations in timing of message delivery (e.g., morning or night) are not important for the effect of reminders.\n\nWestreich, D. (2019). Epidemiology by Design: A Causal Approach to the Health Sciences. Oxford University Press, Incorporated.\n\nTHREATS TO INTERNAL VALIDITY\nThese causal inference assumptions will be likely familiar to anyone who uses the potential outcomes framework or Pearl’s graphical causal models approach, but folks with a psychology or education background might be more accustomed to identifying and avoiding threats to internal validity in the Campbell tradition (Shadish et al., 2002). You’ll recall from earlier in this chapter that internal validity pertains to the robustness of a causal claim that the observed variation in 𝑌 is caused by 𝑋. A study with a weak causal claim would be said to have low internal validity. Matthay & Glymour (2020) offer a helpful crosswalk between potential outcomes, DAGs, and the Campbell tradition, and West et al. (2010) compare Campbell’s approach with the potential outcomes approach (sometimes called the Rubin Causal Model).\n\nMatthay, E. C., & Glymour, M. M. (2020). A graphical catalog of threats to validity: Linking social science with epidemiology. Epidemiology, 31(3), 376.\n\nWest, S. G. et al. (2010). Campbell’s and rubin’s perspectives on causal inference. Psychological Methods, 15(1), 18.\n\n\n\nQuantitude Podcast, Episode 26, The Internal Validity Pre-Flight Checklist.\n\nIn the Campbell tradition, researchers are taught to identify the possible threats to internal validity, practice “control by design”, and strive for “coherent pattern matching”. In short, this means to think about the alternative plausible explanations for causal effects (threats), add design elements such as additional comparison groups to remove or reduce these threats (control by design), and make more complex hypotheses to raise the inferential bar and strengthen one’s causal claim (coherent pattern matching).\nPlausible alternative explanations for the causal effect are threats to internal validity. Shadish et al. (2002) enumerate eight specific threats and warn that threats can accumulate (see Table 5.1). The core threat is confounding, which Shadish et al. (2002) frame as a selection threat.\n\nShadish, W. R. et al. (2002). Experimental and quasi-experimental designs for generalized causal inference. Cengage Learning.\n\n\n\n\n\nTable 5.1:  Threats to internal validity from Shadish et al. (2002). \n \n  \n    Threat \n    Description \n  \n \n\n  \n    Ambiguous temporal precedence \n    Lack of clarity about which variable occurred first may yield confusion about which variable is the cause and which is the effect. \n  \n  \n    Selection \n    Systematic differences over conditions in respondent characteristics that could also cause the observed effect. \n  \n  \n    History \n    Events occurring concurrently with treatment could cause the observed effect. \n  \n  \n    Maturation \n    Naturally occurring changes over time could be confused with a treatment effect. \n  \n  \n    Regression artifact \n    When units are selected for their extreme scores, they will often have less extreme scores on other variables, an occurrence that can be confused with a treatment effect. \n  \n  \n    Attrition \n    Loss of respondents to treatment or to measurement can produce artifactual effects if that loss is systematically correlated with conditions. \n  \n  \n    Testing \n    Exposure to a test can affect test scores on subsequent exposures to that test, an occurrence that can be confused with a treatment effect. \n  \n  \n    Instrumentation \n    The nature of a measure may change over time or conditions in a way that could be confused with a treatment effect. \n  \n\n\n\n\n\n\nInternal validity is not a property of your research design per se; it’s a characteristic of your claim. While it’s true that some designs, like randomized controlled trials, face fewer threats to internal validity in theory, it’s also true that study implementation matters. The RCT label does not insulate a poorly conducted RCT from criticism. It’s best to think consider the internal validity of claims on a study-by-study basis. Also, there is not a statistical test that will tell you if your claim has high internal validity. You can use the results of various statistical tests to probe the support for your causal claim, but no test will tell you that a claim has high or low internal validity. P-values, for instance, are silent on the issue.\n\n\nViolations of assumptions like exchangability, positivity, and consistency threaten the internal validity of a claim, but the Campbell tradition does not use these terms."
  },
  {
    "objectID": "causalinference.html#coda-coffee-saves-lives",
    "href": "causalinference.html#coda-coffee-saves-lives",
    "title": "5  Causal Inference",
    "section": "5.5 Coda: Coffee Saves Lives?",
    "text": "5.5 Coda: Coffee Saves Lives?\nBefore we wrap-up this chapter, let’s return to the opening question: Does drinking a cup of coffee daily lower the risk of dying? Ding et al. (2015) suggested that the answer is yes. What do you think?\n\nDing, M. et al. (2015). Association of coffee consumption with total and cause-specific mortality in 3 large prospective cohorts. Circulation, 132(24), 2305–2315.\nThe authors took a confounder-control approach to the analysis, but they did not share a DAG. All of the variables in Figure 5.14 are covariates they mentioned adjusting for in their analysis. Is this the complete set of relevant variables? What are the backdoors that need closing? Are there any colliders?\n\n\n\n\nFigure 5.14: What’s in the DAG?\n\n\n\nCausal inference is hard, especially when dealing with observational data. We can’t rely on data or statistics alone to answer causal questions. In short, we need causal models."
  },
  {
    "objectID": "causalinference.html#keep-learning",
    "href": "causalinference.html#keep-learning",
    "title": "5  Causal Inference",
    "section": "5.6 Keep Learning",
    "text": "5.6 Keep Learning\n\nScott Cunningham’s “Causal Inference: The Mixtape”, ghr.link/cun\nNick Huntington-Klein’s “The Effect: An Introduction to Research Design and Causality”, ghr.link/eff\nRichard McElreath’s “Statistical Rethinking”, ghr.link/sre\nMorgan and Winship’s “Counterfactuals and Causal Inference: Methods and Principles for Social Research”, ghr.link/mor\nHernán and Robins’, “Causal Inference: What If”, ghr.link/wha\nDaniel Westreich’s, “Epidemiology by Design”, ghr.link/wes\nScott Cunningham’s, “A Selected History of Quantitative Causal Inference”, ghr.link/sub"
  },
  {
    "objectID": "construct.html#using-conceptual-models-to-plan-study-measurement",
    "href": "construct.html#using-conceptual-models-to-plan-study-measurement",
    "title": "6  Measurement and Construct Validation",
    "section": "6.1 Using Conceptual Models to Plan Study Measurement",
    "text": "6.1 Using Conceptual Models to Plan Study Measurement\n\n\nIt’s frustrating to get to the analysis phase and realize you are missing key variables. Ask me how I know. The best protection against this outcome is to plan your analysis upfront, including data simulation and mocking up tables and figures. It’s more effort in the study design phase, but not more effort overall.\nThe first step in planning study measurement is to decide what to measure. Consider starting this process by creating a conceptual model, such as a DAG or a theory of change. Conceptual models can help you identify what data you must collect or obtain to answer your research question.\n\nDAG EXAMPLE\nFor instance, Barnard-Mayers et al. (2022) created the DAG shown in Figure 6.3 to plan a causal analysis of the effect of the human papillomavirus (HPV) vaccine on abnormal cervical cytology among girls with perinatal HIV infection. The process of creating the DAG helped the authors to (i) identify a sufficient adjustment set that closes all backdoor paths, and (ii) understand the limitations of the available dataset. With this DAG in hand, they realized that they lacked good data on sexual history, structural racism, and maternal history, leaving their proposed analysis susceptible to confounding. It was back to the drawing board.\n\nBarnard-Mayers, R. et al. (2022). A case study and proposal for publishing directed acyclic graphs: The effectiveness of the quadrivalent human papillomavirus vaccine in perinatally HIV infected girls. Journal of Clinical Epidemiology, 144, 127–135.\n\n\n\n\nFigure 6.3: A proposed causal DAG by Barnard-Mayers et al. (2022), at ghr.link/hpv.\n\n\n\n\n\nLOGIC MODEL EXAMPLE\nFor intervention studies, a theory of change or logic model can aid in measurement planning. To see this in practice, let’s return to the paper by Patel et al. (2017), introduced in Chapter X, that reports on the results of a randomized controlled trial in India to test the efficacy of a lay counsellor-delivered, brief psychological treatment for severe depression called the Healthy Activity Program, or HAP. I encourage you to pause here, read the article, and create your own logic model for the HAP intervention. Figure 6.4 displays my understanding of the HAP logic model.\n\nFor a refresher on conceptual models, see Chapter X.\n\n\n\n\n\nFigure 6.4: HAP logic model.\n\n\n\n\nInputs\nInputs are the resources needed to implement a program. HAP inputs included money and the intervention curriculum (Anand, Arpita and Chowdhary, Neerja and Dimidjian, Sona and Patel, Vikram, 2013). A key part of any cost-effectiveness analysis is an accounting of expenditures (see Chapter X). On average, HAP cost $66 per person to deliver.\n\nAnand, Arpita and Chowdhary, Neerja and Dimidjian, Sona and Patel, Vikram. (2013). Healthy Activity Program.\n\n\nActivities\n\n\n\n\n\nWatch Dr. Vikram Patel talk about task sharing to scale up the delivery of mental health services in low-income settings.\n\nThe main HAP activities were psychotherapy for patients and supervision of lay counselors. HAP was designed to be delivered in an individual, face-to-face format (telephone when necessary) over 6 to 8 weekly sessions each lasting 30 to 40 minutes (Chowdhary et al., 2016). Supervision consisted of weekly peer-led group supervision and twice monthly individual supervision.\n\nChowdhary, N. et al. (2016). The healthy activity program lay counsellor delivered treatment for severe depression in india: Systematic development and randomised evaluation. The British Journal of Psychiatry, 208(4), 381–388.\nIn intervention studies like this, it’s important to determine if the intervention was delivered as intended. This is called treatment fidelity, and it’s a measure of how closely the actual implementation of a treatment or program reflects the intended design. The study authors measured fidelity in several ways, including external ratings of a randomly selected 10% of all intervention sessions. An expert not involved in the program listened to recorded sessions and compared session content against the HAP manual. They also had counselors document the duration of each session.\n\n\n\n\n\n\n\nImplementation failure vs theory failure\n\n\n\nLow treatment fidelity usually results in an attenuation (i.e., shrinking) of treatment effects, which is a threat to internal validity. If the study shows no effect but treatment fidelity is low, the null result may not be valid. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it is not possible to truly replicate the study.\n\n\n\n\n\nOutputs\nOutputs are counts of activities. Patel et al. counted the number of sessions delivered to patients in the treatment arm, as well as the number of these patients who completed the program (69% had a planned discharge). Presumably they also tracked the number of counselors trained and supervision sessions conducted.4\n\n\n\n\n\n\n\nNon-Compliance with study assignment\n\n\n\nOutputs are key reporting metrics for staff working on the “Monitoring” side of M&E units (see Chapter 1), but intervention researchers also need access to monitoring data to understand and describe how the program was delivered. This is related to treatment fidelity, but also to treatment compliance. Treatment compliance is a measure of the extent to which people were treated or not treated according to their study assignment. Sometimes people assigned to the treatment group do not take-up the treatment, or they complete only part of the planned intervention.\nNoncompliance to randomization on the treatment side is called one-sided noncompliance. When members of the control group5 are also noncompliant with randomization, meaning they are treated despite being assigned to the no treatment condition, this is called two-sided noncompliance. Analysis strategies for one- and two-sided noncompliance are discussed in a later chapter.\nTreatment compliance is one of those terms like “research subjects” that can make you cringe a bit. Research is voluntary, and participants have the right to decline a treatment offer or stop treatment at any point. We call this behavior “non-compliance”, which sounds to some like participants are misbehaving. Non-compliance can make your analysis and interpretation more complicated, but participants are not to blame. Look instead to the root causes in the research design, study procedures, or the intervention itself to find ways to limit non-compliance.\n\n\n\n\n\nOutcomes and impacts\nThe hypothesized outcome in the HAP study was a reduction in depression:\n\nThe two primary outcomes were depression severity assessed by the modified Beck Depression Inventory version II (BDI-II) and remission from depression as defined by a PHQ-9 score of less than 10, both assessed 3 months after enrollment.\n\nThe authors assumed that the long-term impact of reducing depression at scale would be improvements in quality of life for patients and their families, increased workforce productivity, and a reduction in costs to society.\n\n\n\nFROM CONCEPTUAL MODEL TO STUDY MEASUREMENT\nBoth of these examples provide a solid foundation for measurement planning. If you or I were designing the HAP study, for instance, the example logic model would tell us that we need to collect or obtain data about the following:\n\nExpenditures\nMeasures of treatment fidelity\nCounts of therapy sessions completed, supervision sessions held\nMeasures of depression and several secondary outcomes\n\nGenerating this list is the first step. The next step involves a deep dive on the specifics of measurement. For instance, what is depression, and how can it be quantified?\n\n\n\n\n\n\n\nLong-term impacts often not measured\n\n\n\nMissing from this list are proposed impacts: measures of quality of life, productivity, and societal costs. This is because long-term impacts are often assumed but not measured. In most cases, study timelines are too short to detect long-term (or distal) effects. Often, the best we can do is measure proximal effects in the near term. For instance, if the proposed mechanism of change is intervention ➝ more days in school ➝ greater earnings as adults, a study would need to collect data after a few decades! There are examples of long running cohort studies that follow people over big spans of time, but these studies are exceptions."
  },
  {
    "objectID": "construct.html#measurement-terminology",
    "href": "construct.html#measurement-terminology",
    "title": "6  Measurement and Construct Validation",
    "section": "6.2 Measurement Terminology",
    "text": "6.2 Measurement Terminology\nTable 6.1 lists four measurement terms to know.\n\n\n\n\n\nTable 6.1:  Common measurement terms, adapted from Glennerster and Takavarasha (2013). \n \n  \n    Term \n    Definition \n    Example \n  \n \n\n  \n    Construct \n    A characteristic, behavior, or phenomenon to be assessed and studied. Often cannot be measured directly (latent). \n    Depression \n  \n  \n    Outcome (or Endpoint) \n    The hypothesized result of an intervention, policy, program, or exposure. \n    Decreased depression severity \n  \n  \n    Indicator \n    Observable measures of outcomes or other study constructs. \n    Depression severity score \n  \n  \n    Instrument \n    The tools used to measure indicators. \n    A depression scale (questionnaire) made up of questions (items) about symptoms of depression that is used to calculate a severity score \n  \n\n\n\n\n\n\n\nCONSTRUCTS\nAt the top of the list are study constructs. Constructs are the high-level characteristics, behaviors, or phenomena you investigate in a study. Constructs are the answer to the question, “What is your study about?”.\nIn the Patel et al. (2017) example, the key construct of interest was depression. Constructs like depression have no direct measure; there is not (yet) a blood test that indicates whether someone is depressed or “has depression”. Therefore, depression is an example of a latent construct. Many constructs in the social and behavioral sciences are latent constructs—such as empowerment, corruption, democracy.\n\n\nOUTCOMES AND ENDPOINTS\nOutcomes (or endpoints) are the specific aspects of a construct that you are investigating. In intervention research, program evaluation, and causal inference more generally, outcomes are often framed as the hypothesized result of an intervention, policy, program, or exposure. In a theory of change or logic model, outcomes take on the language of change: increases and decreases.\nIn the clinical trial literature, study targets are also called endpoints. Death. Survival. Time to disease onset. Blood pressure. Tumor shrinkage. These are all endpoints you might seek to measure after offering some treatment (e.g., an experimental drug).\n\n\nYou’ll also see outcomes and endpoints referred to as dependent variables, response variables, 𝑌, or left-hand side variables (referring to an equation).\nMost studies are designed to generate evidence about one or two primary outcomes linked directly to the main study objective. In the HAP study, Patel et al. (2017) hypothesized two primary outcomes: a reduction in severe depression and a reduction in the prevalence of depression.\nSecondary outcomes may be registered, investigated, and reported as well, but these analyses will often be framed as exploratory in nature if the study design is not ideal for measuring these additional outcomes. Patel et al. (2017) specified several secondary outcomes, including increases in behavioural activation and reductions in disability, total days unable to work, suicidal thoughts or attempts, intimate partner violence, and resource use and costs of illness (Patel et al., 2014).\n\nPatel, V. et al. (2014). The effectiveness and cost-effectiveness of lay counsellor-delivered psychological treatments for harmful and dependent drinking and moderate to severe depression in primary care in India: PREMIUM study protocol for randomized controlled trials. Trials, 15, 101.\n\n\nINDICATORS AND INSTRUMENTS\nIndicators are observable metrics of outcomes, endpoints, or other study constructs.\n\n\nThe language of qualitative studies is a bit different. These studies emphasize study constructs, but not indicators or measures. Quantification is not typically the goal.\n\n\n\n\n\n\n\nIndicator categories\n\n\n\nIn intervention and evaluation research—which is only a subset of quantitative research, remember—there are three main categories of indicators:\n\nInput indicators: Measures of what is needed to implement the program.\nProcess indicators: Measures of program implementation (e.g., fidelity).\nOutcome indicators: Measures of the program or study endpoints (hypothesized outcomes or impacts).\n\n\n\n\nIndicators and instruments go together. Instruments are the tools used to measure indicators. Instruments can take many forms, including surveys, questionnaires, environmental sensors, anthropometric measures, blood tests, imaging, satellite imagery, and the list goes on.\nReturning to the HAP study, Patel et al. (2017) hypothesized that the intervention would reduce severe depression and the prevalence of depression (the primary outcomes). Table 6.2 summarizes how each was operationalized and measured.\n\n\n\n\n\nTable 6.2:  HAP outcomes, indicators, and instruments. \n \n  \n    Outcome \n    Indicator \n    Instrument \n  \n \n\n  \n    Depression severity \n    a continuous measure of severity where higher scores suggest someone is experiencing more severe symptoms of depression \n    assessed with the Beck Depression Inventory, version II \n  \n  \n    Depression prevalence \n    a binary indicator of the presence of depression based on a person’s depression score relative to a reference cutoff score; < 10 on the Patient Health Questionnaire-9) \n    assessed with the Patient Health Questionnaire-9 \n  \n\n\n\n\n\n\nThe HAP study instruments are reproduced in Figure 6.5. The authors measured depression with two instruments: (i) the 21-item Beck Depression Inventory version II (Beck et al., 1996); and (ii) the 9-item Patient Health Questionnaire-9 (Kroenke et al., 2002).\n\nBeck, A. T. et al. (1996). Manual for the beck depression inventory-II. Psychological Corporation.\n\nKroenke, K. et al. (2002). The PHQ-9: A new depression diagnostic and severity measure. Psychiatric Annals, 32(9), 509–515.\nResponses to each BDI-II item are scored on a scale of 0 to 3 and summed to create an overall depression severity score that can range from 0 to 63, where higher scores indicate more severe depression. PHQ-9 responses are also scored on a scale of 0 to 3 based on the frequency of symptoms and summed to create a total score with a possible range of 0 to 27. Based on prior clinical research, the HAP authors defined the cutoff for depression as a score of at least 10 on the PHQ-9.\n\n\n\n\nFigure 6.5: Instruments used in Patel et al. (2017). Source: ghr.link/ins."
  },
  {
    "objectID": "construct.html#what-makes-a-good-indicator",
    "href": "construct.html#what-makes-a-good-indicator",
    "title": "6  Measurement and Construct Validation",
    "section": "6.3 What Makes a Good Indicator?",
    "text": "6.3 What Makes a Good Indicator?\nWhen you select and define indicators of outcomes and other key variables, this is called operationalizing your constructs. Operationalization a critical part of measurement planning. When you finish the study and present your findings, one of the first things colleagues will ask is, “How did you define and measure your outcome?” Hopefully you can say that your indicators are DREAMY™.\n\n\nSMART is another acronym worth knowing. SMART indicators are Specific, Measurable, Achievable, Relevant, and Time-Bound.\n\n\n\nDefined\nclearly specified\n\n\nRelevant\nrelated to the construct\n\n\nExpedient\nfeasible to obtain\n\n\nAccurate\nvalid measure of construct\n\n\nMeasurable\nable to be quantified\n\n\ncustomarY\nrecognized standard\n\n\n\n\nDEFINED\nIt’s important to clearly specify and define all study variables, especially the indicators of primary outcomes. This is a basic requirement that enables a reader to critically appraise the work, and it serves as a building block for future replication attempts.\nPatel et al. (2017) defined two indicators of depression:\n\nDepression severity: BDI-II total score measured at 3 months after the treatment arm completed the intervention\nDepression prevalence: the proportion of participants scoring 10 or higher on the PHQ-9 total score measured at 3 months post intervention\n\n\n\nRELEVANT\nIndicators should be relevant to the construct of interest. For instance, scores on the BDI-II and PHQ-9 are clearly measures of depression. An example of an irrelevant indicator would be a total score on the Beck Anxiety Inventory, a separate measure of anxiety. While anxiety and depression are often comorbid, anxiety is a distinct construct.\n\n\nEXPEDIENT\nIt should be feasible to collect data on the indicator given a specific set of resource constraints. Asking participants to complete a 21-item questionnaire and a 9-item questionnaire, as in the HAP study, does not represent a large burden on study staff or participants. However, collecting and analyzing biological samples (e.g., hair, saliva, or blood) might be too difficult in some settings.\n\n\nACCURATE\nAccurate is another word for “valid”. Indicators must be valid measures of study constructs (a topic discussed extensively later in this chapter). When deciding on indicators and instruments, the HAP authors had to ask themselves whether scores on the BDI-II and PHQ-9 distinguish between depressed and non-depressed people in their target population. The authors cited their own previous work to support the decision to use these instruments (Patel et al., 2008).\n\nPatel, V. et al. (2008). Detecting common mental disorders in primary care in india: A comparison of five screening questionnaires. Psychological Medicine, 38(2), 221–228.\n\n\nMEASUREABLE\nIndicators must be quantifiable. Psychological constructs like depression are often measured using questionnaires like the BDI-II and the PHQ-9. Other constructs require more creativity. For instance, how would you measure government corruption? Asking officials to tell on themselves isn’t likely to yield a helpful answer. Olken (2005) took a different approach in Indonesia—they dug core samples of newly built roads to estimate the true construction costs. The authors then compared cost estimates based on these samples to the government’s reported construction expenditures to construct a measure of corruption (reported expenditures > estimated costs).\n\nOlken, B. A. (2005). Monitoring corruption: Evidence from a field experiment in indonesia. National Bureau of Economic Research.\n\n\nCUSTOMARY\nIn general, it’s good advice to use standard indicators, follow existing approaches, and adopt instruments that have already been established in a research field. There are several ways to do this.\n\n\nSometimes the status quo stinks and you’ll want to conduct a study to overcome the limitations of the standard methods.\nOne way is to read the literature and find articles that measure your target constructs. For example, if you’re planning an impact evaluation of a microfinance program on poverty reduction and wish to publish the results in an economics journal, start by reading highly cited work by other economists to understand current best practices. How do these scholars define and measure outcomes like income, consumption, and wealth?\nSystematic reviews and methods papers are also good resources for learning about measurement. For instance, Karyotaki et al. (2022) critically appraised several task sharing mental health interventions, including HAP. Their review is a resource for for understanding how depression is operationalized and measured across studies. Larsen et al. (2021) evaluated nine commonly used depression screening tools in sub-Saharan Africa on the basis of their diagnostic performance, cultural adaptation, and ease of implementation. If you wanted to measure depression in a target population in sub-Saharan Africa, their paper should be high on your reading list.\n\nKaryotaki, E. et al. (2022). Association of task-shared psychological interventions with depression outcomes in low-and middle-income countries: A systematic review and individual patient data meta-analysis. JAMA Psychiatry.\n\nLarsen, A. et al. (2021). Is there an optimal screening tool for identifying perinatal depression within clinical settings of sub-saharan africa? SSM-Mental Health, 1, 100015.\n\nWHO. (2018). Global reference list of 100 core health indicators. World Health Organization.\n\nUnited Nations. (2023). SDG indicators.\nA third approach is to search for nationally or internationally recognized standards. If studying population health, for instance, a good source of customary indicators is the World Health Organization’s Global Reference List of the 100 core health indicators (WHO, 2018). Another good source of customary indicators for population health is the United Nations Sustainable Development Goals (SDG) metadata repository, which includes 231 unique indicators to measure 169 targets for 17 goals (United Nations, 2023)."
  },
  {
    "objectID": "construct.html#constructing-indicators",
    "href": "construct.html#constructing-indicators",
    "title": "6  Measurement and Construct Validation",
    "section": "6.4 Constructing Indicators",
    "text": "6.4 Constructing Indicators\nSome indicators are based on a single measurement and require only a definition. For instance, a hemoglobin level of less than 7.0 g/dl is an indicator of severe anemia. If you were evaluating the impact of a new diet on severe anemia, you would need only to record the result of a blood test (instrument). Lucky you. Most indicators are more complex and must be constructed.\n\nNUMERATORS AND DENOMINATORS\nPopulation-level global health indicators often involve numerators and denominators. For instance, the WHO defines the maternal mortality ratio as (World Health Organization):\n\nWorld Health Organization. Maternal mortality ratio (per 100 000 live births).\n\nThe denominator for the maternal mortality rate is the number of women of reproductive age.\n\n\nthe number of maternal deaths during a given time period per 100,000 live births during the same time period\n\nConstructing this indicator requires data on the counts of maternal deaths and live births. Each has a precise definition.\n\n\n\nMaternal deaths\nThe annual number of female deaths from any cause related to or aggravated by pregnancy or its management (excluding accidental or incidental causes) during pregnancy and childbirth or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy.\n\n\nLive births\nThe complete expulsion or extraction from its mother of a product of conception, irrespective of the duration of the pregnancy, which, after such separation, breathes or shows any other evidence of life such as beating of the heart, pulsation of the umbilical cord, or definite movement of voluntary muscles, whether or not the umbilical cord has been cut or the placenta is attached\n\n\n\n\n\nCOMPOSITE INDICATORS\nLatent (unobservable) constructs like empowerment, quality of life, and depression, and some manifest (observable) constructs like wealth, are often measured with multiple items on surveys or questionnaires and then combined into indexes or scales.\nThe terms index and scale are often used interchangeably, but they are not quite synonyms. While they share in common the fact that multiple items or observations go into their construction, making them composite measures or composite indicators, the method for and purpose of combining these items or observations are distinct (see Figure 6.6).\nIn an index, indicators give rise to the construct that is being measured. For example, a household’s wealth is determined by the assets it owns (e.g., livestock, floor quality). Conversely, in a scale, the indicators exist because of the construct. Depression manifests in symptoms such as a loss of appetite.\n\n\n\n\nFigure 6.6: Scale vs index.\n\n\n\n\nIndexes\n\n\n\n\n\nWatch this introduction to the Equity Tool, a wealth index alternative.\n\nIndexes combine items into an overall composite, often without concern for how the individual items relate to each other. For instance, the Dow Jones Industrial Average is a stock-market index that represents a scaled average of stock prices of 30 major U.S. companies such as Disney and McDonald’s. Companies with larger share prices have more influence on the index. The Dow Jones is a popular indicator of market strength and is constantly monitored during trading hours.\nAn index popular in the global health field is the wealth index. The wealth index uses household survey data on assets as a measure of household economic status (Rutstein et al., 2004). The data come from national surveys conducted by the Demographic and Health Surveys (DHS) Program (DHS, n.d.). Asset variables include individual and household assets (e.g., phone, television, car), land ownership, and dwelling characteristics, such as water and sanitation facilities, housing materials (i.e., wall, floor, roof), persons sleeping per room, and cooking facilities. A household gets an overall score that is the sum of the weights for having (or not having) each asset.\n\nRutstein, S. O. et al. (2004). The DHS wealth index (DHS Comparative Reports No. 6). ORC Macro.\n\nDHS. (n.d.). Demographic and Health Surveys (DHS) Program.\n\nA common way to present wealth index scores is to divide the sample distribution into quintiles. Each household in the sample falls into 1 of 5 wealth quintiles reflecting their economic status relative to the sample, from poorest (1st quintile) to richest (5th quintile).\n\n\n\n\n\n\nWatch Dr. Selim Jahan, former lead author of the Human Development Report, discuss the history of the Human Development Index.\n\nAnother widely known index among global health practitioners is the Human Development Index, or HDI (UNDP, n.d.). It combines country-level data on three dimensions: (i) life expectancy at birth; (ii) expected years of schooling for kids entering school and mean years of schooling completed by adults; and (iii) Gross National Income per capita. The HDI is produced by the United Nations Development Program.\n\nUNDP. (n.d.). Human Development Index.\n\n\n\n\nFigure 6.7: Human Development Index. Source: UNDP.\n\n\n\n\n\nScales\nScales also combine items into an overall composite, but unlike the components of most indexes, scale items should be intercorrelated because they stem from a common, latent cause. For instance, the BDI-II used in the HAP trial was designed to measure the construct of depression with 21 related items that ask people to report on their experience of common problems. Figure 6.8 shows how these items were correlated in the Patel et al. (2017) data.6\n\nPatel, V. et al. (2017). The healthy activity program (HAP), a lay counsellor-delivered brief psychological treatment for severe depression, in primary care in india: A randomised controlled trial. The Lancet.\n\n\n\n\nFigure 6.8: BDI-II correlations in the HAP trial.\n\n\n\nThis heatmap visualizes data from participants in the control arm, collected three months after the treatment arm completed the HAP program. These correlations can range from -1 to 1, and most item pairs show moderately sized, positive correlations in the 0.2 to 0.3 range. An exception on the low side is bdi02 (tiredness) and bdi04 (changes in appetite). With a correlation coefficient of 0, it seems like there is no association between tiredness and changes in appetite. bdi04 (changes in appetite) also has a few other small correlations with other items, which could suggest that, in this population, changes in appetite often do not manifest alongside other symptoms of depression.\n\n\nDoes this mean that bdi04 (changes in appetite) should have been excluded from the outcome measure? Possibly. I’ll return to this question later in this chapter.\n\n\nIndex and Scale Construction\nA key decision in creating composite indicators like the wealth index and the BDI-II scale score is how to weight the individual components. In the case of the wealth index, should owning a car be given the same weight as owning a phone in the construction of wealth? When it comes to measuring depression severity, should feeling sad be given the same weight as feeling suicidal?\nThe answer is ‘no’ for the wealth index. The wealth index is constructed using relative weights derived from a data reduction technique called principal component analysis, or PCA, in which indicators are standardized (i.e., transformed into z-scores) so that they each have a mean of 0 and a standard deviation of 1. A principal component is a linear combination of the original indicators; thus, every indicator (e.g., yes/no response to owning a phone) has a loading factor that represents the correlation between the individual indicator and the principal component. The first principal component always explains the most variance, so the factor loadings on the first principal component are assigned as the weights for each asset in the index. A household gets an overall score that is the sum of the weights for having (or not having) each asset.\n\n\n\nVisit the DHS Program’s website for country files and detailed instructions to construct the wealth index.\n\nLet’s look at an example. Figure 6.9 shows how one item—a household’s water source—contributes to the construction of the overall index. On the left is the wording of the survey item asked to each household in the India 2015-15 DHS Survey. On the right are the PCA results provided by the DHS Program. If a survey respondent said their household has water piped into its dwelling, the household’s wealth index score would be—in part, this is only one item—the sum of the values for having and not having each type of water source. Scan the values and you’ll see that safer sources of water such as piped water into the dwelling get higher scores (0.102) compared to possibly contaminated sources such as surface water from a lake or stream (-0.055). These relative weights are summed to make the overall wealth index score for each household.\n\n\n\n\nFigure 6.9: Wealth index construction example.\n\n\n\nIn contrast, the BDI-II—and many scales like it—use equal weighting (or unit weighting). Responses to each BDI-II item are scored on a scale of 0 to 3 and summed to create an overall depression severity score that can range from 0 to 63. Figure 6.10 shows data from four people in the HAP study. Each item contributes equally to the sum score (bdi_total).\n\n\nThe possible range is 0 to 60 in the case of Patel et al. (2017) because they purposively omitted one item.\n\n\n\n\nFigure 6.10: BDI-II data example.\n\n\n\nAn alternative construction method for scales like the BDI-II is optimal weighting with a congeneric model (McNeish et al., 2020). In this method, items more closely related to the construct are weighted more heavily in the construction of the scale score. We can do this in R using a package for confirmatory factor analysis such as the {lavaan} package (Rosseel, 2012).\n\nRosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36.\n\n# specify the model\n    model <- '\n    total_factor =~ bdi01 + bdi02 + bdi03 + bdi04 + bdi05 + \n                    bdi06 + bdi07 + bdi08 + bdi09 + bdi10 +\n                    bdi11 + bdi12 + bdi13 + bdi14 + bdi15 + \n                    bdi16 + bdi17 + bdi18 + bdi19 + bdi20\n  '\n  \n# fit the model\n    fit <- lavaan::cfa(\n        model, data = df, ordered = TRUE, std.lv = TRUE\n        )\n\n# plot a path diagram with model coefficients\n    lavaanPlot::lavaanPlot(\n        model = fit, \n      node_options = list(shape = \"box\"), \n    edge_options = list(color = \"grey\"), \n    labels = list(total_factor = \"Depression Severity\"),\n    coefs = TRUE\n    )\n\nFigure 6.11 shows a path diagram of a congeneric factor model for the BDI-II data from the HAP trial control arm at the 3-month endline. The loadings from the latent depression severity score are uniquely estimated for each item. The items bdi14 (feeling like a failure) and bdi17 (indecisiveness) have the highest loading of 0.76, meaning these items are most closely related to the construct of depression severity. Thus, these items contribute the most to the overall factor score. bdi04 (changes in appetite) has the weakest relationship to the construct and contributes the least to the factor score. (Remember, in the commonly used equal weighting approach, these items would contribute equally.)\n\n\n\n\nFigure 6.11: Path diagram of a congeneric factor model for the BDI-II fit to data from the HAP trial control arm at the 3-month endline.\n\n\n\nDoes it matter which construction method you choose, equal weighting sum scores or optimal weighting factor scores? McNeish et al. (2020) argue that it can. Figure 6.12 demonstrates that two people with the same sum score can have substantially different factor scores.\n\nHigher sum scores and higher factor scores represent more severe depression. The factor score is on a standardized Z-scale, so negative numbers indicate below average severity.\n\n\n\n\n\nFigure 6.12: Sum scores vs factor scores. This plot highlights two individuals with equal sum scores (11) but different factor scores (-0.63 vs -1.35).\n\n\n\nTwo people with identical sum scores but different factor scores are highlighted in orange in Figure 6.12. In Figure 6.13 I reproduce their responses to each BDI item. Person A endorsed several items with the highest loadings, which helps to explain their more severe factor score. Person B endorsed fewer items overall, but endorsed them strongly.\n\n\n\n\nFigure 6.13: Different pattern of item endorsement from two people with identical BDI-II sum scores. 0=no endorsement. 3=highest endorsement.\n\n\n\nSo do you think Person A and B have the same level of depression severity, as suggested by equivalent sum scores of 11? Or are they experiencing differing levels of severity as suggested by the factor scores? If you said different, you might prefer to construct optimally weighted factor scores, especially if you are using the score as an outcome indicator in a trial.\n\n\nIn a clinical setting the ease of calculating sum scores might outweigh other considerations.\n\nMcNeish, D. et al. (2020). Thinking twice about sum scores. Behavior Research Methods, 52, 2287–2305.\nMcNeish et al. (2020) make some helpful suggestions that you can revisit when you are faced a decision about how to construct scale scores. For now, I think a good takeaway is that measurement is complicated and we should think hard about what numbers mean and how scores are constructed. The next two sections will help you do just that."
  },
  {
    "objectID": "construct.html#construct-validation",
    "href": "construct.html#construct-validation",
    "title": "6  Measurement and Construct Validation",
    "section": "6.5 Construct Validation",
    "text": "6.5 Construct Validation\n\n\n\nThis Open Science Framework project website hosts a comprehensive reading list on study measurement.\n\nConstruct validation is the process of establishing that the numbers we generate with a method of measurement actually represent the idea—the construct—that we wish to measure (Cronbach et al., 1955). Using the BDI-II example, where sum scale scores can range from 0 to 63, validating this construct means establishing that higher numbers correspond with greater depression severity, or that scores above a certain threshold, such as 29 (out of 63), correctly classify someone as having “severe depression”. If our numbers don’t mean what we think they mean, our analyses don’t either.\n\nCronbach, L. J. et al. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281.\n\nFlake, J. K. et al. (2022). Construct validity and the validity of replication studies: A systematic review. American Psychologist, 77(4), 576.\nYou might be thinking that construct validation is not a top concern in your work because you, a principled scientist, are using “validated” scales. If so, you’d be wrong. Validity is not a property of an instrument. Flake et al. (2022) make this point clearly:\n\nThus, validity is not a binary property of an instrument, but instead a judgment made about the score interpretation based on a body of accumulating evidence that should continue to amass whenever the instrument is in use. Accordingly, ongoing validation is necessary because the same instrument can be used in different contexts or for different purposes and evidence that the interpretation of scores generalizes to those new contexts is needed.\n\nI won’t argue that you must always start from scratch to validate the instruments you select, but it’s important to think critically about why you believe an instrument will produce valid results in your context. For instance, if you are using an instrument originally validated with a sample of 200 white women in one small city in America, what gives you confidence that the numbers produced carry the same meaning in rural India?\n\nPHASES OF CONSTRUCT VALIDATION\nLoevinger (1957) outlined three phases of construct validation: substantive, structural, and external. The goal of the substantive phase is to ensure that the content of the instrument is comprehensive and presented in a manner that makes sense to people. Once this phase is satisfied, you move onto the structural phase where you gather data and analyze the psychometric properties of the instrument and its items. If acceptable, you proceed to the external phase where scores generated by the instrument are compared to other instruments or criteria. Table 6.3 provides examples of validity evidence for each phase (inspired by Flake et al. (2017)).\n\nLoevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological Reports, 3(3), 635–694.\n\nFlake, J. K. et al. (2017). Construct validation in social and personality research: Current practice and recommendations. Social Psychological and Personality Science, 8(4), 370–378.\n\n\n\n\n\n\nTable 6.3:  Stages of construct validation \n  \n    \n    \n    \n    \n  \n  \n    \n    \n      Phase\n      Validity Evidence\n      Question\n      Examples\n    \n  \n  \n    Substantive\nContent validity\nWhat topics should be included in the instrument based on theory and prior work?\nComplete a literature review, talk with experts, conduct focus groups to explore local idioms\n    \nItem development\nHow should the construct be assessed?\nConduct cognitive interviewing to ensure local understanding of item wording and response options\n    Structural\nItem analysis\nAre the instrument items working as intended?\nAnalyze patterns of responding, select items that discriminate between cases and non-cases\n    \nFactor analysis\nHow can the observed variables be summarized or represented by a smaller number of unobserved variables (factors)?\nConduct exploratory and/or confirmatory factor analysis\n    \nReliability\nAre responses to a set of supposedly related items consistent within people and over time?\nExamine internal consistency and test-retest correlations for evidence of stability\n    \nMeasurement invariance\nDoes the instrument function equivalently across different groups or conditions?\nConduct multiple group factor analysis, item response theory analysis\n    External\nCriterion validity\nHow well does the instrument predict or correlate with an external criterion or outcome of interest?\nEstablish concurrent validity, predictive/diagnostic validity\n    \nConstruct validity\nHow well does the instrument measure the intended construct?\nEstablish convergent/discriminant/known groups validity\n  \n  \n  \n\n\n\n\n\n\nConstruct Validation Phase 1: Substantive\n\n\nIf adopting or adapting an existing instrument, you can still evaluate whether the instrument has evidence of content validity for your study setting and target population.\nThe first phase of developing a new instrument is to identify all of the relevant domains (the content) needed to fully assess the construct of interest. The process often starts with a review of the literature, conversations with experts, and potentially focus groups with members of the target population.\nFor instance, my colleagues and I conducted a study in rural Kenya where we examined how people understood depression in the context of pregnancy and childbirth (Green et al., 2018). We started by reviewing the scholarly literature on how depression was assessed in Kenya and elsewhere. Then we convened groups of women in our target population and asked them to describe what observable features characterize depression (sadness, or huzuni, in Swahili) during the perinatal period. Working with each group, we co-examined the overlap (and lack thereof) of their ideas and existing depression screening tools (see Figure 6.14). A group of Kenyan mental health professionals then gave feedback on the results based on their local clinical expertise.\n\n\n\n\nFigure 6.14: Illustrative sorting of depression cover terms by focus groups, from Green et al. (2018).\n\n\n\nOnce you know the domains to include, you can proceed to create items that assess these domains. Your instrument will have evidence of content validity if you can demonstrate that it assesses all of the conceptually or theoretically relevant domains and excludes unrelated content.\n\n\n\nCheck out this helpful “how to” guide for cognitive interviewing.\n\nAs part of this process, it’s important to ensure that members of your target population understand the meaning of each item and the response scale. In the Kenya study, we used a technique called cognitive interviewing whereby we asked members of the target population to describe the meaning of each item and suggest improvements.\n\n\nConstruct Validation Phase 2: Structural\nThe structural phase comes after you collect pilot data from a sample drawn from your target population. In this phase, you’ll quantitatively evaluate how people respond to the items, identify items that appear to best represent the latent construct(s), and examine whether the items are measured consistently and equivalently across groups.\n\nItem Analysis\nA common initial exploratory data analysis practice is to plot the response distributions of each item. If you ask people to rate their agreement with a statement like, “I feel sad”, and if 100% of people in your sample respond “strongly agree”, the item has zero variance. When all or nearly all of your sample responds the same way to an item, that item tells you nothing useful. The appropriate next step is to drop the item or conduct additional cognitive interviewing to modify the item in a way that will elicit variation in responses.\n\n\nYou might decide to keep an item with low variability if it’s a critical item for clinical detection like suicidal ideation.\nIf you have data that enable you to plot response distributions by group, you can also examine the extent to which items distinguish between groups. Figure 6.15 shows a hypothetical example where 100 people responded to three items, each measured on a 4-point scale from “Never” to “Often”.\n\n\n\n\nFigure 6.15: Visual example of item analysis.\n\n\n\nitem_1 has very little variability. Almost everyone responded “Never”. This item does not tell us much, and I might decide to drop or improve it. item_2 has more variability, but it does not distinguish between cases and non-cases. In combination with other variables it might be useful, so I might decide to keep it unless I need to trim the overall length of the questionnaire. item_3 looks the most promising. It elicits variability in responses, and a larger proportion of the Cases group endorsed the item.\n\n\nFactor Analysis\nFactor analysis is a statistical method that helps us understand hidden patterns and relationships within a large set of data. It looks for commonalities among different variables and groups them into smaller, meaningful categories called factors. By doing this, factor analysis simplifies complex data and allows us to uncover the underlying dimensions or concepts that are influencing the observed patterns.\nThere are two main types of factor analysis: exploratory factor analysis, or EFA, and confirmatory factor analysis, or CFA. EFA is used when we have little prior knowledge about the underlying structure of the variables. It helps in identifying the number of factors and the pattern of relationships among variables.\nOn the other hand, CFA is conducted when we have a pre-specified hypothesis or theory about the factor structure and seeks to confirm whether the observed data align with the proposed model. CFA tests the fit of the predetermined model and assesses the validity of the measurement instrument.\nFlora et al. (2017) discuss when you might use EFA vs CFA:\n\nFlora, D. B. et al. (2017). The purpose and practice of exploratory and confirmatory factor analysis in psychological research: Decisions for scale development and validation. Canadian Journal of Behavioural Science, 49(2), 78.\n\nResearchers developing an entirely new scale should use EFA to examine the dimensionality of the items…CFA should be used when researchers have strong a priori hypotheses about the factor pattern underlying a set of observed variables.\n\nEFA Example\nTo get a glimpse of what this means, let’s imagine that, as members of the HAP study team, we created the BDI-II items from scratch and wanted to examine the dimensionality of the items. One way to do this is to use an R package like {lavaan} to fit EFA models with 1, 2, or 3 factors.\n\n    lavaan::efa(data = df, \n              nfactors = 1:3, \n              rotation = \"oblimin\",\n              estimator = \"WLSMV\",\n              ordered = TRUE)\n\nTable 6.4 displays the factor loadings for the 2-factor model. Factor loadings represent the strength and direction of the relationship between observed variables (i.e., the BDI-II items) and the underlying factors. Think of factor loadings as indicators of how closely each variable is associated with a particular factor. Higher positive factor loadings suggest a strong positive relationship, indicating that the variable is more representative of that factor, while lower or negative factor loadings indicate a weaker or opposite association. These loadings help us understand which variables are most important in measuring a specific factor and contribute to our overall understanding of the underlying structure of the data.\n\n\n\n\n\n\nTable 6.4:  Factor loadings for BDI-II items in a 2-factor exploratory factor analysis model. Data from the HAP trial control arm at the 3-month endline \n  \n    \n    \n    \n    \n  \n  \n    \n      Factor loadings\n    \n    \n    \n      item\n      label\n      f1\n      f2\n    \n  \n  \n    bdi03\nLow energy\n0.71\n\n    bdi06\nSadness\n0.67\n\n    bdi05\nLoss of interest\n0.66\n\n    bdi02\nTiredness\n0.54\n\n    bdi10\nAgitation\n0.48\n\n    bdi16\nConcentration\n0.46\n\n    bdi08\nLoss of pleasure\n0.45\n\n    bdi07\nCrying\n0.43\n\n    bdi14\nFailure\n\n0.89\n    bdi13\nGuilty feelings\n\n0.78\n    bdi20\nSuicidal thoughts\n\n0.73\n    bdi15\nWorthlessness\n\n0.62\n    bdi12\nSelf-dislike\n\n0.6\n    bdi19\nPessimism\n\n0.57\n    bdi18\nPunishment\n\n0.55\n    bdi17\nIndecisiveness\n\n0.49\n    bdi11\nSelf-criticism\n\n0.45\n    bdi01\nSleep\n\n\n    bdi04\nAppetite\n\n\n    bdi09\nIrritability\n\n\n  \n  \n    \n      Loadings less than 0.40 (absolute value) are not presented.\n    \n  \n  \n\n\n\n\n\nWhat we see is a cluster of items that load strongly on factor 1, a cluster of items that load strongly on factor 2, and a few items such as bdi04 (appetite) that are not strongly associated with either factor. The software does not know how to label these factors qualitatively, so it just names them f1 and f2. It’s up to us to examine the pattern of loadings and determine whether the factors have a clear meaning. My sense is that f1 captures the affective dimension of depression (e.g., sadness, crying), whereas f2 is about negative cognition (e.g., guilty feelings, self-dislike).\nBut is a 2-factor model the best way to represent the data? There are many ways we could try to answer this question, but unfortunately there is no consensus about what approach is best. In the code below that produces Figure 6.16, I use the Method Agreement procedure as implemented in the {parameters} package which (currently) looks across 19 different approaches and tallies the votes for the number of factors to extract. The winner is a 1-factor model.\n\n    efa_n_factors <- parameters::n_factors(df)\n    library(see)\n    plot(efa_n_factors)\n\n\n\n\n\nFigure 6.16: Assessing the number of factors to retain for the EFA model.\n\n\n\nCFA Example\nRemember that for this EFA example, we fancied ourselves as HAP team members who created the BDI-II items from scratch. In reality, of course, the BDI-II has been around for a long time, and CFA is probably a better choice for our situation. While many research groups have proposed multi-factor solutions for the BDI-II (Beck et al., 1988), we know the instrument is scored as a 1-factor model (consistent with our EFA results, yay!). To demonstrate a strength of CFA, however, let’s compare two different 1-factor models: a parallel model where items are weighted equally (feeling sad contributes the same as feeling suicidal) and a congeneric model where items are optimally weighted.\n\n# congeneric model (optimally weighted)\n# https://osf.io/8fzj4\n  model_1f_congeneric <- '\n  # all loadings are uniquely estimated\n  # first loading is set to 1 by default and must be freed\n    total_factor =~ NA*bdi01 + bdi02 + bdi03 + bdi04 + bdi05 + \n                    bdi06 + bdi07 + bdi08 + bdi09 + bdi10 +\n                    bdi11 + bdi12 + bdi13 + bdi14 + bdi15 + \n                    bdi16 + bdi17 + bdi18 + bdi19 + bdi20\n                    \n  # constrain factor variance to 1#\n    total_factor~~1*total_factor\n  '\n\n  fit_1f_congeneric <- lavaan::sem(\n    model_1f_congeneric, data = df, ordered = TRUE\n  )\n  \n# parallel model (equally weighted)\n# https://osf.io/2gzty\n  model_1f_parallel <- '\n  # fix all factor loadings to 1\n    total_factor =~ 1*bdi01 + 1*bdi02 + 1*bdi03 + 1*bdi04 + 1*bdi05 + \n                    1*bdi06 + 1*bdi07 + 1*bdi08 + 1*bdi09 + 1*bdi10 +\n                    1*bdi11 + 1*bdi12 + 1*bdi13 + 1*bdi14 + 1*bdi15 + \n                    1*bdi16 + 1*bdi17 + 1*bdi18 + 1*bdi19 + 1*bdi20\n    \n  # constrain all residual variances to same value\n    bdi01~~theta*bdi01\n    bdi02~~theta*bdi02\n    bdi03~~theta*bdi03\n    bdi04~~theta*bdi04\n    bdi05~~theta*bdi05\n    bdi06~~theta*bdi06\n    bdi07~~theta*bdi07\n    bdi08~~theta*bdi08\n    bdi09~~theta*bdi09\n    bdi10~~theta*bdi10\n    bdi11~~theta*bdi11\n    bdi12~~theta*bdi12\n    bdi13~~theta*bdi13\n    bdi14~~theta*bdi14\n    bdi15~~theta*bdi15\n    bdi16~~theta*bdi16\n    bdi17~~theta*bdi17\n    bdi18~~theta*bdi18\n    bdi19~~theta*bdi19\n    bdi20~~theta*bdi20\n  '\n  \n  fit_1f_parallel <- lavaan::sem(\n    model_1f_parallel, data = df, ordered = TRUE\n  )\n\nTable 6.5 shows different metrics for evaluating the fit of the models to the data. Evaluating model fit is an advanced topic, so I’ll simply note that the parallel (equally weighted) model fit shows mixed results. The Comparative Fit Index (CFI) value is greater than 0.90 as recommended, but the root mean square error of approximation value (RMSEA), which should be low, is above the commonly used cutoff of 0.08. The congeneric (optimally weighted) model looks better based on the same fit indices. The likelihood ratio test (not shown) confirms this.\n\n\n\n\n\n\nTable 6.5:  Comparing fit indices between the parallel and congeneric models. \n  \n    \n      Fit indices\n    \n    \n    \n      model\n      npar\n      chisq\n      cfi\n      tli\n      agfi\n      rmsea\n    \n  \n  \n    parallel (equally weighted)\n62\n704.5171\n0.9013458\n0.9002963\n0.8709023\n0.10812584\n    congeneric (optimally weighted)\n80\n263.2575\n0.9821879\n0.9800924\n0.9466522\n0.04831517\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement invariance\n\n\n\nMeasurement invariance is another aspect of measurement that we can investigate in a CFA framework. Measurement invariance means that an instrument consistently measures the same underlying construct across different groups or conditions. Let’s say we wanted to compare BDI-II scores by gender. Ideally, the BDI-II items will be equally valid and reliable for all genders, so that any differences observed in the scores truly reflect actual gender differences in depression, rather than differences caused by the instrument itself.\n\n\n\n\n\nReliability\nClassical test theory is a framework for evaluating and understanding the characteristics of tests and assessments, such as the BDI-II questionnaire. According to classical test theory, every observed score consists of two parts: the true score and measurement error. The true score represents person’s actual ability or trait being measured (e.g., depression severity), while measurement error includes various factors that can introduce variability into the observed scores.\nMeasurement error can be random or systematic. Random error is noise—unpredictable and inconsistent variations that occur in measurements. Even the most precise physics instruments have some random error, whether from human error or equipment limitations. When random error is high—that is, when the noise is greater than the signal—measurements are unreliable because they are inconsistent. Reliability refers to the consistency of measurements.\nSystematic error, on the other hand, is bias, and bias in measurement takes us away from the true score in a particular direction. Systematic error results in artificially inflated or deflated scores. Measurement bias contributes to unreliability, but it’s main victim is validity.\nA common teaching example is that a bathroom scale is valid if it correctly measures your weight, and is reliable if it gives you the same reading if you step off and back on. Validity and reliability work together to ensure sound measurement, but as you can see, they are independent concepts. A scale that consistently tells you, someone who weighs 65kg, that you weigh 80kg (±0.10kg) demonstrates reliable measurement, but not valid measurement. Consistency is reliability, regardless of being right or wrong.\n\n\n\nFor an in-depth look at reliability, see this chapter by William Revelle.\n\nThere is no one test of an instrument’s reliability because variation in measurement can come from many different sources: items, time, raters, form, etc. Therefore, we can assess different aspects of reliability of measurement, including test-retest reliability, internal consistency reliability, and inter-rater reliability, to name a few.\n\nReliability: Test-retest\nAn instrument is said to exhibit good test-retest reliability if it maintains roughly the same ordering between people when the instrument is repeatedly administered to the same individuals under the same conditions. Often this is benchmarked as a correlation of at least 0.70.\nWhen I first learned about test-retest reliability, I thought it meant that an instrument would return the same answer from one assessment to the next in the absence of change—that perfect reliability meant identical scores at time 1 and time 2. That’s not quite right though. Two sets of scores obtained from a group of people can be perfectly reliable, meaning they preserve the group order perfectly and have a correlation coefficient of 1.0, but also have zero agreement. I created Figure 6.17 to make this clear.\n\n\n\n\nFigure 6.17: Test-retest reliability and agreement are not the same.\n\n\n\nEach panel plots mock BDI-II sum scores for 10 people measured four days apart. An individual’s point falls on the diagonal line if they have identical scores at time 1 and time 2.\nPanel 1 illustrates perfect reliability and perfect agreement. The correlation coefficient is 1.0, and the ordering of people is perfectly preserved. Now look at Panel 2. This is also a scenario with perfect reliability; the ordering is preserved, but no one has identical scores at time 1 and time 2. There is zero agreement because there is a time effect!\nPanel 3 is more typical of what researchers present in papers claiming an instrument is reliable. In this example, there is very little agreement in scores from time 1 to time 2, but for the most part the ordering of depression severity is unchanged among the sample.\nFinally, in Panel 4 I simulated random values for time 1 and time 2, so any agreement is by chance. Reliability is low (0.43), and the ordering is not preserved. If your instrument shows evidence of low test-retest reliability like this over a short period where you expect stable scores, you have to wonder what the instrument is measuring.\n\n\n\n\n\n\n\nTest-retest period\n\n\n\nA key decision in assessing test-retest reliability is how long to wait in between administrations. If your instrument assesses depression symptoms in the past 2 weeks, you can’t wait more than 2 weeks because your respondent’s frame of reference will change too much and symptoms of depression can come and go. On the other hand, you shouldn’t re-administer the instrument the same day or even the next day because your respondent will likely recall what they said in an effort to appear consistent.\n\n\n\n\n\nReliability: Internal consistency\nRepeating administrations of an instrument with the same people to assess test-retest reliability is not always feasible, so in 1951 the educational psychologist Lee Cronbach came up with a way to measure what he called internal consistency reliability in a single administration. Rather than measuring the correlation in scale scores at multiple time points, internal consistency reliability evaluates how closely the scale items ascertained in a single administration are related to each other. If the items are not highly correlated with each other, it’s unlikely that they are measuring the same latent construct. In other words, the items are not internally consistent when it comes to measuring the construct.\nThe most commonly used index of internal consistency reliability is Cronbach’s alpha, but it has many detractors (McNeish, 2018). It’s still widely used because it’s easy to calculate with any statistics software. The basic approach is to divide the mean covariance between items by the mean item variance:\n\\[\\alpha = (N*\\bar{c}) / (\\bar{v}+(N−1)*\\bar{c})\\]\nwhere 𝑁 equals the number of items, \\(\\bar{c}\\) is the mean covariance between items, and \\(\\bar{v}\\) is the mean item variance. This means that Cronbach’s alpha quantifies how much of the overall variability is due to the relationships between the items. It can range from 0 to 1 where 1 indicates perfect reliability.\n\n\nUse the Kuder-Richardson 20 formula (KR20) if you have binary variables.\n\n\n\nBefore we calculate Cronbach’s alpha, let’s glimpse the HAP data. As a reminder, this data comes from participants in the control arm, collected three months after the treatment arm completed the HAP program.\n\n\nRefer to Figure 6.8 to see a visualization of the item correlation matrix.\n\n\n# A tibble: 236 × 20\n   bdi01 bdi02 bdi03 bdi04 bdi05 bdi06 bdi07 bdi08 bdi09 bdi10 bdi11 bdi12 bdi13\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     2     1     2     1     1     3     3     3     0     3     2     2     2\n 2     2     2     1     0     1     3     3     3     0     2     3     1     2\n 3     3     3     3     3     3     3     3     2     3     3     3     2     2\n 4     1     2     2     0     1     3     3     3     3     3     3     1     0\n 5     1     3     3     1     3     3     3     3     3     3     3     2     2\n 6     2     3     2     1     1     1     3     2     1     2     3     2     2\n 7     2     2     3     2     3     3     3     3     3     3     3     3     3\n 8     3     3     3     2     2     3     0     2     3     2     3     1     3\n 9     2     3     3     3     3     2     3     3     3     3     3     1     3\n10     3     2     1     2     3     3     3     3     3     3     3     3     3\n# ℹ 226 more rows\n# ℹ 7 more variables: bdi14 <dbl>, bdi15 <dbl>, bdi16 <dbl>, bdi17 <dbl>,\n#   bdi18 <dbl>, bdi19 <dbl>, bdi20 <dbl>\n\n\n\n\n\nNow we can use the {psych} package to calculate Cronbach’s alpha (William Revelle, 2023). We get a lot of output, so focus first on the top line with the raw alpha value of 0.87. Most reviewers are trained to question your approach if you report an alpha value lower than 0.70, so some researchers will look to see if alpha can be improved by dropping any items. That doesn’t appear to be the case in the HAP data.\n\nWilliam Revelle. (2023). Psych: Procedures for psychological, psychometric, and personality research. Northwestern University.\n\npsych::alpha(df)\n\n\nReliability analysis   \nCall: psych::alpha(x = df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.87      0.87    0.89      0.26   7 0.012  1.4 0.66     0.26\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.85  0.87   0.9\nDuhachek  0.85  0.87   0.9\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nbdi01      0.87      0.87    0.89      0.27 6.9    0.012 0.0115  0.27\nbdi02      0.87      0.87    0.89      0.27 6.9    0.012 0.0108  0.26\nbdi03      0.87      0.87    0.88      0.26 6.7    0.012 0.0113  0.26\nbdi04      0.88      0.88    0.89      0.27 7.1    0.012 0.0098  0.27\nbdi05      0.87      0.87    0.89      0.27 6.9    0.012 0.0111  0.26\nbdi06      0.86      0.86    0.88      0.25 6.3    0.013 0.0109  0.25\nbdi07      0.87      0.87    0.88      0.26 6.5    0.012 0.0114  0.26\nbdi08      0.87      0.87    0.88      0.25 6.5    0.012 0.0115  0.25\nbdi09      0.87      0.87    0.89      0.26 6.8    0.012 0.0114  0.26\nbdi10      0.87      0.87    0.88      0.26 6.5    0.012 0.0114  0.25\nbdi11      0.87      0.87    0.88      0.26 6.6    0.012 0.0118  0.25\nbdi12      0.87      0.87    0.88      0.26 6.5    0.012 0.0111  0.25\nbdi13      0.87      0.87    0.88      0.26 6.5    0.012 0.0108  0.25\nbdi14      0.86      0.86    0.88      0.25 6.4    0.013 0.0100  0.25\nbdi15      0.87      0.87    0.88      0.25 6.5    0.012 0.0110  0.25\nbdi16      0.86      0.87    0.88      0.25 6.4    0.013 0.0113  0.25\nbdi17      0.86      0.86    0.88      0.25 6.3    0.013 0.0107  0.25\nbdi18      0.87      0.87    0.88      0.26 6.7    0.012 0.0114  0.26\nbdi19      0.87      0.87    0.88      0.26 6.6    0.012 0.0113  0.26\nbdi20      0.87      0.87    0.88      0.26 6.6    0.012 0.0113  0.25\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nbdi01 236  0.42  0.43  0.37   0.34 1.37 1.12\nbdi02 236  0.40  0.41  0.36   0.33 1.89 1.15\nbdi03 236  0.48  0.49  0.45   0.41 1.42 1.07\nbdi04 236  0.31  0.32  0.25   0.23 1.09 1.07\nbdi05 236  0.43  0.43  0.39   0.35 1.39 1.27\nbdi06 236  0.68  0.68  0.67   0.62 1.85 1.20\nbdi07 236  0.61  0.59  0.57   0.53 1.58 1.41\nbdi08 236  0.60  0.60  0.58   0.54 1.47 1.21\nbdi09 236  0.46  0.47  0.42   0.39 0.89 1.19\nbdi10 236  0.59  0.59  0.56   0.52 1.73 1.28\nbdi11 236  0.54  0.53  0.50   0.46 1.89 1.38\nbdi12 236  0.58  0.59  0.56   0.52 1.28 1.18\nbdi13 236  0.58  0.59  0.56   0.52 1.19 1.22\nbdi14 236  0.66  0.66  0.65   0.60 1.24 1.27\nbdi15 236  0.60  0.60  0.58   0.53 1.06 1.24\nbdi16 236  0.63  0.63  0.61   0.57 1.83 1.17\nbdi17 236  0.68  0.67  0.66   0.62 1.57 1.31\nbdi18 236  0.54  0.52  0.49   0.45 1.27 1.42\nbdi19 236  0.55  0.54  0.51   0.47 1.29 1.28\nbdi20 236  0.52  0.55  0.51   0.48 0.43 0.65\n\nNon missing response frequency for each item\n         0    1    2    3 miss\nbdi01 0.32 0.17 0.31 0.19    0\nbdi02 0.19 0.14 0.25 0.42    0\nbdi03 0.22 0.38 0.17 0.23    0\nbdi04 0.39 0.26 0.20 0.14    0\nbdi05 0.36 0.22 0.10 0.32    0\nbdi06 0.19 0.25 0.10 0.47    0\nbdi07 0.42 0.05 0.08 0.46    0\nbdi08 0.33 0.16 0.23 0.28    0\nbdi09 0.55 0.22 0.02 0.21    0\nbdi10 0.29 0.11 0.17 0.42    0\nbdi11 0.33 0.03 0.07 0.57    0\nbdi12 0.40 0.12 0.29 0.19    0\nbdi13 0.45 0.13 0.21 0.21    0\nbdi14 0.44 0.16 0.13 0.28    0\nbdi15 0.52 0.14 0.12 0.22    0\nbdi16 0.22 0.11 0.27 0.39    0\nbdi17 0.33 0.17 0.10 0.40    0\nbdi18 0.53 0.05 0.04 0.38    0\nbdi19 0.42 0.15 0.14 0.29    0\nbdi20 0.63 0.33 0.01 0.03    0\n\n\nNow that I’ve shown you Cronbach’s alpha, I’ll let you know that many measurement researchers will tell you that we should move on from it, or at least use it more critically (McNeish, 2018). A good alternative is omega. Omega isn’t as commonly included in software programs as Cronbach’s alpha is, but several R packages will estimate one or more variants of omega. See McNeish (2018) and Revelle (2023) for in-depth tutorials.\n\nMcNeish, D. (2018). Thanks coefficient alpha, we’ll take it from here. Psychological Methods, 23(3), 412.\n\nRevelle, W. (2023). Using R and the psych package to find ω.\n\nCronbach’s alpha is also known as an index of tau-equivalent reliability because one assumption is that each item contributes equally to the total score.\n\n\n\n\n\nMBESS::ci.reliability(df)\n\n$est\n[1] 0.8770798\n\n$se\n[1] 0.01158661\n\n$ci.lower\n[1] 0.8543705\n\n$ci.upper\n[1] 0.8997891\n\n$conf.level\n[1] 0.95\n\n$type\n[1] \"omega\"\n\n$interval.type\n[1] \"robust maximum likelihood (wald ci)\"\n\n\nCronbach’s alpha will often underestimate reliability, but in the HAP data we see that the omega estimate, 0.88, is pretty close to the alpha value, 0.87. Regardless of the method, it looks like the BDI-II items consistently measured the same construct in the trial sample.\n\n\nI get a slightly different alpha value that what Patel et al. (2017) report because I’m only using data from the control arm for these examples.\n\n\nReliability: Inter-rater\nAnother form of reliability is inter-rater reliability, a measure of the extent to which two (or more) observers (raters) agree on what they observe. Imagine that you and I watch a new movie and a friend asks us for our reviews. I say it was ‘just OK’, but you say the movie was ‘great’. Our ratings are not reliable (not consistent).\nNow you might say that a problem with this example is that movie reviews are subjective, and you’d be right. But this is true of many phenomena we might want to assess with an observational instrument.\nFor instance, in the HAP trial researchers wanted to document whether the therapy sessions were implemented with fidelity—did lay counselors deliver the program as the intervention designers intended? If not, then the trial results would be hard to interpret, especially if the trial found that HAP is not efficacious. But how do we know if a counselor delivered the therapy with fidelity to the design? That is to say, how do we rate therapy quality?\nA common approach, employed in the HAP trial, is to train staff to observe a random sample of sessions (live or recorded) and rate the counselors using a standard observational rating system (instrument). In the HAP trial, sessions were rated by fellow lay counselors (peers), supervisors (experts), and an independent observer. Observers rated audio recordings of selected sessions using a 25-item instrument called the Quality of the Healthy Activity Programme (Q-HAP) scale (Singla et al., 2014). The Q-HAP consists of 15 treatment-specific items and 10 general skills items, each rated by observers on a 5 point scale from ‘not done’ (0) to ‘excellent’ (4). See Figure 6.18 for an example.\n\n\n\n\nFigure 6.18: Quality of the Healthy Activity Programme instrument.\n\n\n\nThe key reliability issue is to establish that the observers are consistent raters. For instance, when listening to the same session, do different observers see and record the same things? If yes, they are reliable observers and can be trusted to rate sessions independently, increasing the number of sessions a team can rate.\n\n\nRemember, reliability is not the same thing as validity. Whether the Q-HAP is a valid measure of therapy quality is a different question. The Q-HAP developers do not really engage with this question, writing, “Regarding validity, however, because scales were derived from instruments which are used by other psychological treatment researchers worldwide, we have assumed that they possess a degree of validity by extension.”\n\nMcGraw, K. O. et al. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30.\nSingla et al. (2014) quantified inter-rater reliability with the intra-class correlation coefficient, or ICC. The ICC is an index of how much of the variability in the measurements is due to true differences between subjects/items and how much is due to disagreement among raters. There are 10 different forms of ICC (McGraw et al., 1996), and it’s important to pick the right one for your purpose.\nSingla et al. (2014) calculated ICC(2,3). ICC(2,3) is used to assess the reliability and consistency of measurements made by multiple observers on multiple subjects/items. It is often employed when the raters are selected randomly from a larger pool, and each subject or item is assessed by different combinations of these raters. ICC values can range from 0 to 1, where values closer to 1 indicate high agreement.\n\nSingla, D. R. et al. (2014). Improving the scalability of psychological treatments in developing countries: An evaluation of peer-led therapy quality assessment in goa, india. Behaviour Research and Therapy, 60, 53–59.\nSingla et al. (2014) reported that the peer observers had moderate agreement on the Q-HAP treatment specific subscale (ICC(2,3) = .616, N = 97) and the general skills sub scale (ICC(2,3) = .622, N = 189). The results for expert observers were similar.\n\n\n\n\nConstruct Validation Phase 3: External\nThe final phase of developing a new instrument is to evaluate how it performs against external criteria or in relation to existing tools. Two main buckets of validity in this phase are construct validity and criterion validity.\n\nConstruct validity\nConstruct validity is a framework for evaluating whether an instrument measures the intended theoretical construct. Let’s review several types of construct validity, including convergent, discriminant, and known groups.\n\nConstruct: Convergent and discriminant validity\nPsychologists in particular like to think about nomological validity and talk in terms of convergent and discriminant validity. Establishing evidence for nomological validity means showing that your new instrument is positively correlated with theoretically related constructs (convergent validity) and uncorrelated (or only weakly correlated) with theoretically unrelated constricts (discriminant validity).\n\n\n‘Nomological’ from the Greek ‘nomos’ meaning ‘law’. The nomological network is the theoretical map of how different constructs are related. For instance, in a nomological network about mental health, depression and anxiety are distinct disorders but have theoretical overlap and are often comorbid.\nFor instance, a measure of depression should not be strongly associated with a measure of narcissism because we don’t consider depression and narcissism to be theoretically linked. But we would expect a measure of depression to be associated to some degree with a measure of anxiety because they are often co-morbid, meaning that people with depression symptoms also commonly report symptoms of anxiety.\nIn practice, when developing or adapting an instrument, you should design a validation study that includes measures of conceptually related and unrelated constructs so you can evaluate if the correlations are as predicted. If not, you have more work ahead.\n\n\nConstruct: Known groups validity\nAnother method for establishing evidence of construct validity is via known groups validity. With known groups validity you examine whether your instrument distinguishes between groups of people who are already known to differ on the construct of interest. For example, if you were developing a new measure of family conflict, you could recruit families who have been referred for services and a comparison group of families who have not been referred, administer your new questionnaire to both groups, and compare the group-level results. Known groups validity would predict that referred families would score higher on average on your measure of family distress compared to families not referred for support. See Puffer et al. (2021) for this approach to recruitment (but not analysis).\n\nPuffer, E. S. et al. (2021). Development of the family togetherness scale: A mixed-methods validation study in kenya. Frontiers in Psychology, 12, 662991.\n\nKnown groups validity is similar to diagnostic accuracy, discussed below as a type of criterion validity. A key difference is that diagnostic accuracy involves analyzing classification predictions for individuals whereas known groups methods look at differences in group averages.\n\n\n\n\nCriterion validity\nCriterion validity assesses how well an instrument relates to an external criterion or a gold standard. There are three main types of criterion validity: concurrent validity, predictive validity, and diagnostic accuracy.\n\nCriterion: Concurrent validity\nConcurrent validity examines the relationship between the new instrument and some criterion that is assessed at the same time. For example, Beck et al. (1988) reviewed 35 studies that compared BDI scores to four existing measures of depression and reported strong, positive correlations.\n\nBeck, A. T. et al. (1988). Psychometric properties of the beck depression inventory: Twenty-five years of evaluation. Clinical Psychology Review, 8(1), 77–100.\n\nConcurrent and convergent validity are similar. They both involve comparing the new instrument to other instruments measured at the same time. Concurrent compares to an existing gold standard measure of the same construct, while convergent compares to measures of related constructs. We’re splitting hairs in my view.\n\n\n\nCriterion: Predictive validity\nPredictive validity assesses how well an instrument predicts future outcomes or behaviors. A common application is evaluating the utility of a new hiring test to identify job candidates most likely to succeed in a particular role. A test has good predictive ability if the results correlate with job performance measured at a later time.\n\n\nCriterion: Diagnostic accuracy\nDiagnostic accuracy refers to the ability of an instrument to correctly identify individuals with or without a particular condition or disease. The new instrument under investigation is referred to as the index test, and the existing gold standard test is known as the criterion. For instance, if you are developing a new rapid diagnostic test for a bacterial infection that returns results in minutes, your rapid test would be the index test and the bacteria culture test would be the criterion or the gold standard.\nReturning to an earlier example, Green et al. (2018) developed a new perinatal depression screening questionnaire in Kenya—the Perinatal Depression Screening (PDEPS)—and evaluated its diagnostic accuracy by comparing PDEPS scores (the index test) to the results of separate clinical interviews conducted by Kenyan counselors (gold standard) who were blind to women’s questionnaire data. In this study, 193 pregnant and postpartum women completed the new screening questionnaire and separately participated in a clinical interview within three days. Clinical interviewers identified 10/193 women who met diagnostic criteria for Major Depressive Episode.\n\n\n\n\nFigure 6.19: Example confusion matrix.\n\n\n\nFigure 6.19 displays a confusion matrix for the study. The analysis found that a score of 13 or greater on the PDEPS correctly identified 90% of true cases. It only missed 1 out of 10 cases. This is known as sensitivity or the true positive rate. This cutoff on the PDEPS also correctly identified 90% of non-cases. This is known as specificity or the true negative rate. A score of 13 is the optimal cutoff that maximizes sensitivity and specificity.\n\n\nFor some applications or in certain settings you might choose to prioritize sensitivity over specificity, or vice versa. For instance, if false negatives are very costly for individuals and society, you might choose a cutoff that favors high sensitivity.\n\n\n\n\n\nCROSS-CULTURAL VALIDITY\nIf you are not yet ready to agree with my opening statement that “measurement is one of the hardest parts of science”, please allow me to tell you what else you need to consider if you wish to develop or adapt instruments for use in cross-cultural contexts.\nKohrt et al. (2011) is a fantastic guide for our discussion. This paper is motivated by the fact that there are few culturally adapted and validated instruments for assessing child mental health in low-income settings. This lack of instruments is a barrier to designing, delivering, and assessing services for children and families. To remedy this situation, the authors propose six criteria for evaluating the cross-cultural validity of instruments that are applicable to any topic.\n\nWhat is the purpose of the instrument?\nWhat is the construct to be measured?\nWhat are the contents of the construct?\nWhat are the idioms used to identify psychological symptoms and behaviors?\nHow should questions and responses be structured?\nWhat does a score on the instrument mean?\n\n\nWhat is the purpose of the instrument?\nThe authors start by reminding us that validity is not an inherent property of an instrument, adding that validity can vary by setting, population, and purpose. The last point is often overlooked. It’s important to design instruments to be fit for purpose. If your objective is to measure the efficacy of an intervention, let’s say, adapting an instrument validated for measuring the prevalence of disease may not be ideal. Start by defining your purpose.\n\n\nWhat is the construct to be measured?\nWe covered this question about construct validity extensively in the previous section, but it’s worth noting the authors’ distinction between three different types of constructs: (i) local constructs, (ii) Western psychiatric constructs; and (iii) cross-cultural constructs.\nIn their formulation, local constructs are the unique ways that the target population conceptualizes an issue. For instance, in the perinatal depression study mentioned earlier (Green et al., 2018), a local construct that emerged in focus group discussions was that women experiencing depression often feel like they just want to go back to their maternal home.7 This is not a characteristic of depression that you will find in any commonly used screening instruments, but it has relevance to this population.\n\nGreen, E. P. et al. (2018). Developing and validating a perinatal depression screening tool in kenya blending western criteria with local idioms: A mixed methods study. Journal of Affective Disorders, 228, 49–59.\n\nLocal constructs are also known as idioms of distress or culture-bound syndromes.\n\nWestern psychiatric constructs—which for our purposes we’ll recast more broadly as standard global health indicators—have their origin outside of the target population and may or may not be relevant. Kohrt et al. (2011) give the example of posttraumatic stress disorder as a Western construct with no perfectly synonymous concepts in Nepal. In contrast, cross-cultural constructs are universally recognized phenomena with some degree of shared meaning across settings and cultural groups.\n\n\nWhat are the contents of the construct?\nAnswering this question about content validity requires a close inspection of the instrument’s elements for relevancy. For instance, Kohrt et al. (2011) found that a common item on ADHD screening instruments in high-income countries, “stands quietly when in line”, is not applicable in Nepal because this behavior is not a universal expectation of children. Thus it’s not a sign of hyperactivity in this context and should not be included in a screening instrument.\n\nWhether you are creating a new instrument or adapting an existing measure, a few rounds of qualitative research will help you to explore which elements like this should be included or dropped. See the previous example linked to Figure 6.14.\n\n\n\nWhat are the idioms used to identify psychological symptoms and behaviors?\nOnce you know what domains an instrument should assess, it’s important to get the language right. Translation is never sufficient for establishing validity, but it’s a critical piece of the process.\nA standard recommendation is to conduct forward translation, blinded back translation, and reconciliation. In this approach, a translator fluent in both languages translates the items from the original language to the target language. Next, a new linguist translates the target language product back to the original language, but does so blinded, without seeing the original text. In the final step, the translators work together to compare the original text and the back-translated text, resolve disagreements, and improve the translations to ensure semantic equivalence.\n\n\nHow should questions and responses be structured?\nAnother important consideration is technical equivalence, or ensuring that the possible response sets are understood in the same way across groups or settings. For instance, if adapting an instrument that asks people to respond on a 4-point scale—never, rarely, sometimes, often—it’s helpful to verify that this format is understood by the target population once translated. Cognitive interviewing is a good approach.\n\n\nWhat does a score on the instrument mean?\nThis final question comes up in the external phase of construct validation that we discussed previously. There is not much more to say here, but Kohrt et al. (2011) make a point about the lack of criterion validation that bears repeating for anyone interested in conducting a prevalence study:\n\nKohrt, B. A. et al. (2011). Validation of cross-cultural child mental health and psychosocial research instruments: Adapting the depression self-rating scale and child PTSD symptom scale in nepal. BMC Psychiatry, 11(1), 1.\n\nUltimately, for prevalence studies, diagnostic validation is crucial. The misapplication of instruments that have not undergone diagnostic validation to make prevalence claims is one of the most common errors in global mental health research.\n\n\n\n\nTHREATS TO CONSTRUCT VALIDITY\nShadish et al. (2002) outlined 14 threats to construct validity that are presented in Table 6.6. See Matthay et al. (2020) for a translation of these threats to DAGs. I find that I often return to this list when designing a new study to consider whether there are threats lurking in my measurement strategy.\n\nShadish, W. R. et al. (2002). Experimental and quasi-experimental designs for generalized causal inference. Cengage Learning.\n\nMatthay, E. C. et al. (2020). A graphical catalog of threats to validity: Linking social science with epidemiology. Epidemiology, 31(3), 376.\n\n\n\n\n\nTable 6.6:  Threats to construct validity, adapted from Shadish et al. (2002) and Matthay and Glymour (2020). \n \n  \n    Threat Name \n    Definition \n  \n \n\n  \n    Inadequate explication of constructs \n    Failure to adequately explicate a construct may lead to incorrect inferences about the causal relationship of interest. \n  \n  \n    Construct confounding \n    Exposures or treatments usually involve more than one construct, and failure to describe all the constructs may result in incomplete construct inference. \n  \n  \n    Confounding constructs with levels of constructs \n    Inferences made about the constructs in a study fail to respect the limited range of the construct that was actually studied, i.e., effect estimates are extrapolated beyond the range of the observed data. \n  \n  \n    Mono-operation bias \n    Any one operationalization (measurement or intervention implementation) of a construct both underrepresents the construct of interest and measures irrelevant constructs, complicating the attribution of observed effects. \n  \n  \n    Mono-method bias \n    When all operationalizations (measurements or intervention implementations) use the same method (e.g., selfreport), that method is part of the construct actually studied. \n  \n  \n    Treatment sensitive factorial structure \n    The structure of a measure may change as result of treatment. This change may be hidden if the same scoring is always used. \n  \n  \n    Reactive self-report changes \n    Self-reports can be affected by participant motivation to be in a treatment condition. This motivation may change after assignment is made. \n  \n  \n    Compensatory equalization \n    When treatment provides desirable goods or services, administrators, staff, or constituents may provide compensatory goods or services to those not receiving treatment. This action must then be included as part of the treatment construct description. \n  \n  \n    Compensatory rivalry \n    Participants not receiving treatment may be motivated to show they can do as well as those receiving treatment. This action must then be included as part of the treatment construct description. \n  \n  \n    Resentful demoralization \n    Participants not receiving a desirable treatment may be so resentful or demoralized that they may respond more negatively than otherwise. This response must then be included as part of the treatment construct description. \n  \n  \n    Reactivity to the experimental situation \n    Participant responses reflect not just treatments and measures but also participants’ perceptions of the experimental situation. These perceptions are part of the treatment construct actually tested. \n  \n  \n    Experimenter expectancies \n    The experimenter can influence participant responses by conveying expectations about desirable responses. These expectations are part of the treatment construct as actually tested. \n  \n  \n    Novelty and disruption effects \n    Participants may respond unusually well to a novel innovation or unusually poorly to one that disrupts their routine. This response must then be included as part of the treatment construct description. \n  \n  \n    Treatment diffusion \n    Participants may receive services from a condition to which they were not assigned, making construct descriptions of both conditions more difficult."
  },
  {
    "objectID": "construct.html#measurement-schmeasurement",
    "href": "construct.html#measurement-schmeasurement",
    "title": "6  Measurement and Construct Validation",
    "section": "6.6 Measurement, Schmeasurement",
    "text": "6.6 Measurement, Schmeasurement\n\n\n\nTwo Psychologists, Four Beers, Episode 32, Measurement Schmeasurement.\n\n“Measurement, Schmeasurement” is the title of a great paper by Flake et al. (2020). The subtitle is, “Questionable Measurement Practices and How to Avoid Them”. The authors’ thesis is that measurement is a critical part of science, but questionable measurement practices undermine the validity of many studies and ultimately slow the progress of science. They define questionable measurement practices as:\n\ndecisions researchers make that raise doubts about the validity of measure use in a study, and ultimately the study’s final conclusions.\n\nThese decisions, they argue, stem from ignorance, negligence, and in some cases misrepresentation. According to Flake et al. (2020), questionable measurement practices live on because many researchers have an attitude of, “measurement, schmeasurement”. In other words, who cares.\n\nIf you are not a native English speaker, measurement, schmeasurement might be a confusing phrase. It’s an example of shm-reduplication used to indicate lack of interest or derision. Science, schmiance. See ghr.link/shm for more examples.\n\nI’m persuaded by this argument, having been in the room when investigators have spent weeks thinking about research design only to uncritically “throw in” a bunch of measures at the end. The thinking is often, why not, we’re going to the trouble of doing the study, let’s measure everything. Sometimes a student or colleague needs a project, and they’re invited to add something to the survey battery. In situations like this, the validity of measurement is never at the forefront. Doing it right is hard, and hey, measurement, schmeasurement, right?\nFlake et al. (2020) believe that fixing this problem begins with greater transparency and better reporting about measurement decisions. They also offer six questions to consider in the design phase and to report in publications:\n\nFlake, J. K. et al. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465.\n\nWhat is your construct?\nWhy and how did you select your measure?\nWhat measure did you use to operationalize the construct?\nHow did you quantify your measure?\nDid you modify the scale? And if so, how and why?\nDid you create a measure on the fly? If so, justify your decision and report all measurement details for the new measure along with any validity evidence.\n\nIf you ask and answer these questions for your next study, I’m convinced that you will improve your measurement strategy and thus strengthen the validity of your conclusions."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abimbola, S. et al. (2020). Will global health survive its\ndecolonisation? Lancet (London, England), 396(10263),\n1627–1628.\n\n\nAmrhein, V. et al. (2019). Scientists rise up\nagainst statistical significance. Nature,\n567(7748), 305–307.\n\n\nAnand, Arpita and Chowdhary, Neerja and Dimidjian, Sona and Patel,\nVikram. (2013). Healthy\nActivity Program.\n\n\nBarnard-Mayers, R. et al. (2022). A case\nstudy and proposal for publishing directed acyclic graphs: The\neffectiveness of the quadrivalent human papillomavirus vaccine in\nperinatally HIV infected girls. Journal of Clinical\nEpidemiology, 144, 127–135.\n\n\nBauer, M. S. et al. (2020). Implementation\nscience: What is it and why should I care?\nPsychiatry Research, 283, 112376.\n\n\nBeck, A. T. et al. (1988). Psychometric properties of the beck\ndepression inventory: Twenty-five years of evaluation. Clinical\nPsychology Review, 8(1), 77–100.\n\n\nBeck, A. T. et al. (1996). Manual for the beck depression\ninventory-II. Psychological Corporation.\n\n\nBem, D. J. (2011). Feeling the future:\nExperimental evidence for anomalous retroactive influences\non cognition and affect. 100, 407–425.\n\n\nBenjamin, D. J. et al. (2018). Redefine statistical\nsignificance. Nature Human Behaviour, 2(1), 6–10.\n\n\nBennett, L. M. et al. (2018). Collaboration team science: Field\nguide. US Department of Health & Human Services, National\nInstitutes of Health.\n\n\nBhatt, N. et al. (2021). Perceptions of family planning services and its\nkey barriers among adolescents and young people in eastern nepal: A\nqualitative study. PloS One, 16(5), e0252184.\n\n\nBoyle, P. (2021). How are COVID-19 deaths counted? It’s\ncomplicated.\n\n\nCalin-Jageman, R. J. et al. (2019). The New\nStatistics for Better Science: Ask How\nMuch, How Uncertain, and What Else Is\nKnown. The American Statistician, 73(sup1),\n271–280.\n\n\nCampbell, D. T. (1969). Reforms as\nexperiments. American Psychologist, 24(4), 409.\n\n\nCampbell, W. C. (n.d.). Nobel\nlecture.\n\n\nCDC. (2020). Reporting and Coding Deaths Due to\nCOVID-19.\n\n\nChowdhary, N. et al. (2016). The healthy activity program lay counsellor\ndelivered treatment for severe depression in india: Systematic\ndevelopment and randomised evaluation. The British Journal of\nPsychiatry, 208(4), 381–388.\n\n\nCNN Wire. (2015). New\nstudy suggests coffee could literally be a lifesaver.\n\n\nCostello, A. et al. (2000). Moving to research partnerships in\ndeveloping countries. Bmj, 321(7264), 827–829.\n\n\nCronbach, L. J. et al. (1955). Construct validity in psychological\ntests. Psychological Bulletin, 52(4), 281.\n\n\nCronbach, L. J. (1982). Designing\nevaluations of educational and social programs. Jossey-Bass.\n\n\nCunningham, S. (2020). Causal\ninference: The mixtape. Yale University Press.\n\n\nDadonaite, B. (2019). Oral\nrehydration therapy: A low-tech solution that has saved millions of\nlives. In Our World in Data.\n\n\nDHS. (n.d.). Demographic and Health Surveys (DHS)\nProgram.\n\n\nDienes, Z. (2008). Understanding psychology as a science: An\nintroduction to scientific and statistical inference. Macmillan\nInternational Higher Education.\n\n\nDing, M. et al. (2015). Association of coffee consumption with total and\ncause-specific mortality in 3 large prospective cohorts.\nCirculation, 132(24), 2305–2315.\n\n\nFlake, J. K. et al. (2017). Construct validation in social and\npersonality research: Current practice and recommendations. Social\nPsychological and Personality Science, 8(4), 370–378.\n\n\nFlake, J. K. et al. (2020). Measurement schmeasurement: Questionable\nmeasurement practices and how to avoid them. Advances in Methods and\nPractices in Psychological Science, 3(4), 456–465.\n\n\nFlake, J. K. et al. (2022). Construct validity and the validity of\nreplication studies: A systematic review. American\nPsychologist, 77(4), 576.\n\n\nFlora, D. B. et al. (2017). The purpose and practice of exploratory and\nconfirmatory factor analysis in psychological research: Decisions for\nscale development and validation. Canadian Journal of Behavioural\nScience, 49(2), 78.\n\n\nFood and Drug Administration Amendments Act of 2007. (n.d.). Pub. L. No.\n110-85, 121 Stat. 904 (2007).\n\n\nFox, M. P. et al. (2022). On the need to revitalize descriptive\nepidemiology. American Journal of Epidemiology.\n\n\nGelman, A. (2015). What’s the most\nimportant thing in statistics that’s not in the textbooks?\n\n\nGelman, A. et al. (2020). Regression and other stories.\nCambridge University Press.\n\n\nGreen, E. P. et al. (2018). Developing\nand validating a perinatal depression screening tool in kenya blending\nwestern criteria with local idioms: A mixed methods study.\nJournal of Affective Disorders, 228, 49–59.\n\n\nHaber, N. et al. (2018). Causal language and strength of inference in\nacademic and media articles shared in social media (CLAIMS): A\nsystematic review. PloS One, 13(5), e0196346.\n\n\nHay, S. I. et al. (2023). Conflicting\nCOVID-19 excess mortality estimates – Authors’\nreply. The Lancet, 401(10375), 433–434.\n\n\nHeiss, A. (2020a). Causal inference. In R for political data\nscience (pp. 235–273). Chapman; Hall/CRC.\n\n\nHeiss, A. (2020b). Ways to close\nbackdoors in DAGs.\n\n\nHernán, M. A. (2018). The c-word: Scientific euphemisms do not improve\ncausal inference from observational data. American Journal of Public\nHealth, 108(5), 616–619.\n\n\nHirsch, L. A. (2021). Is it possible to decolonise global health\ninstitutions? Lancet, 397(10270), 189–190.\n\n\nHogan, M. C. et al. (2010). Maternal\nmortality for 181 countries, 1980–2008: A systematic analysis of\nprogress towards millennium development goal 5. The Lancet,\n375(9726), 1609–1623.\n\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of\nthe American Statistical Association, 81(396), 945–960.\n\n\nHolst, J. (2020). Global health–emergence, hegemonic trends and\nbiomedical reductionism. Globalization and Health,\n16(1), 1–11.\n\n\nHulley, S. et al. (2007). Getting started: The anatomy and\nphysiology of clinical research (S. Hulley et al., Eds.; Third, pp.\n3–15). Lippincott Williams & Wilkins.\n\n\nHuntington-Klein, N. (2021). The\neffect: An introduction to research design and causality.\nChapman; Hall/CRC.\n\n\nICMJE. (n.d.). Recommendations for the Conduct, Reporting, Editing, and\nPublication of Scholarly work in Medical Journals.\n\n\nIHME. (n.d.). COVID-19\nProjections. In Institute for Health Metrics and\nEvaluation.\n\n\nIi, Y. B. et al. (2018). Advancing equitable\nglobal health research partnerships in Africa. BMJ\nGlobal Health, 3(4), e000868.\n\n\nIsrael, B. A. et al. (1998). Review of community-based research:\nAssessing partnership approaches to improve public health. Annual\nReview of Public Health, 19(1), 173–202.\n\n\nJaroudi, S. et al. (2017). Vitamin d supplementation and cancer risk.\nJAMA, 318(3), 299–299.\n\n\nKaryotaki, E. et al. (2022). Association of task-shared psychological\ninterventions with depression outcomes in low-and middle-income\ncountries: A systematic review and individual patient data\nmeta-analysis. JAMA Psychiatry.\n\n\nKing, G. et al. (2021). Designing\nSocial Inquiry: Scientific\nInference in Qualitative\nResearch, New Edition.\nPrinceton University Press.\n\n\nKohrt, B. A. et al. (2011). Validation of cross-cultural child mental\nhealth and psychosocial research instruments: Adapting the depression\nself-rating scale and child PTSD symptom scale in nepal. BMC\nPsychiatry, 11(1), 1.\n\n\nKoplan, J. P. et al. (2009). Towards a common definition of global\nhealth. The Lancet, 373(9679), 1993–1995.\n\n\nKroenke, K. et al. (2002). The PHQ-9: A new depression diagnostic and\nseverity measure. Psychiatric Annals, 32(9), 509–515.\n\n\nKruk, M. E. et al. (2016). Transforming Global\nHealth by Improving the Science\nof Scale-Up. PLOS Biology,\n14(3), e1002360.\n\n\nKyobutungi, C. et al. (2021). PLOS\nGlobal Public Health, charting a\nnew path towards equity, diversity and inclusion in global health.\nPLOS Global Public Health, 1(10), e0000038.\n\n\nLam, F. et al. (2019). A\nretrospective mixed-methods evaluation of a national ORS\nand zinc scale-up program in Uganda between 2011 and\n2016. Journal of Global Health, 9(1), 010504.\n\n\nLappe, J., Garland, C., et al. (2017). Vitamin D Supplementation and Cancer Risk.\nJAMA, 318(3), 299–300.\n\n\nLappe, J., Watson, P., et al. (2017). Effect of vitamin d and calcium\nsupplementation on cancer incidence in older women: A randomized\nclinical trial. JAMA, 317(12), 1234–1243.\n\n\nLarsen, A. et al. (2021). Is there an optimal screening tool for\nidentifying perinatal depression within clinical settings of sub-saharan\nafrica? SSM-Mental Health, 1, 100015.\n\n\nLeary, M. (2012). Introduction to\nbehavioral research methods (6th ed.). Pearson.\n\n\nLoevinger, J. (1957). Objective tests as instruments of psychological\ntheory. Psychological Reports, 3(3), 635–694.\n\n\nMacias Gil, R. et al. (2020). COVID-19 pandemic: Disparate health impact\non the hispanic/latinx population in the united states. The Journal\nof Infectious Diseases, 222(10), 1592–1595.\n\n\nMatthay, E. C., & Glymour, M. M. (2020). A graphical catalog of\nthreats to validity: Linking social science with epidemiology.\nEpidemiology, 31(3), 376.\n\n\nMatthay, E. C., Hagan, E., et al. (2020). Alternative causal inference\nmethods in population health research: Evaluating tradeoffs and\ntriangulating evidence. SSM Population Health, 10,\n100526.\n\n\nMcElreath, R. (2020). Statistical\nRethinking: A Bayesian\nCourse with Examples in R and\nStan. CRC Press.\n\n\nMcGraw, K. O. et al. (1996). Forming inferences about some intraclass\ncorrelation coefficients. Psychological Methods, 1(1),\n30.\n\n\nMcNeish, D. (2018). Thanks coefficient alpha, we’ll take it from here.\nPsychological Methods, 23(3), 412.\n\n\nMcNeish, D. et al. (2020). Thinking twice about sum scores. Behavior\nResearch Methods, 52, 2287–2305.\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A\nmethodological paradox. Philosophy of Science, 34(2),\n103–115.\n\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories\nare often uninterpretable. Psychological Reports,\n66(1), 195–244.\n\n\nMerson, M. H. et al. (2018). Global health: Diseases, programs,\nsystems, and policies (4th ed.). Jones & Bartlett Learning.\n\n\nMicah, A. E. et al. (2021). Tracking\ndevelopment assistance for health and for COVID-19: A\nreview of development assistance, government, out-of-pocket, and other\nprivate spending on health for 204 countries and territories,\n1990–2050. The Lancet, 398(10308), 1317–1343.\n\n\nMinistry of Health and Population et al. (2011). Nepal Demographic and Health Survey 2011.\nMinistry of Health; Populationh.\n\n\nMorris, Z. S. et al. (2011). The answer is 17 years,\nwhat is the question: Understanding time lags in translational\nresearch. Journal of the Royal Society of Medicine,\n104(12), 510–520.\n\n\nNafilyan, V. et al. (2021). Occupation and\nCOVID-19 mortality in England: A national\nlinked data study of 14.3 million adults. medRxiv.\n\n\nOlken, B. A. (2005). Monitoring corruption: Evidence from a field\nexperiment in indonesia. National Bureau of Economic Research.\n\n\nOur World in Data. (2023a). Coronavirus (COVID-19)\nDeaths.\n\n\nOur World in Data. (2023b). Excess mortality during the Coronavirus pandemic\n(COVID-19).\n\n\nPai, M. (n.d.). Decolonizing\nGlobal Health: A\nMoment To Reflect On\nA Movement. In Forbes.\n\n\nPatel, V. et al. (2014). The effectiveness and\ncost-effectiveness of lay counsellor-delivered psychological treatments\nfor harmful and dependent drinking and moderate to severe depression in\nprimary care in India: PREMIUM study protocol\nfor randomized controlled trials. Trials, 15, 101.\n\n\nPatel, V. et al. (2008). Detecting common mental disorders in primary\ncare in india: A comparison of five screening questionnaires.\nPsychological Medicine, 38(2), 221–228.\n\n\nPatel, V. et al. (2017). The healthy\nactivity program (HAP), a lay counsellor-delivered brief psychological\ntreatment for severe depression, in primary care in india: A randomised\ncontrolled trial. The Lancet.\n\n\nPearl, J. (1995). Causal diagrams for empirical research.\nBiometrika, 82(4), 669–688.\n\n\nPearl, J. et al. (2018). The book of why: The new science of cause\nand effect (1st ed.). Basic Books, Inc.\n\n\nPolicy Cures Research. (n.d.). G-FINDER data portal.\n\n\nPradhan, E. et al. (2019). Integrating postpartum contraceptive\ncounseling and IUD insertion services into maternity care in nepal:\nResults from stepped-wedge randomized controlled trial. Reproductive\nHealth, 16(1), 69.\n\n\nPuffer, E. S. et al. (2013). Developing a family-based HIV prevention\nintervention in rural kenya: Challenges in conducting community-based\nparticipatory research. Journal of Empirical Research on Human\nResearch Ethics, 8(2), 119–128.\n\n\nPuffer, E. S. et al. (2021). Development of the family togetherness\nscale: A mixed-methods validation study in kenya. Frontiers in\nPsychology, 12, 662991.\n\n\nRevelle, W. (2023). Using\nR and the psych package to find ω.\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation:\nGraphical causal models for observational data. Advances in Methods\nand Practices in Psychological Science, 1(1), 27–42.\n\n\nRosseel, Y. (2012). lavaan: An R package for structural\nequation modeling. Journal of Statistical Software,\n48(2), 1–36.\n\n\nRossi, P. H. et al. (2003). Evaluation: A systematic\napproach. Sage Publications.\n\n\nRothschild, C. W. et al. (2020). A risk scoring tool\nfor predicting Kenyan women at high risk of contraceptive\ndiscontinuation. Contraception, 2, 100045.\n\n\nRøttingen, J.-A. et al. (2013). Mapping of\navailable health research and development data: What’s there, what’s\nmissing, and what role is there for a global observatory? Lancet\n(London, England), 382(9900), 1286–1307.\n\n\nRoundtable, I. of M. (US). C. R. et al. (2002). Definitions of\nClinical Research and Components\nof the Enterprise. National Academies Press (US).\n\n\nRubin, D. B. (1974). Estimating causal effects of treatments in\nrandomized and nonrandomized studies. Journal of Educational\nPsychology, 66(5), 688.\n\n\nRutstein, S. O. et al. (2004). The DHS wealth\nindex (DHS Comparative Reports No. 6). ORC Macro.\n\n\nSatcher Health Leadership Institute. (2021). Health Equity\nTracker.\n\n\nShadish, W. R. et al. (2002). Experimental and quasi-experimental\ndesigns for generalized causal inference. Cengage Learning.\n\n\nShmueli, G. (2010). To explain or to predict? Statistical\nScience, 25(3), 289–310.\n\n\nSingla, D. R. et al. (2014). Improving the scalability of psychological\ntreatments in developing countries: An evaluation of peer-led therapy\nquality assessment in goa, india. Behaviour Research and\nTherapy, 60, 53–59.\n\n\nSirleaf, E. J. et al. (2021). Achieving\nvaccination justice: A call for global cooperation.\nPLOS Global Public Health, 1(10), 1–3.\n\n\nSkolnik, R. (2019). Global health 101, fourth edition. Jones\n& Bartlett Learning.\n\n\nSridhar, D. (2012). Who\nSets the Global Health\nResearch Agenda? The\nChallenge of Multi-Bi\nFinancing. PLOS Medicine, 9(9),\ne1001312.\n\n\nUNDP. (n.d.). Human Development\nIndex.\n\n\nUnited Nations. (2023). SDG\nindicators.\n\n\nWasserstein, R. L. et al. (2016). The ASA\nStatement on p-Values: Context,\nProcess, and Purpose. The American\nStatistician, 70(2), 129–133.\n\n\nWest, S. G. et al. (2010). Campbell’s and rubin’s perspectives on causal\ninference. Psychological Methods, 15(1), 18.\n\n\nWestreich, D. et al. (2013). The table 2 fallacy: Presenting and\ninterpreting confounder and modifier coefficients. American Journal\nof Epidemiology, 177(4), 292–298.\n\n\nWestreich, D. (2019). Epidemiology by\nDesign: A Causal\nApproach to the Health\nSciences. Oxford University Press, Incorporated.\n\n\nWhittaker, C. et al. (2021). Under-reporting of deaths limits our\nunderstanding of true burden of covid-19. BMJ, 375.\n\n\nWHO. (n.d.). Investments on grants for biomedical research by\nfunder, type of grant, health category and recipient.\n\n\nWHO. (2013). Social\ndeterminants of health: Key concepts.\n\n\nWHO. (2018). Global reference list of\n100 core health indicators. World Health\nOrganization.\n\n\nWHO. (2020). Clinical\ntrials.\n\n\nWHO. (2021). Health\nEquity.\n\n\nWilliam Revelle. (2023). Psych: Procedures for\npsychological, psychometric, and personality research.\nNorthwestern University.\n\n\nWong, C. H. et al. (2019). Estimation of\nclinical trial success rates and related parameters.\nBiostatistics, 20(2), 273–286.\n\n\nWoolf, S. H. (2008). The\nMeaning of Translational Research\nand Why It Matters.\nJAMA, 299(2), 211–213.\n\n\nWorld Health Organization. Maternal\nmortality ratio (per 100 000 live births).\n\n\nWorld Health Organization. (2021). WHO civil registration and vital statistics\nstrategic implementation plan 2021-2025.\n\n\nWu, W.-J. et al. (2020). Community-based postpartum contraceptive\ncounselling in rural nepal: A mixed-methods evaluation. Sexual and\nReproductive Health Matters, 28(2), 1765646.\n\n\nWynants, L. et al. (2020). Prediction models for diagnosis\nand prognosis of covid-19: Systematic review and critical appraisal.\nBMJ, 369, m1328."
  }
]