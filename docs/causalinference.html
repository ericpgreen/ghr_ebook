<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.383">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Global Health Research Methods - 3&nbsp; Causal Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./references.html" rel="next">
<link href="./collaborations.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Global Health Research Methods</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ghr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Global Health Research</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./collaborations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Build Collaborations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causalinference.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-causal-inference" id="toc-what-is-causal-inference" class="nav-link active" data-scroll-target="#what-is-causal-inference"> <span class="header-section-number">3.1</span> What is Causal Inference?</a>
  <ul class="collapse">
  <li><a href="#causes" id="toc-causes" class="nav-link" data-scroll-target="#causes">CAUSES</a></li>
  <li><a href="#effects" id="toc-effects" class="nav-link" data-scroll-target="#effects">EFFECTS</a></li>
  <li><a href="#causal-inference-methods" id="toc-causal-inference-methods" class="nav-link" data-scroll-target="#causal-inference-methods">CAUSAL INFERENCE METHODS</a></li>
  </ul></li>
  <li><a href="#causal-diagrams-and-confounder-control" id="toc-causal-diagrams-and-confounder-control" class="nav-link" data-scroll-target="#causal-diagrams-and-confounder-control"> <span class="header-section-number">3.2</span> Causal Diagrams and Confounder-Control</a>
  <ul class="collapse">
  <li><a href="#causal-stories" id="toc-causal-stories" class="nav-link" data-scroll-target="#causal-stories">CAUSAL STORIES</a></li>
  <li><a href="#components-of-a-dag" id="toc-components-of-a-dag" class="nav-link" data-scroll-target="#components-of-a-dag">COMPONENTS OF A DAG</a></li>
  <li><a href="#decide-which-nodes-to-include-in-a-dag" id="toc-decide-which-nodes-to-include-in-a-dag" class="nav-link" data-scroll-target="#decide-which-nodes-to-include-in-a-dag">DECIDE WHICH NODES TO INCLUDE IN A DAG</a></li>
  <li><a href="#tracing-all-paths" id="toc-tracing-all-paths" class="nav-link" data-scroll-target="#tracing-all-paths">TRACING ALL PATHS</a></li>
  <li><a href="#effect-identification" id="toc-effect-identification" class="nav-link" data-scroll-target="#effect-identification">EFFECT IDENTIFICATION</a></li>
  <li><a href="#effect-estimation" id="toc-effect-estimation" class="nav-link" data-scroll-target="#effect-estimation">EFFECT ESTIMATION</a></li>
  <li><a href="#the-big-fine-print" id="toc-the-big-fine-print" class="nav-link" data-scroll-target="#the-big-fine-print">THE BIG FINE PRINT</a></li>
  </ul></li>
  <li><a href="#instrument-based-approaches-and-quasi-experimental-designs" id="toc-instrument-based-approaches-and-quasi-experimental-designs" class="nav-link" data-scroll-target="#instrument-based-approaches-and-quasi-experimental-designs"> <span class="header-section-number">3.3</span> Instrument-Based Approaches and Quasi-Experimental Designs</a>
  <ul class="collapse">
  <li><a href="#experiments-destroy-confounding" id="toc-experiments-destroy-confounding" class="nav-link" data-scroll-target="#experiments-destroy-confounding">EXPERIMENTS DESTROY CONFOUNDING</a></li>
  <li><a href="#exogenous-variation-without-randomization" id="toc-exogenous-variation-without-randomization" class="nav-link" data-scroll-target="#exogenous-variation-without-randomization">EXOGENOUS VARIATION WITHOUT RANDOMIZATION</a></li>
  </ul></li>
  <li><a href="#key-assumptions-and-threats-to-internal-validity" id="toc-key-assumptions-and-threats-to-internal-validity" class="nav-link" data-scroll-target="#key-assumptions-and-threats-to-internal-validity"> <span class="header-section-number">3.4</span> Key Assumptions and Threats to Internal Validity</a>
  <ul class="collapse">
  <li><a href="#threats-to-internal-validity" id="toc-threats-to-internal-validity" class="nav-link" data-scroll-target="#threats-to-internal-validity">THREATS TO INTERNAL VALIDITY</a></li>
  </ul></li>
  <li><a href="#coda-coffee-saves-lives" id="toc-coda-coffee-saves-lives" class="nav-link" data-scroll-target="#coda-coffee-saves-lives"> <span class="header-section-number">3.5</span> Coda: Coffee Saves Lives?</a></li>
  <li><a href="#keep-learning" id="toc-keep-learning" class="nav-link" data-scroll-target="#keep-learning"> <span class="header-section-number">3.6</span> Keep Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="causalinference" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causal Inference</span></span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Extra! Extra! Read all about it: “New study suggests coffee could literally be a lifesaver” <span class="citation" data-cites="literally:2015">(<a href="references.html#ref-literally:2015" role="doc-biblioref">CNN Wire, 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-literally:2015" class="csl-entry" role="doc-biblioentry">
CNN Wire. (2015). <em><a href="https://fox59.com/news/new-study-suggests-coffee-could-literally-be-a-lifesaver/">New study suggests coffee could literally be a lifesaver</a></em>.
</div></div><p>This is a real headline about a study published in the journal <em>Circulation</em>. Now, I’m as big a coffee fan as the next guy, but <em>literally</em> a life saver? Here’s what the study authors wrote in their paper <span class="citation" data-cites="ding:2015">(<a href="references.html#ref-ding:2015" role="doc-biblioref">Ding et al., 2015</a>)</span>:</p>
<blockquote class="blockquote">
<p>Higher consumption of total coffee, caffeinated coffee, and decaffeinated coffee was associated with lower risk of total mortality…Relative to no consumption of coffee, the pooled hazard ratio for death was 0.95 (95% confidence interval [CI], 0.91–0.99) for 1.0 or less cup of total coffee per day.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<p>This finding comes from a pooled analysis of three large prospective cohort studies of <em>health professionals</em> in the U.S. (95% white) followed for up to 36 years. Participants completed a food frequency questionnaire every few years that described their coffee intake, and researchers searched national and state death registries to gather data on deaths throughout the follow-up period.</p>
</div></div><p>“Associated with” tells us that there’s a relationship between mortality and coffee consumption in the observed data. The sort of people who drink a cup of coffee daily have a 5% lower risk of dying over 2 to 3 decades. (This is a 0.7% <em>absolute</em> decrease in the incidence of mortality according to my back of the envelope analysis.) Does this mean that coffee <em>prevents</em> death?</p>
<p>The study authors say no. But also, maybe! This is what I call <strong>Causal Deniability</strong>. <span class="citation" data-cites="haber:2018">(For more examples, see <a href="references.html#ref-haber:2018" role="doc-biblioref">Haber et al., 2018</a>.)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-haber:2018" class="csl-entry" role="doc-biblioentry">
Haber, N. et al. (2018). Causal language and strength of inference in academic and media articles shared in social media (CLAIMS): A systematic review. <em>PloS One</em>, <em>13</em>(5), e0196346.
</div></div><p><strong>Step 1:</strong> Avoid the word “causal” and warn that correlation is not causation.</p>
<blockquote class="blockquote">
<p>…given the observational nature of the study design, we could not directly establish a cause-effect relationship between coffee and mortality.</p>
</blockquote>
<p><strong>Step 2:</strong> Ignore the warning and make policy or health recommendations based on a causal interpretation of the findings.</p>
<blockquote class="blockquote">
<p>…coffee consumption can be incorporated into a healthy lifestyle…moderate consumption of coffee may confer health benefits in terms of reducing premature death.</p>
</blockquote>
<p>So which is it, a non-causal association or a causal effect? <span class="citation" data-cites="hernan2018">Hernán (<a href="references.html#ref-hernan2018" role="doc-biblioref">2018</a>)</span> argues that scientists need to stop the charade:</p>
<div class="no-row-height column-margin column-container"><div class="">
<p>We know Ding et al.&nbsp;had causal inference in mind because they adjusted for potential confounders like age. As Hernán reminds us, “confounding is a causal concept that does not apply to associations”.</p>
</div></div>
<blockquote class="blockquote">
<p>We need to stop treating “causal” as a dirty word that respectable investigators do not say in public or put in print. It is true that observational studies cannot definitely prove causation, but this statement misses the point…</p>
</blockquote>
<p>According to <span class="citation" data-cites="hernan2018">Hernán (<a href="references.html#ref-hernan2018" role="doc-biblioref">2018</a>)</span>, the point is that we have to be clear about our scientific goals and use language that reflects these goals. To riff on his idea a bit: Do we want to determine whether “the sort of people who drink a cup of coffee daily have a lower risk of dying” or do we want to determine whether “drinking a cup of coffee daily lowers the risk of dying”?</p>
<div class="no-row-height column-margin column-container"><div id="ref-hernan2018" class="csl-entry" role="doc-biblioentry">
Hernán, M. A. (2018). The c-word: Scientific euphemisms do not improve causal inference from observational data. <em>American Journal of Public Health</em>, <em>108</em>(5), 616–619.
</div></div><p>It’s almost always the latter.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> And to answer this question, we need causal inference.</p>
<section id="what-is-causal-inference" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="what-is-causal-inference"><span class="header-section-number">3.1</span> What is Causal Inference?</h2>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>Casual Inference Podcast</em>, Season 3, Episode 6, <a href="http://ghr.link/his">A Casual Look at Causal Inference History</a>.</p>
</div></div></div>
<p><strong>Causal inference</strong> is what we do when we identify and estimate the causal effect of some proposed cause on an outcome of interest. We use causal inference methods in global health to answer key questions about health policy and practice. Do bed nets prevent malaria, and by how much? Is it better to subsidize bed nets or sell them at full retail cost? And so on.</p>
<div class="column-body">
<div id="fig-causeeffect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/causeeffect.png" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.1: Causes, effects, and outcomes</figcaption><p></p>
</figure>
</div>
</div>
<p>As someone who has likely perfected the art of causal inference in your daily life, you might be surprised to learn that causal inference in science is still a rapidly evolving field. Even core terms like <em>cause</em> and <em>effect</em> are up for debate.</p>
<div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>Effects of causes or causes of effects?</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>The most common type of causal inference question we ask is about the <em>effects of causes</em>. What is the effect of 𝑋 on 𝑌? For instance, what is the effect of a new therapy on depression severity? Given a well-defined cause, 𝑋, we can estimate what happens to 𝑌 if 𝑋 changes. Questions about the <em>causes of effects</em>—what causes 𝑌?—are harder to answer. For example, what causes depression?</p>
</div>
</div>
</div>
<section id="causes" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="causes">CAUSES</h3>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<iframe width="300" height="169" src="https://www.youtube.com/embed/ZaPV1OSEpHw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p>Watch Dr.&nbsp;Judea Pearl discuss what he calls the new science of cause and effect.</p>
</div></div></div>
<p>The Turing Award-winning computer scientist Dr.&nbsp;Judea Pearl and his co-author, mathematician-turned-science writer Dr.&nbsp;Dana Mackenzie, offered the following definition of <strong>causes</strong> in their 2018 instant classic, <em>The Book of Why</em> <span class="citation" data-cites="pearl:2018">(<a href="references.html#ref-pearl:2018" role="doc-biblioref">Pearl et al., 2018</a>)</span>:</p>
<div class="no-row-height column-margin column-container"></div><blockquote class="blockquote">
<p>A variable 𝑋 is a cause of 𝑌 if 𝑌 “listens” to 𝑋 and determines its value in response to what it hears</p>
</blockquote>
<p>This definition has several implications for causal relationships <span class="citation" data-cites="scc">(<a href="references.html#ref-scc" role="doc-biblioref">Shadish et al., 2002</a>)</span>:</p>
<div class="no-row-height column-margin column-container"></div><ol type="1">
<li><strong>Causes must come before effects.</strong> 𝑋 speaks and then 𝑌 listens.<br>
</li>
<li><strong>Causes and effects are associated, meaning they go together or covary.</strong> When 𝑋 happens, 𝑌 is more likely to happen. I say “more likely” because the effect does not <em>always</em> need to happen for there to be a causal relationship between 𝑋 and 𝑌. For instance, smoking increases the probability of developing lung cancer, but not all smokers will develop lung cancer. Most of the relationships we study in global health are like this—probabilistic in nature, not deterministic.</li>
<li><strong>There are no other plausible alternative explanations for the effect other than the proposed cause.</strong> In other words, 𝑌 listens to 𝑋 and not to something else that also happens to be related to 𝑋. During the smoking debate of the 1950s and 1960s, some proponents of smoking asked whether the apparent causal link between smoking and lung cancer could be explained by a smoking gene that predisposed people to smoking and to lung cancer.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ol>
<p>The need to rule out plausible alternative explanations keeps many researchers up at night. Claims from studies that fail to do this convincingly are characterized as having low <strong>internal validity</strong>. In other words, there is not a strong justification for inferring that the observed relationship between 𝑋 and 𝑌 is causal.</p>
<section id="causes-in-global-health-research" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="causes-in-global-health-research">Causes in global health research</h4>
<p>We study a variety of potential causes in global health research and call them by different names. For instance, global mental health researchers often develop and test <em>interventions</em> delivered to individuals or groups to prevent or treat conditions such as depression. Development agencies administer <em>programs</em> to improve people’s well-being, including economic assistance programs intended to reduce poverty. Clinical researchers and biostatisticians test the efficacy of drugs and medical devices, generally referred to as <em>treatments</em> or <em>therapies</em>, on health outcomes, such as the COVID-19 vaccinations developed in 2020. Policy researchers and health economists study the health and financial impacts of <em>policies</em>, such as removing fees to deliver a baby at public health facilities. Epidemiologists estimate the effect of <em>exposures</em>, such as smoking, on the health status or prognosis of a target population.</p>
<div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>No causation without manipulation?</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some scholars believe that it must be possible to manipulate, or change, a variable for that variable to be considered a cause. This would exclude <em>immutable variables</em> such as age and genetic sex as causes of effects because there is not currently a way to plausibly intervene to change them. This is sometimes framed as the need for a “well-defined intervention”. I don’t personally agree with the “no causation without manipulation” mantra, but I value the way it encourages us to focus on research questions that can improve public health.</p>
</div>
</div>
</div>
</section>
</section>
<section id="effects" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="effects">EFFECTS</h3>
<p>Causal inference is an exercise in counterfactual thinking, full of “what if” questions about the road not taken. We ask ourselves these questions all the time. What would have happened if I had taken that job? Said “yes” instead of “no”? How would life be different today?</p>
<section id="counterfactuals-and-potential-outcomes" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="counterfactuals-and-potential-outcomes">Counterfactuals and potential outcomes</h4>
<p>A <strong>counterfactual</strong> is the hypothetical state of a “what if” question. With counterfactual thinking, there is what actually happened, and then there is the hypothetical counterfactual of what would have happened, counter to fact, under the alternative scenario. The difference between what did happen and what would have happened is a known as the <strong>causal effect</strong>.</p>
<p>Robert Frost fans see the problem.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/frost.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">“The Road Not Taken”, by Robert Frost, <em>The Atlantic Monthly</em>, 1915.</figcaption><p></p>
</figure>
</div>
</div></div><blockquote class="blockquote">
<p>Two roads diverged in a yellow wood, <em>and sorry I could not travel both and be one traveler</em>…</p>
</blockquote>
<p>Robert has to choose one road; he can’t take both simultaneously. Robert can either take the road more traveled, a decision we’ll call 𝑋 = 0, or he can take the road less traveled, a decision we’ll call 𝑋 = 1.</p>
<div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>What’s with the Xs and Ys, 1s and 0s?</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In some ways it might be easier to refer to Robert’s decision as the variable <code>road</code>, or 𝑅, and set the values of <code>road</code> to “more traveled” or “less traveled”, representing his two choices. But instead I’m referring to his decision as 𝑋 and to the values of 𝑋 as 0 (more traveled) or 1 (less traveled). Why?</p>
<p>Often we refer to potential causes as 𝑋 and to response variables as 𝑌. And typically, when the treatment (or exposure) 𝑋 can take two levels, such as treated/not treated, we label treated 1, and not treated 0.</p>
<p>In this example, I imagine Robert looking left and then looking right. Observing that the second path was “grassy and wanted wear”, he decided to go right, taking the “one less traveled by”. Frost claims that his decision to take the uncommon path “made all the difference”, so I’m labeling this path (less traveled) as the treatment, 𝑋 = 1.</p>
</div>
</div>
</div>
<p>Robert’s two choices correspond to two <strong>potential outcomes</strong>, or states of the world, that he could experience <span class="citation" data-cites="rubin:1974">(<a href="references.html#ref-rubin:1974" role="doc-biblioref">Rubin, 1974</a>)</span>. There is the potential outcome that results from taking the road <em>more</em> traveled, and the potential outcome that results from taking the road <em>less</em> traveled. We’ll call these scenarios <em>Y<sub>𝑖</sub></em><sup>X=0</sup> (what happens if he takes the road more traveled) and <em>Y<sub>𝑖</sub></em><sup>X=1</sup> (what happens if he takes the road less traveled).</p>
<div class="no-row-height column-margin column-container"><div id="ref-rubin:1974" class="csl-entry" role="doc-biblioentry">
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies. <em>Journal of Educational Psychology</em>, <em>66</em>(5), 688.
</div></div><p>So what does he do? He famously takes the road <em>less</em> traveled. Robert’s factual outcome is what happens after making this choice. His other potential outcome will never be observed. Taking the road more traveled is now the counterfactual. He can only wonder what would have happened, counter to fact, if he had taken the beaten path.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Could Robert experience the counterfactual by returning at a later date to take the other road? No, because he wouldn’t be the same person who stood there at the start. He could not un-experience the road less traveled. Also, we’d be measuring his happiness at two different points in time. Besides, he did not expect to return: “Yet knowing how way leads on to way / I doubted if I should ever come back.”</p>
</div></div><p>And yet, Robert boldly claims that taking the road less traveled “made all the difference”. How can he know for sure? We defined causal effects as the difference between what did happen and what would have happened, but we only observed what happened, not what would have. Therefore, we have a missing data problem.</p>
<p>This is what’s often called the <strong>fundamental problem of causal inference</strong> <span class="citation" data-cites="holland:1986">(<a href="references.html#ref-holland:1986" role="doc-biblioref">Holland, 1986</a>)</span>: we only get to observe <em>one</em> potential outcome for any given person (or unit, more generally). The causal effect of taking the road less traveled is the difference in potential outcomes: 𝛿<sub>i</sub> = 𝑌<sub>𝑖</sub><sup>𝑋=1</sup> - 𝑌<sub>𝑖</sub><sup>𝑋=0</sup>, but 𝑌<sub>𝑖</sub><sup>𝑋=0</sup> is missing. Therefore, we can’t measure this effect for Robert (𝑖).</p>
<div class="no-row-height column-margin column-container"><div id="ref-holland:1986" class="csl-entry" role="doc-biblioentry">
Holland, P. W. (1986). Statistics and causal inference. <em>Journal of the American Statistical Association</em>, <em>81</em>(396), 945–960.
</div></div></section>
<section id="average-treatment-effect" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="average-treatment-effect">Average treatment effect</h4>
<p>We can, however, compare groups of people like Robert who take one road or the other and <em>estimate</em> the <strong>average treatment effect</strong>, or ATE. I’m emphasizing “estimate” because truly calculating the ATE would require knowing both potential outcomes for each person (or unit, like classrooms or schools). We can only observe one potential outcome for any given person, but let’s ignore this for a moment to understand the true ATE.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The average treatment effect is also called the sample average treatment effect, or SATE.</p>
</div></div><p>While we’re pretending, let’s imagine that the response variable Robert writes about in his poem is <em>happiness</em> later in life, and <em>happiness</em>, or 𝑌, is measured on a scale of 0 to 10 where 0 is not at all happy and 10 is very happy.</p>
<p><a href="#fig-po">Figure&nbsp;<span>3.2</span></a> shows fictional happiness data for a sample of 12 people, including Robert, under both potential outcomes. Notice how each person in the left panel has two values: the happiness that would result if they <span style="color: #1f9ac9;">took the road <em>less</em> traveled (𝑋=1)</span> and the happiness that would result if they <span style="color: #e69138;">took the road <em>more</em> traveled (𝑋=0)</span>. Some people, like Traveler 1, would be happiest taking the <span style="color: #e69138;">road more traveled</span>, whereas other people, such as Robert (11), would find greater happiness on the <span style="color: #1f9ac9;">road less traveled</span>. (Remember, we’re pretending here. We never observe both potential outcomes.)</p>
<div class="column-body-outset">
<div id="fig-po" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/figures/po.png" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.2: Potential outcomes, part 1.</figcaption><p></p>
</figure>
</div>
</div>
<p>As shown in the right panel of <a href="#fig-po">Figure&nbsp;<span>3.2</span></a>, the ATE is calculated as the difference between group averages (𝐸[𝑌<sub>𝑖</sub><sup>𝑋=1</sup>]-𝐸[𝑌<sub>𝑖</sub><sup>𝑋=0</sup>]), which is equivalent to the average of traveler’s individual causal effects. In this example where we are all knowing, the ATE is 1.5; taking the <span style="color: #1f9ac9;">road more traveled</span> increases happiness on average by 1.5 points on a scale of 0 to 10.</p>
<p>We’re not all knowing, of course, and we only get to observe one potential outcome for each person. This means we have to estimate the ATE by comparing people like Robert who take the <span style="color: #1f9ac9;">road less traveled</span> to people who take the <span style="color: #e69138;">road more traveled</span>. But how do people come to take one road versus the other?</p>
<p><a href="#fig-po2">Figure&nbsp;<span>3.3</span></a> shows two scenarios (of an infinite number). On the left (A), travelers are randomly assigned to a road. Imagine that they come to the fork in the road and pull directions out of a hat. Half are assigned to the <span style="color: #1f9ac9;">road less traveled</span>, and half to the <span style="color: #e69138;">road more traveled</span>. On the right (B), however, travelers choose a road themselves. Imagine that they go with their gut and all happen to pick the road that maximizes their individual happiness. In both panels, the grey dots represent the unobserved counterfactuals.</p>
<div class="column-body-outset">
<div id="fig-po2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/figures/po2.png" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.3: Potential outcomes, part 2.</figcaption><p></p>
</figure>
</div>
</div>
<p>Notice that under perfect randomization (left, A), the estimated ATE 1.5 is equal to the true (but unknowable) ATE. But on the right (B), when travelers selected their own road, the estimate is 4.7. This does not line up with the true ATE because the estimate includes bias. Remember that bias is anything that takes us away from the truth.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The simple difference in outcomes (SDO) is an estimator for the ATE. It contains the ATE plus bias.</p>
</div></div><p><span class="citation" data-cites="cunningham:2021">Cunningham (<a href="references.html#ref-cunningham:2021" role="doc-biblioref">2020</a>)</span> very effectively shows how to decompose the simple difference estimator (SDO) into three parts: the ATE, selection bias, and heterogeneous treatment effect bias. Selection bias occurs when we are comparing unequal groups. Heterogeneous treatment effect bias occurs when a treatment (or exposure) has differential effects on units based on their characteristics.</p>
<p>As we will see in later chapters, randomization can be a very effective way to neutralize selection bias, but randomization is not always possible or maintained. Most research is non-experimental, or what many would call <strong>observational</strong>. Thus even if we assume that treatment effects are constant (effectively ignoring heterogeneous treatment effect bias), selection bias will remain a threat in many cases. Unfortunately for us, we can’t simply calculate it and subtract it away because the exact quantity is typically unknowable. That leaves us with with research design and statistical adjustment as our only defense. As <span class="citation" data-cites="cunningham:2021">Cunningham (<a href="references.html#ref-cunningham:2021" role="doc-biblioref">2020</a>)</span> states:</p>
<div class="no-row-height column-margin column-container"></div><blockquote class="blockquote">
<p>One could argue that the entire enterprise of causal inference is about developing a reasonable strategy for negating the role that selection bias is playing in estimated causal effects.</p>
</blockquote>
</section>
</section>
<section id="causal-inference-methods" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="causal-inference-methods">CAUSAL INFERENCE METHODS</h3>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<iframe width="300" height="169" src="https://www.youtube.com/embed/JjE6ZK8vvcE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p>Watch Dr.&nbsp;Esther Duflo’s speech at the Nobel Banquet, 10 December 2019.</p>
</div></div></div>
<p>There are different causal inference methods for addressing selection bias, and which one a researcher chooses tends to be heavily influenced by their context and discipline. For instance, clinical researchers, biostatisticians, and behavioral interventionists often prefer to use <strong>experimental designs</strong> that randomly allocate people (or units) to different treatment arms.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> There is also a rich tradition of experimentation in the social sector among economists and public policy scholars. Economists Abhijit Banerjee, Esther Duflo, and Michael Kremer won the 2019 Nobel Prize in Economics<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> for their experimental approach to alleviating global poverty.</p>
<p>But as I stated previously, many research questions in global health are not amenable to experimentation, and the approach to causal inference must be rooted in non-experimental (or observational) data. <span class="citation" data-cites="matthay:2020b">Matthay, Hagan, et al. (<a href="references.html#ref-matthay:2020b" role="doc-biblioref">2020</a>)</span> divide these non-experimental approaches into two main buckets: confounder-control and instrument-based. <strong>Confounder-control</strong> is characterized by the use of statistical adjustment to make groups more comparable. You’ll find many examples of confounder-control in epidemiology and public health journals. <strong>Instrument-based</strong> studies, sometimes called <strong>quasi-experimental</strong> designs, estimate treatment effects by finding and leveraging arbitrary reasons why some people are more likely to be treated or exposed. Instrument-based studies are quite common in economics and psychology. I’ll introduce each approach in turn.</p>
<div class="no-row-height column-margin column-container"></div><div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>Additional causal inference frameworks</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Bradford Hill’s Considerations.</strong> In 1965, Epidemiologist and statistician Sir Austin Bradford Hill proposed a set of nine domains to consider when evaluating the evidence for a causal relationship: (1) strength, (2) consistency, (3) specificity, (4) temporality, (5) biological gradient, (6) plausibility, (7) coherence, (8) experiment, and (9) analogy. It does not feature prominently in current debates about causal inference, and it’s commonly misapplied. <em>Modern Epidemiology</em> has a good summary and critique: <a href="https://ghr.link/mod">ghr.link/mod</a>.</p>
<p><strong>Sufficient Cause Framework and Causal Pies.</strong> The sufficient-component cause model is an approach for conceptualizing cause and effect used largely for pedagogical purposes. Dr.&nbsp;Kenneth Rothman defined sufficient causes as the minimal set of conditions that produce a given outcome. This set of causes is often represented by pie charts.</p>
</div>
</div>
</div>
</section>
</section>
<section id="causal-diagrams-and-confounder-control" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="causal-diagrams-and-confounder-control"><span class="header-section-number">3.2</span> Causal Diagrams and Confounder-Control</h2>
<p><strong>Confounding</strong> is a type of bias where variables 𝑋 and 𝑌 share a common cause 𝑍 that explains some or all of of the 𝑋𝑌 relationship. You’re likely familiar with examples of confounding like ice cream sales and violent crime. If you look just in the data, it looks like increases in ice cream sales could be causing increases in violent crime (or maybe vice versa), but this is what we call a spurious correlation. Ice cream sales and violent crime are both more common when the weather is warm. Once you statistically control for weather, let’s say by looking just at sales on hot days, there is no relationship between ice cream sales and crime.</p>
<p>Causal relationships observed in non-experimental contexts are at high risk of confounding, and the goal of confounder-control studies is to find and statistically adjust for a sufficient set of variables to eliminate confounding. But this is not just an exercise in statistics because data are profoundly dumb <span class="citation" data-cites="pearl:2018">(<a href="references.html#ref-pearl:2018" role="doc-biblioref">Pearl et al., 2018</a>)</span>. A dataset cannot tell you which variables to adjust for, or what is a cause and what is an effect. For that you need information that lives outside of statistical models. You need causal models that are informed by domain expertise <span class="citation" data-cites="mcelreath:2020">(<a href="references.html#ref-mcelreath:2020" role="doc-biblioref">McElreath, 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pearl:2018" class="csl-entry" role="doc-biblioentry">
Pearl, J. et al. (2018). <em>The book of why: The new science of cause and effect</em> (1st ed.). Basic Books, Inc.
</div></div><div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<iframe width="300" height="169" src="https://www.youtube.com/embed/oXSnGcCcLi4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p>Watch Dr.&nbsp;Nick Huntington-Klein introduce causal diagrams.</p>
</div></div></div>
<p>For this reason, a graphical approach based on <strong>causal diagrams</strong> has emerged as a popular tool for causal inference in confounder-control studies <span class="citation" data-cites="pearl:1995">(<a href="references.html#ref-pearl:1995" role="doc-biblioref">Pearl, 1995</a>)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> The most common type of graphical model you’ll encounter is the causal <strong>directed acyclic graph, or DAG</strong>. <a href="#fig-dag">Figure&nbsp;<span>3.4</span></a> shows an example DAG of the effect of taking the road less traveled on happiness.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pearl:1995" class="csl-entry" role="doc-biblioentry">
Pearl, J. (1995). Causal diagrams for empirical research. <em>Biometrika</em>, <em>82</em>(4), 669–688.
</div></div><div class="column-body">
<div id="fig-dag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/dag%20example.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.4: Causal directed acyclic graph (DAG) of the effect of taking the road less traveled on happiness.</figcaption><p></p>
</figure>
</div>
</div>
<section id="causal-stories" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="causal-stories">CAUSAL STORIES</h3>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p>Dr.&nbsp;Scott Cunningham’s book, <em>Causal Inference: The Mixtape</em>, is worth every penny. If you’re short on pennies, read it online <a href="http://ghr.link/cun">for free</a>.</p>
</div></div></div>
<p><span class="citation" data-cites="cunningham:2021">Cunningham (<a href="references.html#ref-cunningham:2021" role="doc-biblioref">2020</a>)</span> frames DAGs as storytelling devices. The story I am telling with this DAG is that the road traveled causes happiness directly and indirectly by creating new social relationships. This DAG also shows my assumption that happiness AND the decision to take the road less traveled are both caused in part by one’s cognitive style (e.g., sense of optimism); they share a common cause. Happiness is also caused by income which, like cognition, is a function of background characteristics like genetics and family.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cunningham:2021" class="csl-entry" role="doc-biblioentry">
Cunningham, S. (2020). <em><a href="https://amzn.to/3YAf3mv">Causal inference: The mixtape</a></em>. Yale University Press.
</div></div><div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>Start by drawing your assumptions</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before I even do anything with this DAG, or any DAG I create, I’ve accomplished a lot just by drawing my assumptions. The DAG represents my belief in the data generating process. It includes all nodes and connections that I believe are relevant to the effect of <code>road traveled</code> on <code>happiness</code>. I’ve made my assumptions clear and can proceed to identify how I will estimate the causal effect of interest.</p>
<p>Now you might call bullshit, and that’s OK. You can draw a different DAG that might have different implications for the best analysis strategy. You and I should be able to defend our assumptions and be open to modifications based on subject matter criticism. But whether you draw a DAG or not, there is no escaping the need to make assumptions. DAGs just help to make your assumptions clear and transparent.</p>
</div>
</div>
</div>
</section>
<section id="components-of-a-dag" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="components-of-a-dag">COMPONENTS OF A DAG</h3>
<p>As a graph, DAGs consist of nodes and edges (or arrows). <strong>Nodes</strong> are variables like our exposure of interest, the <code>road traveled</code>, and our outcome of interest, <code>happiness</code> later in life. Nodes can take any form, from discrete values of <code>road traveled</code> (more traveled, less traveled) to continuous values of <code>income</code>. A DAG can include observed (measured) variables and unobserved variables, including background factors such as genetics.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-dag2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/dag%20example.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.5: The same DAG again, reprinted for your convenience.</figcaption><p></p>
</figure>
</div>
</div></div><p>Nodes are connected by <strong>edges</strong>, directed arrows that make causal statements about how two variables are related. For instance, by drawing an arrow from <code>road traveled</code> to <code>happiness</code>, I’m asserting that the road one travels causes happiness. Arrows do not indicate whether this relationship is positive or negative, just that <code>road traveled</code> influences <code>happiness</code>. Equivalently, the absence of an arrow between nodes implies that there is no causal relationship.</p>
<p>The only hard rule in a DAG is that cycles are not permitted. Arrows can go into and out of a node, but there must not be any recursive pathways. For instance, <code>social relationships</code> ⟶ <code>happiness</code> ⟶ <code>social relationships</code> is not allowed. Causal effects must only flow forward in time. Spirals are allowed, however, and offer a way to represent that causal relationships between variables measured at different time points (e.g., <code>social relationships_t1</code> ⟶ <code>happiness_t2</code> ⟶ <code>social relationships_t3</code> ⟶ <code>happiness_t4</code>).</p>
<p>There are three possible relationship structures in a DAG <span class="citation" data-cites="mcelreath:2020">(<a href="references.html#ref-mcelreath:2020" role="doc-biblioref">McElreath, 2020</a>)</span>:</p>
<div class="no-row-height column-margin column-container"></div><ol type="1">
<li><strong>Forks</strong>: In forks like <code>road traveled</code> ⟵ <code>cognition</code> ⟶ <code>happiness</code>, <code>cognition</code> is a common cause of the focal variables of interest. As such, <code>cognition</code> confounds the causal effect of <code>road traveled</code> on <code>happiness</code>; some (or all) of the observed association is due to <code>cognition</code>. When you see a fork, you should think confounding.</li>
<li><strong>Pipes</strong>: Pipes (or chains) involve mediator variables like <code>social relationships</code> that represent an indirect causal chain of effects. For instance, <code>road traveled</code> causes new <code>social relationships</code> which causes <code>happiness</code>. Whether or not you are interested in the indirect causal effect depends on your research question.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
<li><strong>Colliders</strong>: Colliders (or inverted forks) are closed pathways like <code>road traveled</code> ⟶ <code>active</code> lifestyle ⟵ <code>happiness</code> where a node on the pathway only has incoming arrows. These pathways are closed by default and only open when conditioning on the collider, thereby distorting the relationship between <code>road traveled</code> and <code>happiness</code>.</li>
</ol>
<p>As you will see shortly, being able to recognize these relationships will help you to identify your causal effect of interest.</p>
<div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
<strong>Descendants and ancestors</strong>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Graphs like DAGs can also be described by the ancestry of the nodes. A <strong>descendant</strong> variable has at least one incoming arrow. Economists would call this an <strong>endogenous</strong> variable (vs an <strong>exogenous</strong> variable like <code>background</code> that has no incoming arrows). In the example DAG, <code>happiness</code> and <code>social relationships</code> are descendants of <code>road traveled</code>. Specifically, <code>social relationships</code> is a <strong>child</strong> of <code>road traveled</code>, and <code>road traveled</code> is its <strong>parent</strong>. Therefore, <code>road traveled</code> and <code>social relationships</code> are <strong>ancestors</strong> of <code>happiness</code>.</p>
</div>
</div>
</div>
</section>
<section id="decide-which-nodes-to-include-in-a-dag" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="decide-which-nodes-to-include-in-a-dag">DECIDE WHICH NODES TO INCLUDE IN A DAG</h3>
<p>In order to use a DAG to identify a causal effect, you must include all of the relevant nodes and paths <span class="citation" data-cites="rohrer:2018">(<a href="references.html#ref-rohrer:2018" role="doc-biblioref">Rohrer, 2018</a>)</span>. As you can probably imagine, this can get out of hand quickly. Just look at the slide in <a href="#fig-pentagon">Figure&nbsp;<span>3.6</span></a> that diagrams the American military’s perceived challenge in its war in Afghanistan.</p>
<div class="no-row-height column-margin column-container"><div class="">
<p>This includes variables that you can measure and the ones you can’t (or didn’t) observe.</p>
</div></div>
<div class="column-body">
<div id="fig-pentagon" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/pentagon.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.6: War is hard. Source: U.S. Joint Chiefs of Staff. (Not a DAG, per se, but you get the point.)</figcaption><p></p>
</figure>
</div>
</div>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p>Dr.&nbsp;Huntington-Klein’s book, <em>The Effect: An Introduction to Research Design and Causality</em>, should be on your bookshelf. If you’re not in a position to purchase it today, read it online <a href="http://ghr.link/eff">for free</a>.</p>
</div></div></div>
<p><span class="citation" data-cites="huntington:2021">Huntington-Klein (<a href="references.html#ref-huntington:2021" role="doc-biblioref">2021</a>)</span> frames DAG creation as a balancing act:</p>
<div class="no-row-height column-margin column-container"></div><blockquote class="blockquote">
<p>On one hand, we want to omit from the diagram every variable and arrow we can possibly get away with. The simpler the diagram is, the easier it is to understand, and the more likely it is that we’ll be able to figure out how to identify the answer to our research question…On the other hand, omitting things makes the model simpler. But the real world is complex. So in our quest for simplicity, we might end up leaving out something that’s really important.</p>
</blockquote>
<p>A piece of practical advice is to draw a basic DAG and then add additional variables (nodes) only if you believe they causally affect two or more existing nodes in the DAG <span class="citation" data-cites="rohrer:2018">(<a href="references.html#ref-rohrer:2018" role="doc-biblioref">Rohrer, 2018</a>)</span>. For instance, maybe you could argue that being left handed also contributes to one’s decision to take the road less traveled. If handedness does not have an arrow into any other nodes, you can safely leave it out of the DAG.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rohrer:2018" class="csl-entry" role="doc-biblioentry">
Rohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(1), 27–42.
</div></div><p>If this feels daunting, you’re doing it right. Science is hard, and I predict that you’ll find this process easier if you have the humility to know that, at best, your study will approximate the truth. There is a very good chance that your DAG will be wrong or incomplete. Your colleagues might tell you as much. This is part of the scientific process. Criticism should lead you to revise your DAG or strengthen how you defend your assumptions.</p>
</section>
<section id="tracing-all-paths" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="tracing-all-paths">TRACING ALL PATHS</h3>
<p>Once you’ve drawn your DAG, the next step is to list all of the paths from the proposed cause to the outcome of interest. In this example, it means tracing all of the paths that go from <code>road traveled</code> to <code>happiness</code>.</p>
<p>Start with <code>road traveled</code> and move your finger along each path until you get to another variable or to the outcome, <code>happiness</code>. When you encounter variables with arrows coming in or out, trace each path to the outcome. The direction of the arrows does not matter for tracing, just don’t trace back to variables you’ve already visited (in other words, no loops).</p>
<ol type="1">
<li><code>road traveled</code> ⟶ <code>happiness</code></li>
<li><code>road traveled</code> ⟶ <code>social relationships</code> ⟶ <code>happiness</code></li>
<li><code>road traveled</code> ⟵ <code>cognition</code> ⟶ <code>happiness</code></li>
<li><code>road traveled</code> ⟵ <code>cognition</code> ⟵ <code>background</code> ⟶ <code>income</code> ⟶ <code>happiness</code></li>
<li><code>road traveled</code> ⟶ <code>active</code> ⟵ <code>happiness</code></li>
</ol>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p>View the example DAG or create your own at <a href="http://ghr.link/dag">DAGitty.net</a>.</p>
</div></div></div>
<p>You can check your work—or skip the manual tracing process entirely—by creating and analyzing your DAG in R. While you can manually enter your graph into R, a shortcut is to create your DAG visually in your browser at <code>DAGitty.net</code> and copy/paste the model code into R. The function <code>paths()</code> returns the same five paths we traced by hand.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dagitty)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">'dag {</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">bb="0,0,1,1"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">"road traveled" [exposure,pos="0.224,0.452"]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">"social relationships" [pos="0.355,0.547"]</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">active [pos="0.359,0.677"]</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">background [pos="0.460,0.166"]</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">cognition [pos="0.355,0.340"]</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">happiness [outcome,pos="0.501,0.449"]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">income [pos="0.505,0.285"]</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="st">"road traveled" -&gt; "social relationships"</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">"road traveled" -&gt; active</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="st">"road traveled" -&gt; happiness</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">"social relationships" -&gt; happiness</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">background -&gt; cognition</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">background -&gt; income</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">cognition -&gt; "road traveled"</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="st">cognition -&gt; happiness</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">happiness -&gt; active</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="st">income -&gt; happiness</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="st">}'</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">paths</span>(dag)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$paths
[1] "\"road traveled\" -&gt; \"social relationships\" -&gt; happiness"         
[2] "\"road traveled\" -&gt; active &lt;- happiness"                           
[3] "\"road traveled\" -&gt; happiness"                                     
[4] "\"road traveled\" &lt;- cognition -&gt; happiness"                        
[5] "\"road traveled\" &lt;- cognition &lt;- background -&gt; income -&gt; happiness"

$open
[1]  TRUE FALSE  TRUE  TRUE  TRUE</code></pre>
</div>
</div>
</section>
<section id="effect-identification" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="effect-identification">EFFECT IDENTIFICATION</h3>
<p>Next we need to determine which paths are good and which are bad <span class="citation" data-cites="huntington:2021">(<a href="references.html#ref-huntington:2021" role="doc-biblioref">Huntington-Klein, 2021</a>)</span>. “Good paths” identify our research question. “Bad paths” are the alternate explanations for the causal effect that we need to close. In our example DAG, as in many DAGS, good paths often start with an arrow exiting the proposed cause and bad paths have arrows entering the proposed cause.</p>
<div class="no-row-height column-margin column-container"><div class="">
<p>An exception would be if we are only interested in the direct effect of <code>road traveled</code> on <code>happiness</code>, we would label the mediation pathway through <code>social relationships</code> as a bad path and work to close it.</p>
</div></div>
<p><a href="#fig-roadpaths">Figure&nbsp;<span>3.7</span></a> visualizes the five paths in our example DAG and labels them good or bad. Notice that paths 1 and 2—the good paths—start with <code>road traveled</code> and flow forward to <code>happiness</code>. The rest are <strong>backdoor paths</strong> that we need to close.</p>
<div class="column-page">
<div id="fig-roadpaths" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/road%20dag%20paths.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.7: Good and bad paths in our example DAG.</figcaption><p></p>
</figure>
</div>
</div>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p>Dr.&nbsp;Andrew Heiss has a <a href="http://ghr.link/and">great tutorial</a> on ways to close backdoors through regression, inverse probability weighting, and matching.</p>
</div></div></div>
<p>Open backdoor paths bias the causal effect we want to estimate, so we need to close them. We can do this by conditioning on variables that confound the causal effect of interest through techniques like regression. The neat thing is that we do not necessarily need to control for every possible confounder, just a minimum set sufficient to close all backdoor paths.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> For instance, in <a href="#fig-roadpaths">Figure&nbsp;<span>3.7</span></a>, you can see that adjusting for <code>cognition</code> in paths 3 and 4 is sufficient to close the open backdoor paths.</p>
<p>Path 5 is also a backdoor path, but it’s closed by default because <code>active</code> is a collider (see the two incoming arrows). If we condition on <code>active</code>, let’s say by including it as a covariate in our regression, we will inadvertently open this path and introduce bias. There is such a thing as a bad covariate, and sometimes less is more when it comes to statistical models <span class="citation" data-cites="westreich:2013 mcelreath:2020">(<a href="references.html#ref-mcelreath:2020" role="doc-biblioref">McElreath, 2020</a>; <a href="references.html#ref-westreich:2013" role="doc-biblioref">Westreich et al., 2013</a>)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="ref-westreich:2013" class="csl-entry" role="doc-biblioentry">
Westreich, D. et al. (2013). The table 2 fallacy: Presenting and interpreting confounder and modifier coefficients. <em>American Journal of Epidemiology</em>, <em>177</em>(4), 292–298.
</div></div><div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p>The logic of <em>d-separation</em> is rooted in Pearl’s <em>do-calculus</em>. Learn more in <em>The Book of Why</em> or <a href="http://ghr.link/doc">this resource that Heiss created</a>.</p>
</div></div></div>
<p>If this DAG is correct and complete, adjusting for (controlling for) <code>cognition</code> on the backdoor path makes the relationship between <code>road traveled</code> and <code>happiness</code> <strong><em>d-separated</em></strong> (<em>direction separated</em>) from all other nodes. In other words, the causal effect is said to be <strong>identified</strong>.</p>
</section>
<section id="effect-estimation" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="effect-estimation">EFFECT ESTIMATION</h3>
<p>There are various statistical techniques to cut the bad paths so they don’t bias our causal effect of interest. I’ll show you regression. See <span class="citation" data-cites="heiss:2020dags">Heiss (<a href="references.html#ref-heiss:2020dags" role="doc-biblioref">2020b</a>)</span> for other examples.</p>
<div class="no-row-height column-margin column-container"><div id="ref-heiss:2020dags" class="csl-entry" role="doc-biblioentry">
Heiss, A. (2020b). <em><a href="https://ghr.link/doc">Ways to close backdoors in DAGs</a></em>.
</div></div><section id="simulation" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="simulation">Simulation</h4>
<p>For this example, I simulated a dataset on 1000 people who decided to take the road less traveled or the road more traveled.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> This is an observational dataset. There was no random assignment to <code>road_traveled</code>. The core variables include:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Simulation is the only way to demonstrate that controlling for <code>cognition</code> recovers the correct effect. In real world datasets you don’t know the right answer. If you did, causal inference would be easy.</p>
</div></div><ul>
<li><strong><code>cognition</code></strong> (confounding variable): A binary (0/1) variable set to <code>1</code> if the person scored high on a measure of optimism, otherwise <code>0</code>. Imagine that everyone completed a questionnaire before they decided on a road. This questionnaire included items that assessed aspects of their cognitive style, and we used their answers to construct an indicator of high/low optimism.</li>
<li><strong><code>road_traveled</code></strong> (exposure/treatment): A binary (0/1) variable set to <code>1</code> if the person chose the road less traveled, otherwise <code>0</code>. This variable was simulated to be a function of <code>cognition</code>. In this dataset, the odds of taking the road less traveled are 18 times higher for high optimism folks.</li>
<li><strong><code>happiness</code></strong> (outcome): A variable that can range from 0 to 10, where 10 represents greatest happiness. Imagine that data on happiness was collected 10 years after people selected and traveled down one of the roads. This variable was simulated to be a function of <code>cognition</code> and <code>happiness</code>. On average, high optimism folks scored 1 point higher on the measure of <code>happiness</code>.</li>
<li><strong><code>active</code></strong> (collider): A binary (0/1) variable set to <code>1</code> if the person has an active lifestyle. Imagine that this variable was collected at sometime after the person traveled the road. This variable was simulated to be a function of <code>road_traveled</code> and <code>happiness</code>. <code>active</code> does not cause anything in the model.</li>
</ul>
<div class="cell">

</div>
<p>I simulated the data so that the road less traveled <em>increases happiness by 1.5 points</em>. That will be the correct answer going forward. Here are my receipts:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df_road_A <span class="sc">%&gt;%</span> </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean =</span> <span class="fu">mean</span>(happiness_road1<span class="sc">-</span>happiness_road0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 1
   mean
  &lt;dbl&gt;
1  1.50</code></pre>
</div>
</div>
</section>
<section id="the-problem-confounding" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="the-problem-confounding">The Problem: Confounding</h4>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-roadconfounding" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/road_confounding.png" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.8: <code>cognition</code> confounds the 𝑋𝑌 relationship.</figcaption><p></p>
</figure>
</div>
</div></div><p><a href="#fig-roadconfounding">Figure&nbsp;<span>3.8</span></a> should remind you that our core problem in this DAG is that the exposure (<code>road_traveled</code>) and the outcome (<code>happiness</code>) share a common cause: <code>cognition</code>. This is to say that <code>cognition</code> confounds the relationship between 𝑋 and 𝑌.</p>
<p>You can see this visually in <a href="#fig-confounding">Figure&nbsp;<span>3.9</span></a>. In the left panel, people who scored high on optimism are represented in red (vs low in black). Notice that red appears more frequently among the road less traveled group AND red looks to be associated with higher happiness scores. If we just compare happiness scores by road traveled, we get the wrong answer (difference of 2.07). This is because <code>cognition</code> biases the 𝑋𝑌 relationship.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Remember, we know the correct answer is 1.5 because that’s how I simulated the data generating process.</p>
</div></div><p>To get to the right answer, we need to hold <code>cognition</code> constant. In the right panel I show this by looking just at people with a low optimism score. Now if we compare happiness scores by road traveled, we get close to the correct answer (difference of 1.47).<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<div class="cell page-columns page-full">
<div class="cell-output-display column-page">
<div id="fig-confounding" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causalinference_files/figure-html/fig-confounding-1.png" class="img-fluid figure-img" width="1200"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.9: Confounding</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-solution-multiple-regression" class="level4 unnumbered page-columns page-full">
<h4 class="unnumbered anchored" data-anchor-id="a-solution-multiple-regression">A Solution: Multiple Regression</h4>
<p>In practice we might estimate the effect via a technique like multiple regression, which I’ll show you here.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> You’ll see that I modified the variable names to make <a href="#fig-roada">Figure&nbsp;<span>3.10</span></a> easier to read. Most notably, the exposure road traveled is represented as <code>x</code> and the outcome happiness is represented as <code>y</code>. Cognition and active are simply <code>c</code> and <code>a</code>, respectively.</p>
<p><a href="#fig-roada">Figure&nbsp;<span>3.10</span></a> presents the results of three models:</p>
<ol type="1">
<li><code>y ~ x</code>: naive regression of <code>happiness</code> (<code>y</code>) on <code>road_traveled</code> (<code>x</code>)</li>
<li><code>y ~ x + c</code>: same as (1) but also controlling for <code>cognition</code> (<code>c</code>)</li>
<li><code>y ~ x + c + a</code>: same as (2) but also controlling for <code>active</code> (<code>a</code>)</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-roada" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causalinference_files/figure-html/fig-roada-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.10: Multiple regression models</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The <span style="color: #F8766D;">red model <code>y ~ x</code></span> repeats the same mistake as above. It just estimates the impact of road traveled <code>x</code> on happiness <code>y</code> without accounting for the confounding role of cognition <code>c</code>. It returns the wrong answer.</p>
<p>The <span style="color: #00BA38;">green model <code>y ~ x + c</code></span> controls for cognition <code>c</code>, thereby removing the parts of <code>x</code> and <code>y</code> that are explained by <code>c</code> <span class="citation" data-cites="heiss:2020">(<a href="references.html#ref-heiss:2020" role="doc-biblioref">Heiss, 2020a</a>)</span>. This closes the biasing pathway and returns the correct answer.</p>
<div class="no-row-height column-margin column-container"><div id="ref-heiss:2020" class="csl-entry" role="doc-biblioentry">
Heiss, A. (2020a). Causal inference. In <em>R for political data science</em> (pp. 235–273). Chapman; Hall/CRC.
</div><div class="">
<p>This should not be surprising. I simulated the data generating process with <code>c</code> confounding the relationship between <code>x</code> and <code>y</code>. Our DAG was therefore correct, and controlling for <code>c</code> recovers the simulated effect.</p>
</div></div>
<p>The <span style="color: #619CFF;">blue model <code>y ~ x + c + a</code></span> gets us into trouble. In the data I simulated, active <code>a</code> is a collider. It doesn’t cause anything in the DAG, and the bad path it sits on is closed by default. When I control for it by adding it to the regression, I open the pathway and distort the relationship between <code>x</code> and <code>y</code>.</p>
<p>It might come as a surprise that something can go wrong by adding covariates to your model. Many of us are taught that it’s probably good or at least neutral to add all seemingly relevant variables to a regression. This is just bad advice. Some covariates will make your estimates worse, not better.</p>
<div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<iframe width="300" height="169" src="https://www.youtube.com/embed/KNPYUVmY3NM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p>Watch Dr.&nbsp;Richard McElreath’s excellent talk, <em>Science Before Statistics: Causal Inference</em>.</p>
</div></div></div>
<p><span class="citation" data-cites="mcelreath:2020">McElreath (<a href="references.html#ref-mcelreath:2020" role="doc-biblioref">2020</a>)</span> uses the term <strong>causal salad</strong> to describe this very common practice of tossing lots of “control” variables into a statistical model and telling a causal story. This approach can work when the goal is prediction, but it can go very very wrong when the goal is causal inference. One of his core points in <em>Statistical Rethinking</em> is that causal inference requires causal models that are separate from statistical models. Statistics alone can get us to the wrong answers. But if we follow the DAG—our causal model—we know to leave <code>active</code> alone.</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
<section id="the-big-fine-print" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="the-big-fine-print">THE BIG FINE PRINT</h3>
<p>DAGs are useful tools for causal inference, but they are not magic. If your DAG does not completely and correctly identify your causal effect of interest, your estimates will be biased. To make matters worse, there is no test that will tell you if you got it right or wrong.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>A step I skipped for space is verifying conditional independencies. <code>DAGitty.net</code> and the <code>{dagitty}</code> R package will get you started.</p>
</div><div id="ref-mcelreath:2020" class="csl-entry" role="doc-biblioentry">
McElreath, R. (2020). <em><a href="https://amzn.to/3FdX2mk">Statistical <span>Rethinking</span>: <span>A</span> <span>Bayesian</span> <span>Course</span> with <span>Examples</span> in <span>R</span> and <span>Stan</span></a></em>. CRC Press.
</div></div><p>But let’s be clear: there is no approach to causal inference that sidesteps the need for unverifiable assumptions <span class="citation" data-cites="mcelreath:2020">(<a href="references.html#ref-mcelreath:2020" role="doc-biblioref">McElreath, 2020</a>)</span>. DAGs require you to make and defend your assumptions, but so does every other approach (you just might not know it).</p>
</section>
</section>
<section id="instrument-based-approaches-and-quasi-experimental-designs" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="instrument-based-approaches-and-quasi-experimental-designs"><span class="header-section-number">3.3</span> Instrument-Based Approaches and Quasi-Experimental Designs</h2>
<p>If confounder-control is about closing backdoors through statistical adjustment, instrument-based approaches are about isolating front doors <span class="citation" data-cites="huntington:2021">(<a href="references.html#ref-huntington:2021" role="doc-biblioref">Huntington-Klein, 2021</a>)</span>. These approaches are sometimes called quasi-experimental designs because they attempt to mimic the beauty and logic of a perfectly conducted randomized controlled trial (RCT).</p>
<div class="no-row-height column-margin column-container"><div id="ref-huntington:2021" class="csl-entry" role="doc-biblioentry">
Huntington-Klein, N. (2021). <em><a href="https://amzn.to/3XcLYfD">The effect: An introduction to research design and causality</a></em>. Chapman; Hall/CRC.
</div></div><section id="experiments-destroy-confounding" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="experiments-destroy-confounding">EXPERIMENTS DESTROY CONFOUNDING</h3>
<p>As shown in <a href="#fig-obsexpdag">Figure&nbsp;<span>3.11</span></a>, RCTs (experiments), are effective because they close all backdoors that run from the proposed cause to the outcome. For instance, if we were somehow able to randomly assign people to the road <em>less</em> traveled or the road <em>more</em> traveled—and if people complied with these assignments—then the only arrow into <code>road_traveled</code> would be randomization. Randomization would be the only cause of <code>road_traveled</code>.</p>
<div class="column-body-outset">
<div id="fig-obsexpdag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/obsexpdag.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.11: Observational DAG vs experimental DAG.</figcaption><p></p>
</figure>
</div>
</div>
<p>Randomization destroys confounding. This includes confounding from variables you think to measure, like <code>cognition</code> in this example, as well as variables that you can’t or don’t measure for whatever reason. This idea is so powerful that many people refer to experiments as the gold-standard when it comes to causal inference.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
</section>
<section id="exogenous-variation-without-randomization" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="exogenous-variation-without-randomization">EXOGENOUS VARIATION WITHOUT RANDOMIZATION</h3>
<p>When randomization is not possible, confounding is likely. You can try to account for this confounding statistically (confounder control), or you can search for partial causes of the exposure/treatment that are unrelated to the outcome. Sometimes you can get lucky and find an exogenous source of variation in the exposure/treatment and use it to identify the causal effect.</p>
<p>For instance, imagine that we couldn’t randomize who travels which road, but we could restrict access to the road less traveled to people born on or after January 1, 1980. By this arbitrary rule, someone born on January 1, 1980 would be allowed to pass, but someone born on December 31, 1979 would have to take the road more traveled. This is the basic setup for a <strong>regression discontinuity</strong> design that fits in the instrument-based or quasi-experimental bucket (<a href="#fig-rddag">Figure&nbsp;<span>3.12</span></a>).</p>
<div class="column-body-outset">
<div id="fig-rddag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/regression%20discontinuity%20road.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.12: Regression discontinuity DAG.</figcaption><p></p>
</figure>
</div>
</div>
<p>In this design, <code>birthdate ≥ 1980-01-01</code> is an <strong>instrument</strong> that causes exogenous variation in who is exposed to the road less traveled (see the left panel of <a href="#fig-roadrd">Figure&nbsp;<span>3.13</span></a>). This variation is almost as good as randomization because the cutoff is arbitrary. Furthermore, when we limit our investigation to people born right around this arbitrary cutoff, any potential link between birthdate and the outcome is broken. We’d argue that people born just before and just after the cutoff are similar in many observable and unobservable ways. The only difference is that people born before the cutoff weren’t allowed to take the road less traveled. This isolates the front door from <code>road_traveled</code> to <code>happiness</code>.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display column-page">
<div id="fig-roadrd" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="causalinference_files/figure-html/fig-roadrd-1.png" class="img-fluid figure-img" width="1632"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.13: A regression discontinuity example.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The key to this design, and others in this category, is that the instrument changes the probability of exposure (treatment) WITHOUT having any other mechanism of impacting the outcome <span class="citation" data-cites="matthay:2020b">(<a href="references.html#ref-matthay:2020b" role="doc-biblioref">Matthay, Hagan, et al., 2020</a>)</span>. In a later chapter we’ll see examples of regression discontinuity in practice, along with other quasi-experimental designs like instrumental variables, difference-in-differences, and interrupted time series.</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
<section id="key-assumptions-and-threats-to-internal-validity" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="key-assumptions-and-threats-to-internal-validity"><span class="header-section-number">3.4</span> Key Assumptions and Threats to Internal Validity</h2>
<p>I listed three requirements for causal relationships when I defined causes:</p>
<ol type="1">
<li>causes must come before effects;<br>
</li>
<li>causes and effects are associated, meaning they go together or covary; and</li>
<li>there are no other plausible alternative explanations for the effect other than the proposed cause.</li>
</ol>
<p>The third requirement—no other plausible alternative explanations—is the hardest of them all to meet. Every approach to causal inference relies on mostly untestable assumptions related to this requirement.</p>
<p>A key assumption, <em>no confounding</em>, is known by different names across disciplines: <strong>ignorability</strong> of the treatment assignment (statistics), conditional independence (economics), and (conditional) <strong>exchangability</strong> (epidemiology) <span class="citation" data-cites="gelman:2020">(<a href="references.html#ref-gelman:2020" role="doc-biblioref">Gelman et al., 2020</a>)</span>. This assumption says that there is no relationship between the treatment (or exposure) someone receives and their potential outcomes. In observational studies, this assumption needs to hold after adjusting for any covariates. <span class="citation" data-cites="matthay:2020b">Matthay, Hagan, et al. (<a href="references.html#ref-matthay:2020b" role="doc-biblioref">2020</a>)</span> gives the example of a violation of this assumption where people who are more likely to succeed across the board, regardless of treatment, are more likely to be treated. It would be like stacking the deck in favor of the treatment. Randomization destroys this confounding, but it’s a concern for observational studies. With confounder-control approaches in particular, you always have to worry about omitted variable bias—failing to adjust for the sufficient set of confounding variables.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gelman:2020" class="csl-entry" role="doc-biblioentry">
Gelman, A. et al. (2020). <em>Regression and other stories</em>. Cambridge University Press.
</div><div id="ref-matthay:2020b" class="csl-entry" role="doc-biblioentry">
Matthay, E. C., Hagan, E., et al. (2020). Alternative causal inference methods in population health research: Evaluating tradeoffs and triangulating evidence. <em>SSM Population Health</em>, <em>10</em>, 100526.
</div><div class="">
<p>No measurement error is another assumption, but I’ll save that for the chapter on content validity.</p>
</div></div>
<p>Once you adjust for a set of covariates, you must think about the assumption of <strong>positivity</strong>. It says that all possible covariate subgroups in your study must have a non-zero chance of being exposed to the treatment. For instance, if your adjustment set includes biological sex, it would be a positivity violation if males could not receive the treatment. If you are not adjusting for sex, this is not a concern. <span class="citation" data-cites="westreich:2019">Westreich (<a href="references.html#ref-westreich:2019" role="doc-biblioref">2019</a>)</span> gives the example of adjusting for biological sex in a study about the effects of hysterectomy when it’s only possible for people with a uterus to undergo a hysterectomy. The solution is simple: biological sex should not be a covariate in your adjustment set.</p>
<p>Another assumption is <strong>consistency</strong> (or treatment variance irrelevance). This is the idea that any variations in a treatment or exposure are irrelevant to the causal effect <span class="citation" data-cites="westreich:2019">(<a href="references.html#ref-westreich:2019" role="doc-biblioref">Westreich, 2019</a>)</span>. For instance, if the intervention under investigation is text message reminders to promote medication adherence, consistency is the assumption that variations in timing of message delivery (e.g., morning or night) are not important for the effect of reminders.</p>
<div class="no-row-height column-margin column-container"><div id="ref-westreich:2019" class="csl-entry" role="doc-biblioentry">
Westreich, D. (2019). <em><a href="https://books.google.com/books?id=l4m2xwEACAAJ">Epidemiology by <span>Design</span>: <span>A</span> <span>Causal</span> <span>Approach</span> to the <span>Health</span> <span>Sciences</span></a></em>. Oxford University Press, Incorporated.
</div></div><section id="threats-to-internal-validity" class="level3 unnumbered page-columns page-full">
<h3 class="unnumbered anchored" data-anchor-id="threats-to-internal-validity">THREATS TO INTERNAL VALIDITY</h3>
<p>These causal inference assumptions will be likely familiar to anyone who uses the potential outcomes framework or Pearl’s graphical causal models approach, but folks with a psychology or education background might be more accustomed to identifying and avoiding threats to internal validity in the Campbell tradition <span class="citation" data-cites="scc">(<a href="references.html#ref-scc" role="doc-biblioref">Shadish et al., 2002</a>)</span>. You’ll recall from earlier in this chapter that <strong>internal validity</strong> pertains to the robustness of a causal claim that the observed variation in 𝑌 is caused by 𝑋. A study with a weak causal claim would be said to have low internal validity. <span class="citation" data-cites="matthay:2020">Matthay &amp; Glymour (<a href="references.html#ref-matthay:2020" role="doc-biblioref">2020</a>)</span> offer a helpful crosswalk between potential outcomes, DAGs, and the Campbell tradition, and <span class="citation" data-cites="west:2010">West et al. (<a href="references.html#ref-west:2010" role="doc-biblioref">2010</a>)</span> compare Campbell’s approach with the potential outcomes approach (sometimes called the Rubin Causal Model).</p>
<div class="no-row-height column-margin column-container"><div id="ref-matthay:2020" class="csl-entry" role="doc-biblioentry">
Matthay, E. C., &amp; Glymour, M. M. (2020). A graphical catalog of threats to validity: Linking social science with epidemiology. <em>Epidemiology</em>, <em>31</em>(3), 376.
</div><div id="ref-west:2010" class="csl-entry" role="doc-biblioentry">
West, S. G. et al. (2010). Campbell’s and rubin’s perspectives on causal inference. <em>Psychological Methods</em>, <em>15</em>(1), 18.
</div></div><div class="page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>Quantitude Podcast</em>, Episode 26, <a href="http://ghr.link/int">The Internal Validity Pre-Flight Checklist</a>.</p>
</div></div></div>
<p>In the Campbell tradition, researchers are taught to identify the possible threats to internal validity, practice “control by design”, and strive for “coherent pattern matching”. In short, this means to think about the alternative plausible explanations for causal effects (threats), add design elements such as additional comparison groups to remove or reduce these threats (control by design), and make more complex hypotheses to raise the inferential bar and strengthen one’s causal claim (coherent pattern matching).</p>
<p>Plausible alternative explanations for the causal effect are threats to internal validity. <span class="citation" data-cites="scc">Shadish et al. (<a href="references.html#ref-scc" role="doc-biblioref">2002</a>)</span> enumerate eight specific threats and warn that threats can accumulate (see <a href="#tbl-threats">Table&nbsp;<span>3.1</span></a>). The core threat is confounding, which <span class="citation" data-cites="scc">Shadish et al. (<a href="references.html#ref-scc" role="doc-biblioref">2002</a>)</span> frame as a selection threat.</p>
<div class="no-row-height column-margin column-container"><div id="ref-scc" class="csl-entry" role="doc-biblioentry">
Shadish, W. R. et al. (2002). <em><a href="http://amzn.to/1E8UYIG">Experimental and quasi-experimental designs for generalized causal inference</a></em>. Cengage Learning.
</div></div><div class="cell">
<div class="cell-output-display">
<div id="tbl-threats" class="anchored">

<table class="table" style="width: auto !important; ">
<caption>Table 3.1:  Threats to internal validity from Shadish et al. (2002). </caption>
 <thead>
  <tr>
   <th style="text-align:left;font-weight: bold;"> Threat </th>
   <th style="text-align:left;font-weight: bold;"> Description </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Ambiguous temporal precedence </td>
   <td style="text-align:left;width: 10.5cm; "> Lack of clarity about which variable occurred first may yield confusion about which variable is the cause and which is the effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Selection </td>
   <td style="text-align:left;width: 10.5cm; "> Systematic differences over conditions in respondent characteristics that could also cause the observed effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> History </td>
   <td style="text-align:left;width: 10.5cm; "> Events occurring concurrently with treatment could cause the observed effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Maturation </td>
   <td style="text-align:left;width: 10.5cm; "> Naturally occurring changes over time could be confused with a treatment effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Regression artifact </td>
   <td style="text-align:left;width: 10.5cm; "> When units are selected for their extreme scores, they will often have less extreme scores on other variables, an occurrence that can be confused with a treatment effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Attrition </td>
   <td style="text-align:left;width: 10.5cm; "> Loss of respondents to treatment or to measurement can produce artifactual effects if that loss is systematically correlated with conditions. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Testing </td>
   <td style="text-align:left;width: 10.5cm; "> Exposure to a test can affect test scores on subsequent exposures to that test, an occurrence that can be confused with a treatment effect. </td>
  </tr>
  <tr>
   <td style="text-align:left;width: 5cm; font-weight: bold;"> Instrumentation </td>
   <td style="text-align:left;width: 10.5cm; "> The nature of a measure may change over time or conditions in a way that could be confused with a treatment effect. </td>
  </tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Internal validity is not a property of your research design per se; it’s a characteristic of your claim. While it’s true that some designs, like randomized controlled trials, face fewer threats to internal validity in theory, it’s also true that study implementation matters. The RCT label does not insulate a poorly conducted RCT from criticism. It’s best to think consider the internal validity of claims on a study-by-study basis. Also, there is not a statistical test that will tell you if your claim has high internal validity. You can use the results of various statistical tests to probe the support for your causal claim, but no test will tell you that a claim has high or low internal validity. P-values, for instance, are silent on the issue.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Violations of assumptions like exchangability, positivity, and consistency threaten the internal validity of a claim, but the Campbell tradition does not use these terms.</p>
</div></div></section>
</section>
<section id="coda-coffee-saves-lives" class="level2 page-columns page-full" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="coda-coffee-saves-lives"><span class="header-section-number">3.5</span> Coda: Coffee Saves Lives?</h2>
<p>Before we wrap-up this chapter, let’s return to the opening question: Does drinking a cup of coffee daily lower the risk of dying? <span class="citation" data-cites="ding:2015">Ding et al. (<a href="references.html#ref-ding:2015" role="doc-biblioref">2015</a>)</span> suggested that the answer is yes. What do you think?</p>
<div class="no-row-height column-margin column-container"><div id="ref-ding:2015" class="csl-entry" role="doc-biblioentry">
Ding, M. et al. (2015). Association of coffee consumption with total and cause-specific mortality in 3 large prospective cohorts. <em>Circulation</em>, <em>132</em>(24), 2305–2315.
</div></div><p>The authors took a confounder-control approach to the analysis, but they did not share a DAG. All of the variables in <a href="#fig-whatthedag">Figure&nbsp;<span>3.14</span></a> are covariates they mentioned adjusting for in their analysis. Is this the complete set of relevant variables? What are the backdoors that need closing? Are there any colliders?</p>
<div class="column-body-outset">
<div id="fig-whatthedag" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/whatthedag.svg" class="img-fluid figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3.14: What’s in the DAG?</figcaption><p></p>
</figure>
</div>
</div>
<p>Causal inference is hard, especially when dealing with observational data. We can’t rely on data or statistics alone to answer causal questions. In short, we need causal models.</p>
</section>
<section id="keep-learning" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="keep-learning"><span class="header-section-number">3.6</span> Keep Learning</h2>
<ul>
<li>Scott Cunningham’s “Causal Inference: The Mixtape”, <a href="https://ghr.link/cun">ghr.link/cun</a></li>
<li>Nick Huntington-Klein’s “The Effect: An Introduction to Research Design and Causality”, <a href="https://ghr.link/eff">ghr.link/eff</a></li>
<li>Richard McElreath’s “Statistical Rethinking”, <a href="https://ghr.link/sre">ghr.link/sre</a></li>
<li>Morgan and Winship’s “Counterfactuals and Causal Inference: Methods and Principles for Social Research”, <a href="https://ghr.link/mor">ghr.link/mor</a></li>
<li>Hernán and Robins’, “Causal Inference: What If”, <a href="https://ghr.link/wha">ghr.link/wha</a></li>
<li>Daniel Westreich’s, “Epidemiology by Design”, <a href="https://ghr.link/wes">ghr.link/wes</a></li>
</ul>



</section>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>You might be interested in associations if you are developing a prediction model.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>It turns out that a such a gene was later discovered, but it does not explain the clear relationship between smoking and cancer (see <em>The Book of Why</em> for a fascinating history).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Folks in this camp might refer to themselves as clinical trialists.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Formally the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Causal diagrams are not just for confounder-control studies, but this is where you see them used most often.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Often we care about the total causal effect that includes the direct effect (e.g., <code>road traveled</code> ⟶ <code>happiness</code>) AND all indirect, mediated effects (e.g., <code>road traveled</code> ⟶ <code>social relationships</code> ⟶ <code>happiness</code>). Sometimes, however, an aim of a study will to understand possible mechanisms of action that involve mediated pathways.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Both <code>DAGitty.net</code> and the <code>dagitty::adjustmentSets()</code> function in R will tell us which variables make up minimally sufficient adjustment sets.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>“What if I need to adjust for a variable that’s not in my dataset,” you ask. The short answer is that you might be out of luck. Whomp whomp. The longer answer is more optimistic, but probably involves creativity, some assumptions, and math.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>I didn’t simulate <code>social_relationships</code> because we don’t need it to estimate the total effect of <code>road_traveled</code> on <code>happiness</code>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Simulations involve a bit of noise.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Actually you’d probably use something like matching or inverse probability weighting, but those methods make the example too complex. See Heiss (2020) for examples.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>The real world is not a simulation (as far as I know).<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>A lot can go wrong with experiments that can dull their shine (we’ll get to examples later), but it’s hard to deny their claim in theory to strong causal inference.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./collaborations.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Build Collaborations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>